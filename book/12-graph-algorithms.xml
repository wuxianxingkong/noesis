<?xml version="1.0"  encoding="ISO-8859-1" ?> 
<?xml-stylesheet type="text/xsl" href="../book.xsl"?>


<document>
<title>Graph Algorithms</title>


<text>
<q>The key to solving many algorithmic problems is to think of them in terms of graphs... it is amazing how often messy applied probles have a simple description and solution in terms of classical graph properties</q> <cite>Skiena 2008</cite>.
</text>


<text>
1. Graph representation
2. Graph exploration
3. Optimization problems: MST, shortest paths, maximum flow
4. Hard problems, e.g. graph isomorphisms
5. A catalog of graph algorithms
</text>

<note>
<title>A primer on algorithm analysis</title>

<text>
The dominance pecking order in algorithm analysis <cite>Skiena 2008</cite>:
</text>

<equation>
n! >> c^n >> n^3 >> n^2 >> n^{1+\epsilon} >> n \log n 
</equation>

<equation>
>> n >> sqrt(n) >> \log^2 n >> \log n
</equation>

<equation>
>> \log n / \log \log n >> \log \log n >> \alpha(n) >> 1
</equation>

<list>
<item><eqn>\alpha(n)</eqn>: Inverse Ackerman's function (union-find data structure).</item>
<item><eqn>\log n / \log \log n</eqn>: Height of an n-leaf tree of degree <eqn>d = \log n</eqn>.</item>
<item><eqn>n^{1+\epsilon}</eqn>, e.g. <eqn>2^c*n^{1+1/c}</eqn> where <eqn>c</eqn> cannot be arbitrarily large (<eqn>2^c!</eqn>).</item>
</list>

</note>


<note>
<title>Fundamental data structures</title>


<text>
The importance of data structures @ <cite>Skiena 2008</cite>, chapter 3, page 65: <q>Changing the data structure does not change the correctness of the program, since we presumably replace a correct implementation with a different correct implementation. However, the new implementation of the data type realizes different tradeoffs in the time to execute various operations, so the total performance can improve dramatically</q>.
</text>

<text>
3 fundamental ADTs: containers, dictionaries &amp; priority queues
</text>

</note>

<!-- Graph representation -->

<document>
<tag>graph-representation</tag>
<title>Graph representation</title>

<text>
Graphs <eqn>G=(V,E)</eqn> of order <eqn>n</eqn> (number of vertices or nodes) and size <eqn>m</eqn> (number of edges or links)
</text>

<document>
<tag>graph-representation-matrix</tag>
<title>Adjacency matrix</title>

<text>
The most elementary representation of a graph is the adjacency matrix, also known as the connection matrix.
</text>

<text>
Using a <eqn>n \times n</eqn> square matrix <eqn>A</eqn>, where <eqn>A[i][j]=1</eqn> if there is a link from node i to node j, <eqn>A[i][j]=0</eqn> otherwise.
</text>

<text>
+ Easy to check whether a link exists or not, addition/removal of links: <eqn>O(1)</eqn> operations.
</text>

<text>
- Wasted space, specially for sparse networks: <eqn>O(n^2)</eqn> space vs. <eqn>O(n)</eqn> actual size 
- impractical for large networks unless an alternative representation scheme is employed...
</text>

<text>
Incidence matrix: An alternative representation scheme, where we use a <eqn>n \times m</eqn> square matrix <eqn>I</eqn>, where <eqn>I[i][j]=1</eqn> if node i is involved in link j, <eqn>I[i][j]=0</eqn> otherwise. [not used in practice]
</text>
</document>

<document>
<tag>graph-representation-lists</tag>
<title>Adjacency lists</title>

<text>
Linked lists are employed to store the neighbors of each node. 
i.e. a directed link (x,y) will appear in x's adjacency list.
i.e. undirected edges (x,y) will appear twice: in both x's and y's adjacency lists.
+ Implemented using pointers/references or dynamic arrays (to reduce memory fragmentation)
</text>

<text>
+ Faster computation of node degrees: <eqn>O(d)</eqn> (vs. <eqn>O(n)</eqn> for adjacency matrix). NOTE: Can be precomputed
</text>

<text>
+ Slower check of edge existence, insertion/deletion of edges: <eqn>O(d)</eqn> (vs. <eqn>O(1)</eqn> for adjacency matrix)

NOTE: Most algorithms can be easily designed so that they do not need to perform such operations, just by scanning all the links in each adjacency list (e.g. see breadth-first and depth-first traversal below).
</text>

<text>
+ Lower memory requirements: <eqn>O(n+m)</eqn> (vs. <eqn>O(n^2)</eqn> for adjacency matrix)
</text>

<text>
+ Faster graph traversal: <eqn>O(n+m)</eqn> (vs. <eqn>O(n^2)</eqn> for adjacency matrix)
</text>

<text>
Preferred choice for most problems.
</text>

<text>
Adjacency lists for sparse graphs, adjacency matrix only for dense graphs (uncommon in data mining problems).
</text>


<note>
<text>
Algorithmic libraries often provide general-purpose implementations of graph data structures, hence there is no need to implement them yourself unless your problem imposes very specific requirements, e.g. C++
</text>

<list>

<item>LEDA (Library of Efficient Data types and Algorithms), <url>http://www.algorithmic-solutions.com/</url>, originally from the Max Planck Institute for Informatics in Saarbrücken, Germany; since 2001, maintained by the Algorithmic Solutions Software GmbH.</item>

<item>Boost Graph Library (BGL), <url>http://www.boost.org</url>.</item>

<item>MatlabBGL, <url>http://www.cs.purdue.edu/homes/dgleich/packages/matlab_bgl/</url>, for Matlab: BGL port.</item>

<item>Combinatorica, <url>http://www.combinatorica.com</url>, for Mathematica, by Steven Skiena.</item>


</list>
</note>

</document>


<document>
<tag>graph-representation-compression</tag>
<title>Graph compression</title>

<text>
++ Compression (Liu, "Web Data Mining", 232-242)
</text>

</document>

</document>



<!-- Graph exploration -->

<document>
<tag>graph-exploration</tag>
<title>Graph exploration</title>

<text>
A fundamental graph problem: Visiting every node (or link) in a network in a systematic way, i.e. graph traversal.
</text>

<text>
For efficiency, we should not visit the same node repeatedly. For correctness, we should guarantee that every node is eventually visited. IDEA: Keep track of which nodes we have already visited...
</text>


<text>
BFS/DFS define a tree on the vertices of the graph.
</text>

<note>
<title>The Visitor design pattern</title>

<text>
++ Visitor design pattern <cite>Gamma et al. 1994</cite>
</text>

</note>


<document>
<tag>graph-exploration-dfs</tag>
<title>Depth-first search</title>

<text>
Implementation: recursion/stack (LIFO: we explore a path until we meet a dead end [i.e. no unvisited neighbors] and then backtrack). Recursion eliminates the need of an explicit stack and provides a neat implementation of the algorithm.
</text>


<text>
EFFICIENCY: <eqn>O(n+m)</eqn> using adjacency lists
</text>



<text>
Time intervals: a clock ticks every time we enter or exit a node and we keep track of the entry and exit times for each node.
</text>

<text>
Properties:

- Time intervals are properly nested: if we enter x before y, we exit y before x; i.e. all nodes reachable from a given node are explored before we finish with it.

- The difference between exit and entry times indicate the number of descendants of any given node. Actually half the time difference is the number of descendants in the search tree.

- For undirected graphs, DFS partitions edges into tree edges and back edges. Tree edges appear in the DFS tree, whereas back edges always point to an ancestor in the DFS tree (never to a sibling nor cousin node, just because all nodes reachable from a given node are explored before we finish with it).
</text>

<!--
Classifying (x,y) edges:

- parent[y] == x  => Tree edge

- discovered[y] and not processed[y]  => Back edge

For directed graphs:

- discovered[y] and (entry_time[y] &gt; entry_time[x]) => Forward edge

- discovered[y] and (entry_time[y] &lt; entry_time[x]) => Cross edge
-->


<text>
APPLICATIONS
</text>

<text>
Solving mazes: A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Tremaux[1] as a strategy for solving mazes (Wikipedia)
</text>

<text>
ON UNDIRECTED GRAPHS
</text>

<text>
++ Cycle detection: If there are no back edges, all edges belong to the tree, hence the graph is actually a tree.
</text>

<text>
++ Graph biconnectedness: Articulation vertices (a.k.a. cut-nodes): Vertices whose deletion disconnects the graph connected component (i.e. single points of failure). Graphs without articulation points are said to be biconnected, since they have no single points of failure. A brute force algorithm would remove each vertex and check whether the graph is still connected (an <eqn>O(n(n+m))</eqn> algorithm), albeit a linear algorithm is possible based on DFS if we track the earliest reachable node for each node. If the DFS tree represented the whole graph, all internal nodes would be articulation points. Three situations have to be considered <cite>Skiena 2008</cite>: 

1) root cut-nodes (if the root of the DFS tree has more than one child, then it is an articulation point), 

2) bridge cut-nodes (if v is an internal node in the DFS tree and its earliest reachable node is v, then v is an articulation point)

3) parent cut-nodes (if v is an internal node in the DFS tree and its earliest reachable node is its parent in the tree, then the parent is an articulation point unless it is the root of the DFS tree, i.e. it cuts v and its descendants from the graph).
</text>

<text>
++ Edge biconnectedness: Bridges (in terms of link failures instead of node failures). A single edge whose deletion disconnects the graph is called a bridge. Any graph without bridges is said to be edge-biconnected. Edge (x,y) is a bridge when it is a DFS tree edge and no back edge connects from y or below to x or above.
</text>


<text>
The classic sequential algorithm for computing biconnected components in a connected undirected graph due to John Hopcroft and Robert Tarjan (1973) [1] runs in linear time, and is based on depth-first search
+ Hopcroft, J.; Tarjan, R. (1973). "Efficient algorithms for graph manipulation". Communications of the ACM 16 (6): 372-378. doi:10.1145/362248.362272

In the online version of the problem, vertices and edges are added (but not removed) dynamically, and a data structure must maintain the biconnected components. Jeffery Westbrook and Robert Tarjan (1992) [2] developed an efficient data structure for this problem based on disjoint-set data structures. Specifically, it processes n vertex additions and m edge additions in O(m alpha(m, n)) total time, where alpha is the inverse Ackermann function. This time bound is proved to be optimal.
+ Westbrook, J.; Tarjan, R. E. (1992). "Maintaining bridge-connected and biconnected components on-line". Algorithmica 7: 433-464. doi:10.1007/BF01758773. edit

Uzi Vishkin and Robert Tarjan (1985) [3] designed a parallel algorithm on CRCW PRAM that runs in O(log n) time with n + m processors. 
+ Tarjan, R.; Vishkin, U. (1985). "An Efficient Parallel Biconnectivity Algorithm". SIAM Journal on Computing 14 (4): 862-000. doi:10.1137/0214061

Guojing Cong and David A. Bader (2005) [4] developed an algorithm that achieves a speedup of 5 with 12 processors on SMPs.
+ Guojing Cong and David A. Bader, (2005). "An Experimental Study of Parallel Biconnected Components Algorithms on Symmetric Multiprocessors (SMPs)". Proceedings of the 19th IEEE International Conference on Parallel and Distributed Processing Symposium. pp. 45b. doi:10.1109/IPDPS.2005.100
</text>


<text>
ON DIRECTED GRAPHS
</text>

<text>
++ Topological sorting: A common operation on directed acyclic graphs. Ordering the vertices... (such ordering cannot exist if the graph contains a directed cycle, i.e. no back edges). Each DAG has, at least, one topological sort, which gives an ordering to process each vertex before any of its successors (e.g. precedence constraints). Considering the nodes in the reverse order they are processed by DFS returns a valid topological sort.

 ref. first described by Kahn (1962): http://en.wikipedia.org/wiki/Topological_sorting
      Kahn, A. B. (1962), "Topological sorting of large networks", Communications of the ACM 5 (11): 558-562, doi:10.1145/368996.369025.
</text>

<text>
++ Identifying strongly-connected components: 1) Traverse the graph atarting from any given node to discover the nodes that are reachable from the node. 2) Build a graph G'=(V,E') with the same vertex set but all its arcs reversed; i.e. <eqn>(y,z) \in E' iff (x,y) \in E</eqn>. 3) Perform a traveral starting from v in G', which will result in discovering the set of nodes that can reach v. A graph G is strongly connected if all nodes in G can reach v and are reachable from v.
</text>

<text>
Any graph can be partitioned into a set of strongly-connected components. Using DFS, we can easily identify cycles. If we take into account that all the nodes involved in a cycle must belong to the same strongly-connected component, we can collapse the nodes in the cycle into a single vertex and repeat the process. When no cycles remain, each vertex represents a different strongly-connected component.
</text>

<text>
Linear SCC algorithms:
- Kosaraju's algorithm, 1978: http://en.wikipedia.org/wiki/Kosaraju%27s_algorithm
   ref.  Aho, Hopcroft and Ullman credit it to an unpublished paper from 1978 by S. Rao Kosaraju and Micha Sharir. (Alfred V. Aho, John E. Hopcroft, Jeffrey D. Ullman. Data Structures and Algorithms. Addison-Wesley, 1983)
- Cheriyan-Mehlhorn/Gabow algorithm, 1996/1999: http://en.wikipedia.org/wiki/Cheriyan%E2%80%93Mehlhorn/Gabow_algorithm
   ref. Cheriyan, J.; Mehlhorn, K. (1996), "Algorithms for dense graphs and networks on the random access computer", Algorithmica 15: 521-549, doi:10.1007/BF01940880
   ref. Gabow, H.N. (2003), "Searching (Ch 10.1)", in Gross, J. L.; Yellen, J., Discrete Math. and its Applications: Handbook of Graph Theory, 25, CRC Press, pp. 953-984
- Tarjan's strongly connected components algorithm, 1972: http://en.wikipedia.org/wiki/Tarjan's_strongly_connected_components_algorithm
   ref. Tarjan, R. E. (1972), "Depth-first search and linear graph algorithms", SIAM Journal on Computing 1 (2): 146-160, doi:10.1137/0201010


 all efficiently compute the strongly connected components of a directed graph, but Tarjan's and Gabow's are favoured in practice since they require only one depth-first search rather than two. http://en.wikipedia.org/wiki/Strongly_connected_component
</text> 


<text>
Parallelization of DFS 
- for dense graphs @ Section 10.6 <cite>Grama et al. 2003</cite>
- Parallel DFS @ Section 11.4  <cite>Grama et al. 2003</cite>
</text>


</document>



<document>
<tag>graph-exploration-bfs</tag>
<title>Breadth-first search</title>

<text>
Implementation: queue (FIFO: we explore the oldest unexplored vertices first).
</text>

<text>
EFFICIENCY: <eqn>O(n+m)</eqn> using adjacency lists
</text>

<text>
Properties (due to the fact that each path in the tree must be the shortest path in the graph):

- For undirected graphs, edges not appearing in the breadth-first search tree can only point to nodes on the same level or to the level directly below.
</text>



<text>
APPLICATIONS
</text>

<text>
++ Shortest paths can be found by performing a breadth-first search on unweighted/binary graphs (more elaborate algorithms are required for weighted graphs, as we will see later): the tree resulting from BFS defines the shortest paths from the root to the remaining nodes in the graph. IMPLEMENTATION: parent[node] @ Visitor + reversal using recursion/stack
</text>

<text>
++ Connected components: Any node we visit is part of the same connected component. Starting the search from any unvisited node, we can obtain additional connected components.
</text>

<text>
++ Two-coloring graphs = Bipartite testing: A graph is bipartite if it can be colored without conflicts using two colors. Whenever we visit a new node, we color it using the opposite of its parent's color. If edges appear with the same color at both ends, the graph is not bipartite. Otherwise, we have partitioned the graph.
</text>


<text>
++ Distributed algorithm @ MIT Lecture 2 (synchronous) + Lecture 9 (asynchronous) / Section 4.2 + Section 15.4 <cite>Lynch 1997</cite>
@ MIT Distributed algorithms Lecture 21
</text>

<text>
++ Sublinear approximate algorithm @ MIT Sublinear algorithms => Goal: Quickly distinguish inputs that have specific property from those that are far from having the property...
</text>


</document>



<document>
<tag>graph-exploration-best-first</tag>
<title>Best-first search</title>

<text>
Best-first search... heuristics

Judea Pearl described best-first search as estimating the promise of node n by a "heuristic evaluation function f(n) which, in general, may depend on the description of n, the description of the goal, the information gathered by the search up to that point, and most important, on any extra knowledge about the problem domain." ref. Pearl, J. Heuristics: Intelligent Search Strategies for Computer Problem Solving. Addison-Wesley, 1984.

</text>

<text>
http://en.wikipedia.org/wiki/A*_search_algorithm
A* search algorithm

HISTORY: In 1964 Nils Nilsson invented a heuristic based approach to increase the speed of Dijkstra's algorithm. This algorithm was called A1. In 1967 Bertram Raphael made dramatic improvements upon this algorithm, but failed to show optimality. He called this algorithm A2. Then in 1968 Peter E. Hart introduced an argument that proved A2 was optimal when using a consistent heuristic with only minor changes. His proof of the algorithm also included a section that showed that the new A2 algorithm was the best algorithm possible given the conditions. He thus named the new algorithm in Kleene star syntax to be the algorithm that starts with A and includes all possible version numbers or A*. [2]

Hart, P. E.; Nilsson, N. J.; Raphael, B. (1972). "Correction to "A Formal Basis for the Heuristic Determination of Minimum Cost Paths"". SIGART Newsletter 37: 28-29.

Often used for path finding...
</text>


<text>
http://en.wikipedia.org/wiki/B*
B* algorithm
Berliner, Hans (1979). "The B* Tree Search Algorithm. A Best-First Proof Procedure.". Artificial Intelligence 12 (1): 23-40. doi:10.1016/0004-3702(79)90003-1
</text>


<text>
++ Parallel best-first search @ Section 11.5 <cite>Grama et al. 2003</cite>
</text>

</document>

</document>


<!-- Minimum spanning trees -->

<document>
<tag>graph-spanning-trees</tag>
<title>Minimum spanning trees</title>

<text>
A spanning tree of a graph <eqn>G=(V,E)</eqn> is a subset of <eqn>n-1</eqn> edges from <eqn>E</eqn> that connect all the vertices in <eqn>V</eqn> and, therefore, form a tree. In weighted graphs, where edges have numerical values associated to them, known as weights, a minimum spanning tree is a spanning tree whose sum of edge values is as small as possible. A tree is always the smallest connected graph in terms of its number of edges (and also the most vulnerable to single failures), while a MST is the cheapest connected graph in term of edge costs.
</text>

<text>
The algorithms in this section can also be adapted to find maximum spanning trees (just negate the weights and the MST in the negated graph is the maximum spanning tree in the original graph), minimum product spanning trees (replace edge weights with their logarithms so that products are converted into sums), or minimum bottleneck spanning tree (i.e. a spanning tree that minimizes the maximum edge weight, by just employing Kruskal's algorithm). 
</text>

<text>
However, other related problems cannot be solved using the efficient algorithms described in this section:
- Steiner trees (when we can add intermediate vertices), NP-hard in general.
- Low-degree spanning tree (i.e. the minimum spanning tree whose maximum node degree is smaller; in case it where a simple path, this problem would be equivalent to finding a Hamiltonian path = TSP, NP-complete)
</text>

<text>
APPLICATIONS: 
- Whenever we need to connect a set of nodes with minimum cost (e.g. transportation networks, communication networks, distribution networks...). 
- Bottleneck spanning trees have applications when edge weights represent costs, capacities, or strengths
- Clustering (e.g. hierarchical clustering)
</text>


<text>
In unweighted graphs (or weighted graphs where all edges have the same cost), any tree is also a minimum spanning tree. Its cost will always be <eqn>n-1</eqn> times the cost of a single edge. In this case, a spanning tree can easily be found by performing any systematic graph traversal (e.g. DFS or BFS).
</text>

<text>
In weighted graphs, efficient greedy algorithms exist than can be employed to identify minimum spanning trees...
</text>


<document>
<tag>graph-spanning-trees-greedy</tag>
<title>Greedy MST algorithms</title>


<text>
@ MIT Graphs - Shortest paths
</text>

<text>
Greedy algorithms choose what to do next by selecting the best local candidate to be added to the solution and, when the optimality of the chosen heuristics can be proved, they provide efficient algorithms for solving problems such as finding the minimum spanning tree.
</text>

<text>
Candidates: edges/nodes...
</text>


<text>
MSTs are unique when all m edge weights are distinct. The way ties are broken leads to different MSTs...
</text>

<document>
<tag>graph-spanning-trees-greedy-prim</tag>
<title>Prim's algorithm</title>

<text>
Starts from any node and adds one edge at a time until all the vertices are connected... heuristics: select the edge with the smallest weight that will enlarge the number of vertices in the tree
</text>

<text>
No cycles are created since only edges between tree and non-tree nodes are added to the solution
</text>

<text>
EFFICIENCY:
<eqn>n</eqn> iterations through <eqn>m</eqn> edges = <eqn>O(mn)</eqn>
</text>

<text>
PROOF? by contradiction
</text>

<text>
Tree reconstruction: parent vector or dynamic structure...
</text>

</document>

<document>
<tag>graph-spanning-trees-greedy-kruskal</tag>
<title>Kruskal's algorithm</title>

<text>
Instead of starting on any particular node and growing a tree, Kruskal's algorithm builds the minimum spanning tree by connecting sets of vertices...
</text>

<text>
Initially: each vertex a separate component + Iteration: consider the least cost edge and test whether it connects two separate components (if its both endpoints are already in the same connected component, the edge is discarded, since it would create a cycle; if not, it is added to the MST and the components it connects are merged).
</text>

<text>
Since each connected component is always a tree, we do not have to check for cycles
</text>

<text>
EFFICIENCY:
Sorting edges <eqn>O(m \log m)</eqn> + <eqn>m</eqn> iterations / testing connectivity by BFS/DFS <eqn>O(n)</eqn> = <eqn>O(mn)</eqn>
</text>

<text>
PROOF? by contradiction
</text>

</document>

</document>


<document>
<tag>graph-spanning-trees-sequential</tag>
<title>Sequential implementation of MST algorithms</title>

<text>
Naive implementation:
<eqn>O(mn)</eqn> for Kruskal's algorithm 
<eqn>O(mn)</eqn> for Prim's algorithm 
(i.e. <eqn>O(n^3)</eqn> for dense graphs, <eqn>O(n^2)</eqn> for sparse graphs)
</text>

<text>
Prim's algorithm:
1) Maintaining a boolean flag to indicate whether a vertex is already in the tree or not, we can test whether the current edge links to a non-tree vertex in constant time, leading to a <eqn>O(n^2)</eqn> algorithm (both for dense and sparse graphs).
2) Optimization: <eqn>O(n \log k)</eqn> on graphs that have only <eqn>k</eqn> different edge costs.
2) The use of priority queues can lead to a more efficient <eqn>O(m + n \log n)</eqn> implementation, by making it faster to find the minimum cost edge at each iteration.
</text>

<text>
Kruskal's algorithm:
1) Union-find data structure for implementing the component test in <eqn>O(\log n)</eqn> = <eqn>O(m \log m)</eqn> time (i.e. needed to sort the edges).
+ Typically faster than Prim's algorithm for sparse graphs
</text>


<text>
Union-find data structure for representing set partitions (a partitioning of elements, say nodes, intro a collection of disjoint subsets):
1) Union: Component merging in <eqn>O(\log n)</eqn>
2) Find: Same component check in <eqn>O(\log n)</eqn>

Solution: Represent each component as a backwards tree, with pointers from a node to its parent (choosing how to merge in order to limit the height of the trees and reduce the effect of unbalanced trees: the smaller tree will always be a subtree of the larger tree; why? height of the nodes in the larger tree stay the same, heights in the smaller tree are increased by one)

Result: We must double the number of nodes to increase the height of the resulting tree, hence the <eqn>O(\log n)</eqn> result
</text>

<text>
++ Union-find can be done even faster (Skiena, 385)
</text>

</document>

<document>
<tag>graph-spanning-trees-parallel</tag>
<title>Parallel MST algorithms</title>

<text>
Parallel Prim's algorithm
- for dense graphs @ Section 10.2 <cite>Grama et al. 2003</cite>
- for sparse graphs @ Section 10.7.2 <cite>Grama et al. 2003</cite>
+ Bibliographic notes
</text>

</document>

<document>
<tag>graph-spanning-trees-distributed</tag>
<title>Distributed MST algorithms</title>

<text>
Distributed MST algorithms @ MIT 6.852: Distributed Algorithms - Lecture 4 (synchronous) + Lecture 9 (asynchronous) / Section 4.4 + Section 15.5 <cite>Lynch 1997</cite>
</text>

</document>

</document>




<!-- Shortest paths -->

<document>
<tag>graph-shortest-paths</tag>
<title>Shortest paths</title>


<text>
When graphs are weighted, i.e. each edge in the graph is associated to a numerical value, computing their shortest paths is more complicated than performing breadth-first traversals (recall that the BFS tree leads to minimum-number-of-links paths)...
</text>

<note>
<title>The Strategy design pattern</title>

<text>
++ The Strategy design pattern <cite>Gamma et al. 1994</cite>
</text>
</note>


<text>
@ MIT Graphs - Shortest paths
</text>


<!-- Shortest paths: Single source -->

<document>
<tag>graph-shortest-paths-dijkstra</tag>
<title>Single source shortest paths</title>

<text>
APPLICATIONS: Finding directions... (note: hierarchical A*)
</text>

<text>
Finding the shortest path from a source node <eqn>s</eqn> to every other node in the network...
</text>


<text>
IDEA: Shortest path from s to t goes through x, then that path contains the shortest path from s to x
</text>

<text>
Similar to Prim's algorithm... each iteration determines the shortest path from s to a new vertex x by minimizing d(s,v)+w(v,x)

In Prim's algorithm, we just take into account the weighte of the edge we add to the MST; in Dijkstra's algorithm, we consider the cost of the whole path from s to x (i.e. the final edge weight plus the distance from s to the tree vertex that is adjacent to the new edge). Dijkstra's algorithm returns a shortest-path spanning tree from s to all the other nodes in the network.
</text>

<text>
http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm
Dijkstra's algorithm: 
</text>

<text>
EFFICIENCY:
- <eqn>O(n^2)</eqn> using a naive implementation
- <eqn>O(m + n \log n)</eqn> using Fibonacci heaps
</text>

<text>
Related problem: Single-destination shortest path problem for directed graphs.
</text>

<text>
CORRECTNESS: Only on graphs without negative edges!!!
</text>


<text>
Parallelization of Dijkstra's algorithm
- for dense graphs @ Section 10.3 <cite>Grama et al. 2003</cite>
+ Bibliographic notes
</text>

</document>



<!-- Shortest paths: All pairs -->

<document>
<tag>graph-shortest-paths-all-pairs</tag>
<title>All-pairs shortest paths</title>


<text>
Some problems requite computing the shortest paths between all pairs of nodes, e.g. the central node (i.e. the node that minimizes the longest or average distance to every other node) or the network diameter (i.e. the longest shortest-path distance between two nodes in the network). 
</text>

<text>
APPLICATIONS: 
- bounding the amount of time needed to deliver a packet through a network
- transitive closure (reachable vertices can be computing using graph traversal algorithms, an all-pairs shortest path algorithms returns them all in a whole batch, albeit less efficiently).
- pattern recognition problems, e.g. Viterbi algorithm, a dynamic programming algorithm that basically solves a shortest path problem on a DAG.
</text>

<text>
A first solution: Repeating Dijkstra's algorithm for each node
</text>

<text>
+ Best on adjacency matrix (anyway we will need to store <eqn>n \times n</eqn> distances), i.e. one of the rare cases where algorithms on adjacency matrices work better
</text>


<text>
http://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm
Bellman-Ford algorithm: <eqn>O(nm)</eqn> time, <eqn>O(n)</eqn> space
</text>


<text>
http://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm
Floyd-Warshall algorithm: <eqn>O(n^3)</eqn>

- Define the length of the shortest path from i to j using only the k first vertices as possible intermediate nodes: <eqn>W[i,j]^k</eqn>
- Initial shortest-path matrix = adjacency matrix
- <eqn>W[i,j]^k = min \{ W[i,j]^{k-1}, W[i,k]^{k-1} + W[k,j]^{k-1} \}</eqn>
- Compact implementation: 3 nested loops
- Ancillary matrix to reconstruct the paths
</text>



<text>
http://en.wikipedia.org/wiki/Johnson%27s_algorithm
Johnson's algorithm: <eqn>O(n^2 \log n + nm)</eqn> using <eqn>O(nm)</eqn> time for the Bellman-Ford stage of the algorithm, and <eqn>O(n \log n + m)</eqn> for each of <eqn>n</eqn> instantiations of Dijkstra's algorithm. Faster than the Floyd-Warshal algorithm for sparse graphs.
</text>


<text>
Parallelization of Floyd's algorithm 
- for dense graphs @ Section 10.4 <cite>Grama et al. 2003</cite>
+ Bibliographic notes
</text>


<text>
Distributed shortest paths @ MIT 6.852: Lectures 2-3 (synchronous) + Lectures 8-9 (asynchronous)  / Section 4.3 + Section 15.4 <cite>Lynch 1997</cite>
</text>

</document>

</document>




<!-- Network flows -->

<document>
<tag>graph-flow</tag>
<title>Network flows: Maximum flow and minimum cuts</title>

<text>
Interpreting weighted graphs as distribution networks where weights represent capacities
</text>

<text>
Network flow problem: The maximum amount of flow that can be sent from s to t in a given graph.
</text>



<text>
IDEA: <b>Augmenting paths</b>
i.e. iteratively find a path of positive capacity from s to t and add it to the flow
- As in a traffic jam, the flow is limited by the most congested point.
- A flow through a network is optimal if and only if it contains no augmenting path.
</text>

<text>
MECHANISM: Directed <b>residual flow graph</b> R(G,f), where G is the graph and f the current flow
i.e. For each edge (i,j) in G with capacity c(i,j) and flow f(i,j), R can contain two edges
- an edge (i,j) with weight c(i,j)-f(i,j) if c(i,j)>f(i,j)
- an edge (j,i) with weight f(i,j)-c(i,j) if f(i,j)>0

An edge (i,j) in the residual flow graph indicates that flow can still be pushed from i to j (its weight determines the amount of its unused capacity). 

A path from s to t in the residual graph (and the minimum edge weight on this path) defines an augmenting path (and the amount of flow that can be pushed).
</text>

<text>
- For each edge in the residual graph: keep track of the amount of flow currently going through the edge as well as any unused capacity (the residual capacity)
- Initialization: create directed flow edges (i,j) and (j,i); initial flows set to 0; initial residual flows set to their capacity in the original graph.
</text>

<text>
ITERATIVE ALGORITHM
Ford-Fulkerson algorithm <cite>Ford and Fulkerson 1956</cite>
http://en.wikipedia.org/wiki/Ford%E2%80%93Fulkerson_algorithm

- Use the augmented path to update the residual graph (transfer the maximum possible volume from the residual capacity = the path edge with the smallest amount of residual capacity; NOTE: adding positive flow to (i,j) reduces the residual capacity of (i,j) but also increases the residual capacity of (j,i), i.e. augmenting a path requires updates to both forward and reverse links along the augmenting path ).
- Terminate when no such augmenting path exists.

As described, even though the algorithm converges to the optimal solution, it may take arbitrarily long (each augmenting path might add only a little to the flow).
</text>

<text>
IMPLEMENTATION (Edmonds-Karp algorithm <cite>Edmonds and Karp 1972</cite>):
http://en.wikipedia.org/wiki/Edmonds%E2%80%93Karp_algorithm
The algorithm is identical to the Ford–Fulkerson algorithm, except that the search order when finding the augmenting path is defined. The path found must be a shortest path that has available capacity

- BFS to look for any path from the source to the sink that increases the total flow (only consider edges with residual capacity, i.e. positive residual flow)
</text>

<text>
 <cite>Edmonds and Karp 1972</cite> proved that selecting the shortest unweighted augmenting path (a.k.a. shortest geodesic path) guarantees that <eqn>O(n^3)</eqn> augmentations suffice for optimization.

<eqn>O(nm^2)</eqn> running time: each augmenting path can be found in <eqn>O(m)</eqn> time, every time at least one of the <eqn>m</eqn> edges becomes saturated, the distance from the saturated edge to the source along the augmenting path must be longer than last time it was saturated, and that length is at most <eqn>n</eqn>.

Another property of this algorithm is that the length of the shortest augmenting path increases monotonically.
</text>

<text>
@ MIT Graphs - Network flow
</text>


<document>
<tag>graph-minimum-cuts</tag>
<title>Minimum cuts</title>

<text>
A set of edges whose deletion separates s from t is called an s-t cut. Since no s-t flow can exceed the weight of the minimum s-t cut, finding the maximum flow is equivalent to discovering the minimum cut. In fact, the maximum s-t flow equals the weight of the minimum s-t cut.
</text>

<text>
Therefore, flow algorithms can be used to solve edge and vertex connectivity problems...
</text>

</document>

<document>
<tag>graph-matching</tag>
<title>An application: Matching problems</title>

<text>
A matching in a graph <eqn>G=(V,E)</eqn> is a subset of edges <eqn>E' \subset E</eqn> such that no two edges in <eqn>E'</eqn> share a vertex.
</text>

<text>
Remember that a graph is bipartite (or two-colorable) if its vertices can be divided into two sets, L and R, so that every edge in the graph connects one vertex from the first set to a different vertex on the second set.
</text>

<text>
APPLICATIONS:
- Assignment problems (jobs to be done and people who can do them, male and female customers of a dating web site...)
</text>

<text>
The largest possible bipartite matching can be found using network flow. Just add a source node s connected to every vertex in L with capacity 1, add a sink node t connected to every node in R, and assign each edge in the bipartite graph a capacity of 1. The maximum flow from s to t determines the largest matching in the bipartite graph.
</text>

<text>
@ MIT Graphs - Matching
</text>

</document>

</document>





<!-- Graph isomorphism -->

<document>
<tag>graph-isomorphism</tag>
<title>Graph isomorphism</title>


<text>
@ MIT Interactive proofs and zero knowledge
</text>

<text>
Practical algorithms for graph isomorphism:

- McKay (1981), McKay, Brendan D. (1981), "Practical graph isomorphism", Congressus Numerantium 30: 45–87, 10th. Manitoba Conference on Numerical Mathematics and Computing (Winnipeg, 1980).

- Schmidt and Druffel (1976), 
Schmidt, Douglas C.; Druffel, Larry E. (1976), "A Fast Backtracking Algorithm to Test Directed Graphs for Isomorphism Using Distance Matrices", J. ACM (ACM) 23 (3): 433–445, doi:10.1145/321958.321963, ISSN 0004-5411.

- Ullman (1976)
Ullman, Julian R. (1976), "An algorithm for subgraph isomorphism", Journal of the ACM 23: 31–42, doi:10.1145/321921.321925.
</text>


<text>
While they seem to perform well on random graphs, a major drawback of these algorithms is their exponential time performance in the worst case.[9]
</text>

<text>
Best known algorithm: <cite>Luks 1983</cite> has run time <eqn>2^O(\sqrt{n \log n})</eqn> for graphs with n vertices.
</text>

<text>
Isomorphism for many special classes of graphs can be solved in polynomial time, and in practice graph isomorphism can often be solved efficiently:
</text>

<list>

<item>Trees[10]:  P.J. Kelly, "A congruence theorem for trees" Pacific J. Math., 7 (1957) pp. 961–968; Aho, Hopcroft &amp; Ullman 1974. Aho, Alfred V.; Hopcroft, John; Ullman, Jeffrey D. (1974), The Design and Analysis of Computer Algorithms, Reading, MA: Addison–Wesley.</item>
<item>Planar graphs[11]: Hopcroft, John; Wong, J. (1974), "Linear time algorithm for isomorphism of planar graphs", Proceedings of the Sixth Annual ACM Symposium on Theory of Computing, pp. 172–184, doi:10.1145/800119.803896.</item>

<item>Interval graphs[13]: Booth, Kellogg S.; Lueker, George S. (1979), "A linear time algorithm for deciding interval graph isomorphism", Journal of the ACM 26 (2): 183–195, doi:10.1145/322123.322125.</item>

<item>Permutation graphs[14]: Colbourn, C. J. (1981), "On testing isomorphism of permutation graphs", Networks 11: 13–21, doi:10.1002/net.3230110103.</item>

<item>Partial k-trees[15]: Bodlaender, Hans (1990), "Polynomial algorithms for graph isomorphism and chromatic index on partial k-trees", Journal of Algorithms 11 (4): 631–643, doi:10.1016/0196-6774(90)90013-5.</item>

<item>Bounded-parameter graphs: 
- Graphs of bounded genus[16],
Miller, Gary (1980), "Isomorphism testing for graphs of bounded genus", Proceedings of the 12th Annual ACM Symposium on Theory of Computing, pp. 225–235, doi:10.1145/800141.804670, ISBN 0897910176.
Filotti, I. S.; Mayer, Jack N. (1980), "A polynomial-time algorithm for determining the isomorphism of graphs of fixed genus", Proceedings of the 12th Annual ACM Symposium on Theory of Computing, pp. 236–243, doi:10.1145/800141.804671, ISBN 0897910176.

- graphs of bounded degree[17], 
Luks, Eugene M. (1982), "Isomorphism of graphs of bounded valence can be tested in polynomial time", Journal of Computer and System Sciences 25: 42–65, doi:10.1016/0022-0000(82)90009-5.

- graphs with bounded eigenvalue multiplicity[18], 
Babai, László; Grigoryev, D. Yu.; Mount, David M. (1982), "Isomorphism of graphs with bounded eigenvalue multiplicity", Proceedings of the 14th Annual ACM Symposium on Theory of Computing, pp. 310–324, doi:10.1145/800070.802206, ISBN 0897910702.

- k-Contractible graphs (a generalization of bounded degree and bounded genus)[19], 
 Gary L. Miller: Isomorphism Testing and Canonical Forms for k-Contractable Graphs (A Generalization of Bounded Valence and Bounded Genus). Proc. Int. Conf. on Foundations of Computer Theory, 1983, pp. 310–327 (Lecture Notes in Computer Science, vol. 158, full paper in: Information and Control, 56(1–2):1–20, 1983.)

- Color-preserving isomorphism of colored graphs with bounded color multiplicity (i.e., at most k vertices have the same color for a fixed k)[20] ^ Eugene Luks, "Parallel algorithms for permutation groups and graph isomorphism", Proc. IEEE Symp. Foundations of Computer Science, 1986, 292-302
</item>

</list>


</document>



<!-- Algorithm catalog -->

<document>
<tag>graph-algorithm-catalog</tag>
<title>A catalog of graph algorithms</title>

<text>
Graphs of order n (nodes) and size m (links)...
</text>

<document>
<tag>graph-algorithm-catalog-linear</tag>
<title>Linear-time algorithms</title>


<text>
The following problems can be solved with algorithms that run in linear time (i.e. <eqn>O(n+m)</eqn>):
</text>

<list>

<item>Graph traversal, either BFS, DFS, or best-first search (as in A*).</item>

<item>Shortest paths in unweighted graphs (based on BFS): Finding the shortest paths and also counting the number of different shortest paths.</item>

<item>Identifying connected and strongly-connected components (based on graph traversal).</item>

<item>Bipartite testing (based on graph traversal).</item>

<item>Cycle detection (based on DFS).</item>

<item>Identification of articulation points, i.e. graph biconnectedness (based on DFS).</item>

<item>Identification of bridges, i.e. edge-biconnectedness (based on DFS).</item>

<item>Topological sorting (based on DFS).</item>

<item>Single-source shortest paths on DAGs, even with negative weights.</item>

<item>Hamiltonian path on a DAG (topological sorting).</item>

<item>Maximum matching in a tree.</item>


</list>

</document>

<document>
<tag>graph-algorithm-catalog-almost-linear</tag>
<title>Almost linear-time algorithms</title>


<text>
The following algorithms are almost linear, i.e. <eqn>O(n \log n)</eqn>:
</text>

<list>

<item>Minimum spanning trees (using either Prim's or Kruskal's algorithm).</item>

<item>Single-source shortest paths (using Dijkstra's algorithm), only for positive weights.</item>

</list>


<text>
NOTE: When working with large data sets, only linear (<eqn>O(n)</eqn>) or near linear (e.g. <eqn>O(n \log n)</eqn>) are likely to be suitable.
</text>


</document>

<document>
<tag>graph-algorithm-catalog-polynomial</tag>
<title>Polynomial-time algorithms</title>

<text>
The following algorithms require polynomial time:
</text>

<list>

<item>All-pairs shortest paths (e.g. Floyd's algorithm).</item>

<item>Transitive closure (i.e. all-pairs reachability) and reduction.</item>

<item>Finding a directed cycle of minimum total weight, <eqn>O(n^3)</eqn>.</item>

<item>Maximum network flow.</item>

<item>Minimum cuts.</item>

<item>Maximum bipartite matching.</item>

<item>Minimum-size edge cover (i.e. a set of edges, of minimum size, such that each vertex in the graph is incident to at least one edge from the set).</item>

<item>Eulerian cycle</item>

</list>

</document>

<document>
<tag>graph-algorithm-catalog-hard</tag>
<title>Hard problems</title>


<note>
<text>
NP (non-deterministic polynomial) is the set of all decision problems whose solutions can be verified in polynomial time (or, alternatively, the set of decision problems that can be solved in polynomial time on a nondeterministic Turing machine). That is, even though any given solution to such a problem can be verified quickly (in polynomial time), there is no known efficient way to discover a solution to the problem.
</text>

<text>
In computational complexity theory, NP-hard problems are, at least as hard as the hardest problems in NP (i.e. any NP problem can be converted into L by a transformation of the inputs in polynomial time or, in theoretical parlance, every problem in NP is reducible to L in polynomial time). A problem is NP-complete when it is in NP and also NP-hard. Hence, NP-complete is a subset of NP, which is itself a subset of NP-hard.
</text>

<text>
This the P=NP? question is not resolved, and hence we do not know whether solution verification is really easier than solution discovery for NP problems, we could say that NP problems are <q>not-necessarily polynomial time</q> <cite>Skiena 2008</cite>.
</text>

<text>
More general classes exist, such as PSPACE: the set of all decision problems which can be solved by a Turing machine using a polynomial amount of space. Formally, <eqn>P \subseteq NP \subseteq PSPACE</eqn>. Some graph problems are known to be PSPACE-complete, such as the Canadian Traveler Problem (CTP), a generalization of the shortest path problem to graphs that are partially observable (i.e. the graph is revealed while it is being explored) that has many practical applications.

ref. C.H. Papadimitriou; M. Yannakakis (1989). "Shortest paths without a map". Lecture notes in computer science. 372. Proc. 16th ICALP. Springer-Verlag. pp. 610–620.
</text>

</note>


<text>
The following problem occupies a singular position within the complexity hierarchy, since it is not known whether the problem has a fast algorithm that solves it ot is NP-complete:
</text>

<list>

<item>Isomorphism testing, i.e. checking whether the topological structure of two graphs are identical (typically solved using backtracking)</item>

</list>


<text>
The following algorithms are known to be NP-hard (most of them are, in fact, NP-complete): COVERING/PARTITIONING, SUBGRAPHS, ORDERING
</text>

<list>

<item>Hamiltonian circuit on unweighted graphs (both directed and undirected), remains NP-complete even for directed planar graphs (graphs that can be embedded in the plane) and for cubic planar undirected graphs (graphs in which all vertices have degree three).

M. R. Garey, D. S. Johnson, and L. Stockmeyer. Some simplified NP-complete problems. Proceedings of the sixth annual ACM symposium on Theory of computing, p. 47-63. 1974.</item>

<item>The traveling salesman problem (TSP) on weighted graphs, one of the most intensively studied problems in optimization. http://en.wikipedia.org/wiki/Travelling_salesman_problem</item>

<item>Hamiltonian path on unweighted graphs. The Hamiltonian path problem for graph G is equivalent to the Hamiltonian cycle problem in a graph H obtained from G by adding a new vertex and connecting it to all vertices of G.</item>

<item>Eulerian path</item>

<item>Longest simple path</item>

<item>Longest circuit</item>

<item>Path with forbidden pairs or vertices</item>




<item>Minimum-size vertex cover: A vertex cover is a subset of vertices <eqn>V' \in V</eqn> such that every edge in E contains at least one vertex from V'. Obviously, the set of all vertices is a vertex cover. The problem of finding a minimum vertex cover is a classical optimization problem in computer science a, one of Karp's 21 NP-complete problems</item>

<item>Maximum-size independent set: An independent set is a set of vertices U such that no edge in E is incident on two vertices of U. A maximum independent set is a largest independent set and the maximum independent set problem is known to be NP-hard. A set is independent if and only if its complement is a vertex cover. The sum of the size of the maximum independent set and the size of a minimum vertex cover is the number of vertices in the graph.</item>



<item>Maximum cliques, <eqn>O(3^{n/3}) = O(1.4422^n)</eqn>, since any n-vertex graph has at most any n-vertex graph has at most <eqn>3^{n/3}</eqn> maximal cliques;  http://en.wikipedia.org/wiki/Clique_problem</item>

<item>Dense subgraph, i.e. a subgraph with exactly k vertices and at least y edges.</item>

<item>Planar subgraph, i.e. a subgraph with at least k edges that can be embedded on the plane.</item>

<item>Eulerian subgraph, i.e. a subgraph that contains an Eulerian cycle.</item>

<item>Bipartite subgraph, i.e. a subgraph with at least k edges that can is bipartite.</item>

<item>Cubic subgraph, i.e. a cubic subgraph with at least k edges.</item>

<item>Degree-bounded connected subgraph, i.e. a subgraph with at least k edges so that the resulting subgraph is connected and no vertex degree exceeds d.</item>

<item>Transitive subgraph, i.e. (u,w) (w,v) implies (u,v).</item>

<item>Uniconnected subgraph, i.e. subgraph containing at most one directed path between any pair of vertices.</item>

<item>Minimum k-connected subgraph, i.e. k-connected subgraph (cannot be disconnected by removing fewer than k vertices).</item>

<item>Subgraph isomorphism, i.e. given two graphs G and H as input, determine whether G contains a subgraph that is isomorphic to H.</item>

<item>Largest common subgraph, i.e. given two graphs G and H as input, determine their largest isomorphic subgraphs.</item>

<item>Graph contractability, i.e. given two graphs G and H as input, can a graph isomorphic to H be obtained by a sequence of edge contractions? (when two adjacent vertices are replaced by a single one that is adjacent to exactly those vertices that were adjacent to at least one of the original vertices).</item>

<item>Minimum equivalent digraph. i.e. subgraph that contains a directed path from u to v only when the original graph does.</item>



<item>Steiner trees, i.e. find the shortest interconnect for a given set of objects (as MST, but including the possibility of adding extra intermediate nodes).</item>

<item>Constrained spanninig trees: degree-constrained spanning tree (a spanning tree with no vertex with degree above k), maximum-leaf spanning trees, (a spanning tree with k or more leaves), shortest-total-path-length spanning tree (i.e. minimizing the sum of the length of the path in the tree between every pair of nodes), bounded-diameter spanning tree (no simple path with more than d edges)...</item>

<item>Maximum cut (the max-cut problem), i.e. finding a cut whose size is at least the size of any other cut. A polynomial-time algorithm to find maximum cuts in planar graphs exists.[3] ref. Hadlock, F. (1975), "Finding a Maximum Cut of a Planar Graph in Polynomial Time", SIAM J. Comput. 4 (3): 221–225, doi:10.1137/0204019.</item>

<item>Minimum cut into bounded sets, i.e. imposing restrictions on the size of the disjoint sets that partition the network.</item>





<item>Matching on hypergraphs, i.e. when more than two vertices are involved in a single edge. In fact, 3-dimensional matching is known to be NP-hard whereas bipartite matching (a.k.a. 2-dimensional matching) is in P.</item>

<item>The Bandwidth problem, which can be visualized as placing the vertices of a graph at distinct integer points along the x-axis so that the length of the longest edge is minimized. Such placement is called linear graph arrangement, linear graph layout or linear graph placement. e.g. the problem of placement of a set of standard cells in a singe row with the goal of minimizing the maximal propagation delay (which is assumed to be proportional to wire length).</item>

<item>Related: The pathwidth problem, with applications in VLSI design, graph drawing, and computational linguistics.</item>



<item>Clique cover (a.k.a. partition into cliques), i.e. determining whether the vertices of a graph can be partitioned into k cliques. </item>

<item>Feedback vertex set, i.e. a set of vertices whose removal leaves a DAG (a graph without cycles).</item>

<item>A feedback arc set (FAS) or feedback edge set is a set of edges which, when removed from the graph, leave a DAG.  A minimal feedback arc set has the additional property that, if the edges in it are reversed rather than removed, then the graph remains acyclic. Finding a small edge set with this property is a key step in layered graph drawing</item>

<item>Chromatic number / graph k-colorability / vertex coloring: assignment of labels traditionally called "colors" to elements of a graph subject to certain constraints. In its simplest form, it is a way of coloring the vertices of a graph such that no two adjacent vertices share the same color; this is called a vertex coloring. Using dynamic programming and a bound on the number of maximal independent sets, k-colorability can be decided in time and space O(2.445^n).[6] Using the principle of inclusion–exclusion and Yates’s algorithm for the fast zeta transform, k-colorability can be decided in time O(n2^n)[5] for any k. Faster algorithms are known for 3- and 4-colorability, which can be decided in time O(1.3289^n) [7] and O(1.7504^n),[8] respectively. Note that 2-colorable graphs are bipartite graphs and bipartite testing can be performed in linear time.

[5] Björklund, A.; Husfeldt, T.; Koivisto, M. (2009), "Set partitioning via inclusion–exclusion", SIAM Journal on Computing 39 (2): 546–563, doi:10.1137/070683933
[6] Lawler, E.L. (1976), "A note on the complexity of the chromatic number problem", Information Processing Letters 5 (3): 66–67, doi:10.1016/0020-0190(76)90065-X
[7] Beigel, R.; Eppstein, D. (2005), "3-coloring in time O(1.3289n)", Journal of Algorithms 54 (2)): 168–204, doi:10.1016/j.jalgor.2004.06.008
[8] Byskov, J.M. (2004), "Enumerating maximal independent sets with applications to graph colouring", Operations Research Letters 32 (6): 547–556, doi:10.1016/j.orl.2004.03.002
</item>

<item>Edge coloring: assigns a color to each edge so that no two adjacent edges share the same color. an edge coloring of a graph is just a vertex coloring of its line graph (a graph that represents the adjacencies between edges of G)</item>

<item>Face coloring of a planar graph assigns a color to each face or region so that no two faces that share a boundary have the same color. a face coloring of a planar graph is just a vertex coloring of its planar dual (a vertex for each plane region of G, and an edge for each edge in G joining two neighboring regions). </item>

<item>Achromatic number: Does a graph G have an achromatic number K or greater? i.e. is there a partition of V into k disjoint sets so that each V_i is an independent set.</item>

<item>Other graph partitioning problems: into triangles, into cliques, into isomorphic subgraphs, into Hamiltonian subgraphs, into forests, into perfect matchings... are also known NP-complete problems.</item>

</list>

<text>
Contrast subtle differences: Eulerian path (easy) vs. Eulerian cycle (hard); bipartite matching (easy) vs. hypergraph matching (hard); shortest paths (easy) vs. longest paths (hard); Hamiltonian path on a graph (hard) vs. on a DAG (easy); the minimum spanning tree (easy), which is the undirected variant of the feedback arc set problem (hard)...
</text>

<text>
Even when a problem is NP-complete, algorithms can be devised that solve them in practice, either by employing a systematic search on the problem space (e.g. employing backtracking or branch-and-bound algorithms with substantial pruning) or heuristic methods that might not find the optimal solution but often return a good-enough one (e.g. simulated annealing). Heuristics are also employed by approximation algorithms that can get reasonable solutions with guaranteed bounds; i.e. the solution they provide is related to a lower bound on the optimal solution and imposes limits on how badly the approximation algorithm might perform.
</text>

<list>

<item>Vertex cover: Greedy algorithm that chooses an edge at random and adds both intervening nodes to the cover. Since one of the incident vertices must always be on the minimum vertex cover, this simple algorithm efficiently returns a cover that is at most twice as large as the optimal cover.
</item>

<item>The Euclidean Traveling Salesman Problem: When the triangle inequality holds, we can approximate the TSP solution by employing minimum spanning trees. Such trees, which connect all the nodes in a graph, can be traversed in depth. That traversal provides a non-Hamiltonian cycle whose length is at most twice the optimal cycle (why? deleting any link from the cycle returns a Hamiltonian path, i.e. a tree, whose weight cannot be greater than that of the MST.
</item>

<item>Maximum DAG: The largest possible subset of edges so that the resulting subgraph is acyclic. Any permutation of nodes can be interpreted as a topological sorting of the graph. Removing either left-to-right or right-to-left edges, we keep at least half of the edges in the original graph, hence the solution contains at least half as many edges as the optimum.
</item>

</list>

<text>
Alternative heuristics might even get better results for particular instances of the above problems, albeit more complex heuristics do not necessarily guarantee better results. In fact, the result if often the opposite. It should also be noted that some clever postprocessing of the results provided by approximation algorithms might also be worthwhile, since it might improve the approximate solution without weakening the approximation bound.
</text>

</document>


<!--
<figure>
<title>Image example (default file type)</title>
<image scale="20" file="image/cover/0321127420"/>
</figure>

<figure>
<title>Another image example (specific file type)</title>
<image scale="20" file="image/cover/0321117425.jpg" type="jpg"/>
</figure>

<text>
Inline image:
</text>

<image scale="20" file="image/cover/0321117425.jpg" type="jpg"/>
-->

</document>


<document>
<tag>graph-algorithms-notes</tag>
<title>Bibliographic notes</title>


<text>
<cite>Skiena 2008</cite>, <cite>Kleinberg and Tardos 2005</cite>
</text>


<text>
<cite>Grama et al. 2003</cite> includes a chapter on parallel graph algorithms.
</text>

<text>
Network flow; monographs <cite>Ahuja et al. 1993</cite> <cite>Ford and Fulkerson 1962</cite>; surveys <cite>Goldberg et al. 1990</cite> <cite>Schrijver 2002</cite>; original algorithms <cite> Ford and Fulkerson 1956</cite> <cite>Edmonds and Karp 1972</cite>
</text>

<text>
NP problems:
- 1972: Landmark paper, "Reducibility Among Combinatorial Problems"  <cite>Karp 1972</cite>, 21 combinatorial and graph theoretical NP-complete problems
- 1979: Catalog containing hundreds of NP problems<cite>Garey and Johnson 1979</cite>, 65 graph theoretical problems and 51 network design problems.
- Wikipedia: More than 3000 known NP-complete problems, http://en.wikipedia.org/wiki/List_of_NP-complete_problems
</text>

</document>


<document>
<tag>graph-algorithms-references</tag>
<title>References</title>

<reference id="Ahuja et al. 1993">
 <author>Ravindra K. Ahuja</author>
 <author>Thomas L. Magnanti</author>
 <author>James B. Orlin</author>
 <title>Network Flows: Theory, Algorithms, and Applications</title>
 <publisher>Prentice Hall</publisher>
 <year>1993</year>
 <isbn>013617549X</isbn>
 <file>[books]/algorithms/graphs/013617549X [Ahuja] Network Flows.1993.zip</file>
</reference>

<reference id="Edmonds and Karp 1972">
 <author>Jack Edmonds</author>
 <author>Richard M. Karp</author>
 <title>Theoretical improvements in algorithmic efficiency for network flow problems</title>
 <journal>Journal of the ACM</journal>
 <volume>19</volume>
 <number>2</number>
 <pages>248-264</pages>
 <year>1972</year>
 <doi>10.1145/321694.321699</doi>
 <file>Networks/algorithms/flow/1972 JACM [Edmonds &amp; Karp] Network flow.pdf</file>
</reference>

<reference id="Ford and Fulkerson 1956">
 <author>Lester Randolph Ford, Jr.</author>
 <author>Delbert Ray Fulkerson</author>
 <title>Maximal flow through a network</title>
 <journal>Canadian Journal of Mathematics</journal>
 <volume>8</volume>
 <pages>399-404</pages>
 <year>1956</year>
 <file>Networks/algorithms/flow/1956 CJM [Ford &amp; Fulkerson] Maximal flow.pdf</file>
</reference>

<reference id="Ford and Fulkerson 1962">
 <author>Lester Randolph Ford, Jr.</author>
 <author>Delbert Ray Fulkerson</author>
 <title>Flows in Networks</title>
 <publisher>Princeton University Press</publisher>
 <year>1962</year>
 <isbn>0691079625</isbn>
 <file>[books]/algorithms/graphs/0691079625 [Ford &amp; Fulkerson] Flows in Networks.1962.djvu</file>
</reference>


<reference id="Gamma et al. 1994">
 <author>Erich Gamma</author>
 <author>Richard Helm</author>
 <author>Ralph Johnson</author>
 <author>John Vlissides</author>
 <title>Design Patterns: Elements of reusable object-oriented software</title>
 <publisher>Addison-Wesley</publisher>
 <year>1994</year>
 <isbn>0201633612</isbn>
 <file>[books]/design/patterns/0201633612 [GoF] Design Patterns.chm</file>
</reference>



<reference id="Garey and Johnson 1979">
 <author>Michael R. Garey</author>
 <author>David S. Johnson</author>
 <title>Computers and Intractability: A Guide to the Theory of NP-Completeness</title>
 <publisher>W.H. Freeman</publisher>
 <year>1979</year>
 <isbn>0716710455</isbn>
 <file>[books]/algorithms/0716710455 [Garey &amp; Johnson] Computers and Intractability.pdf</file>
</reference>


<reference id="Goldberg et al. 1990">
 <author>Andrew V. Goldberg</author>
 <author>Eva Tardos</author>
 <author>Robert E. Tarjan</author>
 <title>Network Flow Algorithms</title>
 <booktitle>Algorithms and Combinatorics 9</booktitle>
 <publisher>Springer-Verlag</publisher>
 <year>1990</year>
 <isbn>0387526854</isbn>
 <file>Networks/algorithms/flow/1990 AC Network Flow Algorithms.pdf</file>
</reference>


<reference id="Grama et al. 2003">
 <author>Ananth Grama</author>
 <author>Anshul Gupta</author>
 <author>George Karypis</author>
 <author>Vipin Kumar</author>
 <title>Introduction to Parallel Computing</title>
 <publisher>Addison Wesley</publisher>
 <edition>2nd edition</edition>
 <year>2003</year>
 <isbn>0201648652</isbn>
 <file>[books]/algorithms/0201648652 [Kumar] Parallel Computing.2nd.chm</file>
</reference>


<reference id="Karp 1972">
 <author>Richard M. Karp</author>
 <title>Reducibility among combinatorial problems</title>
 <booktitle>Complexity of Computer Computations</booktitle>
 <publisher>IBM Research Symposia Series, Plenum Press</publisher>
 <year>1972</year>
 <isbn>0306307073</isbn>
 <file>Networks/algorithms/1972 [Karp] Reducibility.pdf</file>
</reference>

<reference id="Kleinberg and Tardos 2005">
 <author>Jon Kleinberg</author>
 <author>Éva Tardos</author>
 <title>Algorithm Design</title>
 <publisher>Addison-Wesley</publisher>
 <year>2005</year>
 <isbn>0321295358</isbn>
 <file>[books]/algorithms/0321295358 [Kleinberg] Algorithm Design.pdf</file>
</reference>

<reference id="Lynch 1997">
 <author>Nancy A. Lynch</author>
 <title>Distributed Algorithms</title>
 <publisher>Morgan Kaufmann</publisher>
 <year>1997</year>
 <isbn>1558603484</isbn>
 <file>[books]/algorithms/1558603484 [Lynch] Distributed Algorithms.pdf</file>
</reference>

<reference id="Schrijver 2002">
 <author>Alexander Schrijver</author>
 <title>On the history of the transportation and maximum flow problems</title>
 <journal>Mathematical Programming</journal>
 <volume>91</volume>
 <number>3</number>
 <pages>437-445</pages>
 <year>2002</year>
 <doi>10.1007/s101070100259</doi>
 <file>Networks/algorithms/flow/2002 MP Transportation and network flow.pdf</file>
</reference>

<reference id="Skiena 2008">
 <author>Steven S. Skiena</author>
 <title>The Algorithm Design Manual</title>
 <publisher>Springer</publisher>
 <edition>2nd edition</edition>
 <year>2008</year>
 <isbn>1848000693</isbn>
 <file>[books]/algorithms/1848000693 [Skiena] The Algorithm Design Manual.2nd.pdf</file>
</reference>

</document>

</document>
