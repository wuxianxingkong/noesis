<?xml version="1.0"  encoding="ISO-8859-1" ?> 
<!DOCTYPE sys-ents [ 
  <!ENTITY bibliography   SYSTEM "bibliography.xml">
]> 
<?xml-stylesheet type="text/xsl" href="../book.xsl"?>

<!-- Minimum spanning trees -->

<document>
&bibliography;
<tag>graph-spanning-trees</tag>
<title>Minimum spanning trees</title>



SKELETONS: Algorithms for reducing a network to its skeleton
1) MST
2) Pathfinder algorithms

<text>
A spanning tree of a graph <eqn>G=(V,E)</eqn> is a subset of <eqn>n-1</eqn> edges from <eqn>E</eqn> that connect all the vertices in <eqn>V</eqn> and, therefore, form a tree. In weighted graphs, where edges have numerical values associated to them, known as weights, a minimum spanning tree is a spanning tree whose sum of edge values is as small as possible. A tree is always the smallest connected graph in terms of its number of edges (and also the most vulnerable to single failures), while a MST is the cheapest connected graph in term of edge costs.
</text>

<text>
The algorithms in this section can also be adapted to find maximum spanning trees (just negate the weights and the MST in the negated graph is the maximum spanning tree in the original graph), minimum product spanning trees (replace edge weights with their logarithms so that products are converted into sums), or minimum bottleneck spanning tree (i.e. a spanning tree that minimizes the maximum edge weight, by just employing Kruskal's algorithm). 
</text>

<text>
However, other related problems cannot be solved using the efficient algorithms described in this section:
- Steiner trees (when we can add intermediate vertices), NP-hard in general.
- Low-degree spanning tree (i.e. the minimum spanning tree whose maximum node degree is smaller; in case it where a simple path, this problem would be equivalent to finding a Hamiltonian path = TSP, NP-complete)
</text>

<text>
APPLICATIONS: 
- Whenever we need to connect a set of nodes with minimum cost (e.g. transportation networks, communication networks, distribution networks...). 
- Bottleneck spanning trees have applications when edge weights represent costs, capacities, or strengths
- Clustering (e.g. hierarchical clustering)
- They can be used to obtain approximate solutions to hard problems (Steiner trees or TSP below)
</text>


<text>
In unweighted graphs (or weighted graphs where all edges have the same cost), any tree is also a minimum spanning tree. Its cost will always be <eqn>n-1</eqn> times the cost of a single edge. In this case, a spanning tree can easily be found by performing any systematic graph traversal (e.g. DFS or BFS).
</text>

<text>
In weighted graphs, efficient greedy algorithms exist than can be employed to identify minimum spanning trees...
</text>

<text>
Note: If the graph is not connected, it cannot have a spanning tree but it has a spanning forest. MST algorithms can be easily modified to output minimum spanning forests if needed.
</text>

<document>
<tag>graph-spanning-trees-greedy</tag>
<title>Greedy MST algorithms</title>


<text>
@ MIT Graphs - Shortest paths
</text>

<text>
Greedy algorithms choose what to do next by selecting the best local candidate to be added to the solution and, when the optimality of the chosen heuristics can be proved, they provide efficient algorithms for solving problems such as finding the minimum spanning tree.
</text>

<text>
Candidates: 

- Edges (Kruskal's algorithm <cite>Kruskal 1956</cite>): Each vertex starts as a separate tree and separate trees are merged by adding the lowest-cost edge to the MST that does not create a cycle (i.e. connects two separate subtrees)
</text>

<code>
Kruskal(G):
Sort edges in order of increasing weight
edges_in_tree = 0
while edges_in_tree &lt; n-1
    get next edge (v,w)
    if component(v) != component(w)
       Add edge (v,w) to the MST
       component(v) = component(w)
       edges_in_tree++
</code>

<text>
- Nodes (Prim's algorithm <cite>Prim 1957</cite>, rediscovered by <cite>Dijkstra 1959</cite>): Start with an arbitrary vertex and iteratively grow the tree by adding the lowest-cost edge that links some new vertex to the tree.
</text>

<code>
Prim(G):
Select an arbitrary vertex
while...
</code>

<text>
- The oldest algorithm: Boruvka's algorithm, published in 1926 <cite>Boruvka 1926</cite> and rediscovered by Choquet in 1938 <cite>Choquet 1938</cite>

An MST algorithm that does not require a priority queue for its efficient implementation:

T initially a subgraph containing just the vertices in V
while tree T has less than n-1 edges
	for each connected component C_i of T
		Find the smallest-weight edge (uv) in E with u in C_i and v not in C_i
		Add e to T 
return T



Since the lowest-weight edge incident on each vertex must be in the minimum spanning tree, the union of these edges will return a forest of at most n/2 trees. For each tree, select the edge (x,y) of lowest weight such that <eqn>x \in T</eqn> and <eqn>y \notin T</eqn>, which will also have to belong to the MST. Repeat until all vertices are connected. Each iteration at least halves the number of remaining trees, hence, after log n iterations, each taking linear time, we obtain the MST in O(m log n).

Implementation details

1) Maintain the forest T using adjacency lists to support O(1) edge insertion
2) Traverse the forest T to identify connected components in O(n) using DFS
3) Mark vertices with the ID of the connected component they belong to (extra variable for each vertex)
4) Identify the smallest-weight edge, just by scanning the adjacency lists for the vertices in C_i

</text>


<text>

- An inverse algorithm would also be possible...
  Algoritmo de  Algoritmo de borrado borrado inverso inverso::
  Comenzando con  Comenzando con T=A,  T=A, considerar considerar laslas aristas aristas en  en orden orden decreciente decreciente
  de coste de coste y y eliminar eliminar laslas aristas aristas de T salvo  de T salvo que que esoeso desconectase desconectase T.T.

</text>


<text>
MSTs are unique when all m edge weights are distinct. The way ties are broken leads to different MSTs...
</text>

<document>
<tag>graph-spanning-trees-greedy-prim</tag>
<title>Prim's algorithm</title>

<text>
Starts from any node and adds one edge at a time until all the vertices are connected... heuristics: select the edge with the smallest weight that will enlarge the number of vertices in the tree
</text>

<text>
No cycles are created since only edges between tree and non-tree nodes are added to the solution
</text>

<text>
EFFICIENCY:
<eqn>n</eqn> iterations through <eqn>m</eqn> edges = <eqn>O(mn)</eqn> using adjacency lists, 
<eqn>O(n^2)</eqn> using a weighted adjacency matrix.
</text>

<text>
PROOF? by contradiction
</text>

<text>
Tree reconstruction: parent vector or dynamic structure...
</text>

</document>

<document>
<tag>graph-spanning-trees-greedy-kruskal</tag>
<title>Kruskal's algorithm</title>

<text>
Instead of starting on any particular node and growing a tree, Kruskal's algorithm builds the minimum spanning tree by connecting sets of vertices...
</text>

<text>
Initially: each vertex a separate component + Iteration: consider the least cost edge and test whether it connects two separate components (if its both endpoints are already in the same connected component, the edge is discarded, since it would create a cycle; if not, it is added to the MST and the components it connects are merged).
</text>

<text>
Since each connected component is always a tree, we do not have to check for cycles
</text>

<text>
EFFICIENCY:
Sorting edges <eqn>O(m \log m)</eqn> + <eqn>m</eqn> iterations / testing connectivity by BFS/DFS <eqn>O(n)</eqn> = <eqn>O(mn)</eqn>
</text>

<text>
PROOF? by contradiction
</text>

</document>

</document>


<document>
<tag>graph-spanning-trees-sequential</tag>
<title>Sequential implementation of MST algorithms</title>

<text>
IDEA: Efficient implementation for sparse networks based on adjacency lists
- The complexity of algorithms that work on the adjacency matrix is <eqn>\Omega(n^2)</eqn>, no matter how dense or sparse the network is.
- In contrast, the complexity of algorithms that work on adjacency lists is usually <eqn>\Omega(n+m)</eqn>.
</text>

<text>
Naive implementation:
<eqn>O(mn)</eqn> for Kruskal's algorithm 
<eqn>O(mn)</eqn> for Prim's algorithm 
(i.e. <eqn>O(n^3)</eqn> for dense graphs, <eqn>O(n^2)</eqn> for sparse graphs when adjacency lists are used)
</text>



<text>
Prim's algorithm:
1) Maintaining a boolean flag to indicate whether a vertex is already in the tree or not, we can test whether the current edge links to a non-tree vertex in constant time, leading to a <eqn>O(n^2)</eqn> algorithm (both for dense and sparse graphs).
2) Optimization: <eqn>O(n \log k)</eqn> on graphs that have only <eqn>k</eqn> different edge costs.
3) The use of priority queues can lead to a more efficient implementation, by making it faster to find the minimum cost edge at each iteration.
- <eqn>O(m \log n)</eqn> using standard binary heaps.
- <eqn>O(m + n \log n)</eqn> using Fibonacci <cite>Fredman and Tarjan 1987</cite> or pairing heaps <cite>Stasko and Vitter 1987</cite>.

i.e. the version using binaryheaps outperforms the standard algorithm as long as <eqn>m \in O(n^2 / \log n)</eqn>
</text>

<text>
Kruskal's algorithm:
1) Union-find data structure for implementing the component test in <eqn>O(\log n)</eqn> = <eqn>O(m \log m)</eqn> time (i.e. needed to sort the edges).
+ Typically faster than Prim's algorithm for sparse graphs
</text>


<text>
Union-find data structure for representing set partitions (a partitioning of elements, say nodes, intro a collection of disjoint subsets):
1) Union: Component merging in <eqn>O(\log n)</eqn>
2) Find: Same component check in <eqn>O(\log n)</eqn>

Solution: Represent each component as a backwards tree, with pointers from a node to its parent (choosing how to merge in order to limit the height of the trees and reduce the effect of unbalanced trees: the smaller tree will always be a subtree of the larger tree; why? height of the nodes in the larger tree stay the same, heights in the smaller tree are increased by one)

Result: We must double the number of nodes to increase the height of the resulting tree, hence the <eqn>O(\log n)</eqn> result
</text>

<text>
Set data structures to represent unordered collections of objects. Representing subsets to efficiently test element <eqn>\in</eqn> subset, compute union/intersection of subsets, or insert/delete subset members,

1) Bit vectors: Space efficient, insertion/deletion by toggling single bits, union/intersection by or-ing/and-ing bit vectors.

2) Bloom filters <cite>Broder and Mitzenmacher 2005</cite> use hashing.

3) Dictionaries, which can be viewed as sets (no duplicate keys).

4) Standard collections, which can be viewed as multisets (may have multiple occurrences of the same element).

Set partitions: Pairwise-disjoint subsets, when each element is exactly in one subset...

1) Collection of sets (as a generalized bit vector, a collection of collections, or a dictionary with a subset sttribute): Costly union/intersection operations.

2) Union-find data structure *
++ Union-find can be done even faster: 
- Optimization: Path compression. Retraversing the path on each find operation and explicitly pointing all nodes to the root leads to almost constant-height trees.
- Limitation: Does not support breaking up unions
- Upper bound on m union-find operations on an n-element set: <eqn>O(m \alpha(m, n))</eqn>, where alpha is the inverse Ackermann function, which grows notoriously slowly, and leads to an almost-linear performance. <cite>Tarjan 1979</cite>.


NOTE:
Efficient implementation of basic graph algorithms led to the discovery of several data structures, such as the "union-find"
structure <cite>Tarjan 1979</cite> and Fibonacci heaps <cite>Fredman and Tarjan 1987</cite>. These data structures have been used to speed up many other algorithms.

</text>

<text>
A combination of Boruvka's algorhtm <cite>Boruvka 1926</cite> with Prim's algorithm <cite>Prim 1957</cite> yields an O(m log log n) algorithm. Running log log n iterations of Boruvka's algorhtm yields at most n/log n trees. Then create a graph G' with a vertex for each tree and an edge whose weight corresponds to the lightest edge between <eqn>T_i</eqn> and <eqn>T_j</eqn>. The MST of G' combined with the edges selected by  Boruvka's algorhtm yields the MST of G. Since Prim's algorithm will take O(n+m) time on this n/log n vertex, m edge graph (if implemented using Fibonacci or pairing heaps). <cite>Skiena 2008</cite>. Even tighter bounds, e.g. <eqn>O(n \alpha(m,n))</eqn>, can be achieved <cite>Karger et al. 1995</cite> <cite>Chazelle 2000</cite> <cite>Pettie and Ramachandran 2002</cite> <cite>Pettie and Ramachandran 2008</cite>.
</text>

<text>
Dynamic graph algorithms are incremental algorithms that maintain graph invariants (such as MSTs) under edge insertion and deletion operations. <cite>Holm et al. 2001</cite> describes and efficient algorithm to maintain MSTs (and several other invariant) in amortized polylogarithmic time per update.
</text>


</document>

<document>
<tag>graph-spanning-trees-parallel</tag>
<title>Parallel MST algorithms</title>

<text>
Parallel Prim's algorithm
- for dense graphs @ Section 10.2 <cite>Grama et al. 2003</cite>
- for sparse graphs @ Section 10.7.2 <cite>Grama et al. 2003</cite>
+ Bibliographic notes
</text>


<text>
Prim's and Kruskal's algorithms are iterative (each iteration adds a new vertex or edge to the minimum spanning tree), so different iterations of their main loop cannot be performed in parallel. However, each particular iteration can be parallelized:

- p processors, n vertices: V is partitioned into p subsets using block-striped partitioning (each block containing n/p consecutive vertices)

- Processor P_i stores the data corresponding to the vertex set V_i

- Prim's algorithm: Each processor computers d_i(u) = min { d_i(v) | v \in (V-V_T) \cap V_i } and the global minimum is obtained by single-node accumulation. P_0 will then know which node u to add to the minimum spanning tree and will broadcast it to all processors. The processor P_i responsible for node u will mark it as belonging to the set V_T and each processor will update the values of d[v] for its local vertices.

- Each processor needs w(u,v) for the vertices v \in V_i, so it needs to store the columns of the adjacency matrix corresponding to the vertices it has been assigned. Space requirements \Theta(n/p) using an adjacency matrix, O((n+m)/p) for adjacency lists (if the load is distributed evenly among the different processors).


Efficiency (for dense graphs, using their adjacency matrix)

1) Computation: \Theta(n/p)

2) Single-node accumulation = all-to-one reduction (global minimum): \Theta(log p) @ hypercube network, \Theta(\sqrt{p}) @ 2D mesh network, \Theta(p) @ bus and ring networks 

3) One-to-all broadcast: \Theta(log p) @ hypercube network, \Theta(\sqrt{p}) @ 2D mesh network, \Theta(1) @ bus network, \Theta(p) @ ring network

Overall

1) Computation: \Theta(n^2/p)

2) Communication: 
- \Theta(n \log p) @ hypercube
- \Theta(n \sqrt{p}) @ 2D mesh
- \Theta(n p) @ bus/ring


Hypercube: 
t_p = \Theta \left ( \frac{n^2}{p} + n \log p \right ) 

Mesh:
t_p = \Theta \left ( \frac{n^2}{p} + n \sqrt{p} \right ) 

Bus/ring:
t_p = \Theta \left ( \frac{n^2}{p} + n p \right ) 




Since the sequential run time is W=\Theta(n^2),

speedup

S_{hypercube} \in \Theta \left ( \frac{n}{n/p + \log p} \right ) 

S_{mesh} \in \Theta \left ( \frac{n}{n/p + \sqrt{p}} \right ) 

S_{bus} \in \Theta \left ( \frac{n}{n/p + p} \right ) 


efficiency

E_{hypercube} = \frac{1}{1 + \Theta ( (p \log p) / n ) }

E_{mesh} = \frac{1}{1 + \Theta ( (p^{1.5} / n ) }

E_{bus} = \frac{1}{1 + \Theta ( (p^2 / n ) }

i.e. 
hypercube: a cost optimal parallel implementation requires (p \log p) / n  \in O(1), hence the hypercube implementation of Prim's algorithm can use only p=O(n \log n) different processors.  Isoefficiency function \Theta(p^2 \log p^2) for hypercube networks

tp = n^2/p + n log(p)
ts = n^2 = W

to = p*tp - W = n^2 + np log(p) - n^2 = n p log(p)

E = 1 / (1 + to/W)

Communication overhead: p^2 log^2(p)
	tc = n p log (p)
	W  = n^2
	W = K tc  ===  n^2 = n p log(p)  ===  n = p log(p)  === W=n^2= p^2 log^2(p)
+
Concurrency: p^2
	Since n must grow at least as fast as p... W = n^2 = p^2 
=
Isoefficiency: \Theta(p^2 \log p^2)



mesh: a cost optimal parallel implementation requires p^{1.5}/n  \in O(1), hence the mesh implementation of Prim's algorithm can use only p=O(n^{2/3}) different processors. Isoefficiency function \Theta(p^3) for meshes

tp = n^2/p + n sqrt(p)
ts = n^2 = W

to = p*tp - W = n^2 + np sqrt(p) - n^2 = n p sqrt(p)

E = 1 / (1 + to/W)

Communication overhead: p^3
	tc = n p sqrt (p)
	W  = n^2
	W  = K tc  ===  n^2 = K n p log(p)  ===  n = K p log(p)  ===  W = n^2 = K^2 p^2 p = K^2 p^3
+
Concurrency: p^2
	Since n must grow at least as fast as p... W = n^2 = p^2 
=
Isoefficiency: \Theta(p^3)



ring/bus: a cost optimal parallel implementation requires p^{2}/n \in O(1), hence the bus implementation of Prim's algorithm can use only p=O(\sqrt{n}) different processors. Isoefficiency function: \Theta(p^4) for bus

tp = n^2/p + np
ts = n^2 = W

to = p*tp - W = n^2 + np^2 - n^2 = np^2

to = p*tp - W = n^2 + np sqrt(p) - n^2 = n p sqrt(p)

E = 1 / (1 + to/W)

Communication overhead: p^3
	tc = n p^2
	W  = n^2
	W  = K tc  ===  n^2 = K n p^2  ===  n = K p^2  ===  W = n^2 = K^2 p^4
+
Concurrency: p^2
	Since n must grow at least as fast as p... W = n^2 = p^2 
=
Isoefficiency: \Theta(p^4)



Using a less naive approach, both the all-to-one reduction and the one-to-all broadcast take \Theta(\log p) time for a p-process message-passing parallel computer... hence the same results obtained for hypercube interconnection networks can be obtained for more conventional message-passing parallel computers. !!!

</text>


<dataset>
 <title>Performance of Prim's algorithm on different parallel architectures</title>
 <metadata>
  <field width="25">Architecture</field>
  <field width="25">Maximum number of processors for <eqn>E \in \Theta(1)</eqn></field>
  <field width="25">Parallel run time</field>
  <field width="25">Isoefficiency function</field>
 </metadata>
 <record>
  <field>Hypercube</field>
  <field><eqn>\Theta(n / \log n)</eqn></field>
  <field><eqn>\Theta(n \log n)</eqn></field>
  <field><eqn>\Theta(p^2 \log^2 p)</eqn></field>
 </record>
 <record>
  <field>2D Mesh</field>
  <field><eqn>\Theta(n^{2/3})</eqn></field>
  <field><eqn>\Theta(n^{4/3})</eqn></field>
  <field><eqn>\Theta(p^3)</eqn></field>
 </record>
 <record>
  <field>Ring / Bus</field>
  <field><eqn>\Theta(\sqrt{n})</eqn></field>
  <field><eqn>\Theta(n^{3/2})</eqn></field>
  <field><eqn>\Theta(p^4)</eqn></field>
 </record>
</dataset>

<text>
The performance of the parallel version of a sequential algorithm for dense networks:
- each processor performs roughly the same amount of work
- communication is localized (i.e. limited)
</text>

<text>
It is difficult to achieve an even work load distribution among processors and a low communication overhead for sparse graphs
- equal number of vertices? random number of edges incident on a given vertex, leading to work imbalance among processors
- equal number of edges? adjacency lists might be splitted among different processors and communication overhead might dramatically increase

Hard problem for arbitrary networks, albeit efficient parallel algorithms can be devised if the sparse graph has some structure (e.g. grid graphs, finite element meshes, planar graphs such as maps).
</text>



</document>

<document>
<tag>graph-spanning-trees-distributed</tag>
<title>Distributed MST algorithms</title>

<text>
Distributed MST algorithms @ MIT 6.852: Distributed Algorithms - Lecture 4 (synchronous) + Lecture 9 (asynchronous) / Section 4.4 + Section 15.5 <cite>Lynch 1997</cite>
</text>

<text>
<cite>Gallager et al. 1983</cite>: At most 5n log n + 2m messages...
</text>

</document>


<document>
<tag>graph-spanning-trees-notes</tag>
<title>Bibliographic notes</title>

From <cite>Grama et al. 2003</cite>:

- Sequential minimum spanning tree algorithm due to Prim [Pri57], can be also computed by using either Kruskal's [Kru56] or Sollin's [Sol77] sequential algorithms. 

[Kru56] J. B. Kruskal. On the shortest spanning subtree of a graph and the traveling salesman problem. In Proceedings of the AMS, volume 7, 48–50, 1956.

[Pri57] R. C. Prim. Shortest connection network and some generalizations. Bell Systems Technical Journal, 36:1389–1401, 1957.


- Sollin's (???) algorithm [Sol77] starts with a forest of n isolated vertices. In each iteration, the algorithm simultaneously determines, for each tree in the forest, the smallest edge joining any vertex in that tree to a vertex in another tree. All such edges are added to the forest. Furthermore, two trees are never joined by more than one edge. This process continues until there is only one tree in the forest – the minimum spanning tree. Since the number of trees is reduced by a factor of at least two in each iteration, this algorithm requires at most log n iterations to find the MST. Each iteration requires at most O (n^2) comparisons to find the smallest edge incident on each vertex; thus, its sequential complexity is Q(n^2 log n). 

[Sol77] M. Sollin. An algorithm attributed to Sollin. In S. Goodman and S. Hedetniemi, editors, Introduction to The Design and Analysis of Algorithms. McGraw-Hill, Cambridge, MA, 1977.



- Bentley [Ben80] and Deo and Yoo [DY81] present parallel formulations of Prim's MST algorithm. Deo and Yoo's algorithm is suited to a shared-address-space computer. It finds the MST in Q(n^{1.5}) using Q(n^{0.5}) processors. Bentley's algorithm works on a tree-connected systolic array and finds the MST in time Q(n log n) using n/log n processors. The hypercube formulation of Prim's MST algorithm is similar to Bentley's algorithm.

[Ben80] J. L. Bentley. A parallel algorithm for constructing minimum spanning trees. Journal of the ACM, 27(1):51–59, March 1980.

[DY81] N. Deo and Y. B. Yoo. Parallel algorithms for the minimum spanning tree problem. In Proceedings of the 1981 International Conference on Parallel Processing, 188–189, 1981.



Parallel algorithms based on Sollin's sequential algorithm include:
- Savage and Jaja's Q(log^2 n) algorithm using n^2 processors for the CREW PRAM [SJ81], 
- Chin, Lam, and Chen's Q(log^2 n) algorithm using n \lceil n/\log n \rceil processors for the CREW PRAM [CLC82]
- Awerbuch and Shiloach's Q(log^2 n) algorithm using Q(n^2) for the shuffle-exchange network [AS87]
- Doshi and Varman's Q(n2/p) time algorithm for a p-process ring-connected computer [DV87]
- Leighton's Q(log2 n) algorithm for an n x n mesh of trees network [Lei83]
- Nath, Maheshwari, and Bhatt's Q(log4 n) algorithm for an n x n mesh of trees network [NMB83]
- Huang's Q(n^2/p) for a \sqrt{n} x \sqrt{n} mesh of trees [Hua85].

[SJ81] C. Savage and J. Jaja. Fast, efficient parallel algorithms for some graph problems. SIAM Journal of Computing, 10(4):682–690, November 1981.

[CLC82] F. Y. Chin, J. Lam, and I. Chen. Efficient parallel algorithms for some graph problems. Communications of the ACM, 25(9):659–665, September 1982.

[AS87] B. Awerbuch and Y. Shiloach. New connectivity and MSF algorithms for shuffle-exchange network and PRAM. IEEE Transactions on Computers, C– 36(10):1258–1263, October 1987.

[DV87] K. A. Doshi and P. J. Varman. Optimal graph algorithms on a fixed-size linear array. IEEE Transactions on Computers, C–36(4):460–470, April 1987.

[Lei83] F. T. Leighton. Parallel computation using mesh of trees. In Proceedings of International Workshop on Graph-Theoretic Concepts in Computer Science, 1983.

[NMB83] D. Nath, S. N. Maheshwari, and P. C. P. Bhatt. Efficient VLSI networks for parallel processing based on orthogonal trees . IEEE Transactions on Computers, C–32:21–23, June 1983.

[Hua85] M. A. Huang. Solving some graph problems with optimal or near-optimal speedup on mesh-of-trees networks. In Proceedings of the 26th Annual IEEE Symposium on Foundations of Computer Science, 232–340, 1985.



PATHFINDER ALGORITHMS [SAGE p. 552]

</document>


</document>
