<?xml version="1.0"  encoding="ISO-8859-1" ?> 
<!DOCTYPE sys-ents [ <!ENTITY bibliography SYSTEM "bibliography.xml"> ]> 
<?xml-stylesheet type="text/xsl" href="../book.xsl"?>


<document>
&bibliography;
<title>Graph Algorithms</title>


<text>
<q>The key to solving many algorithmic problems is to think of them in terms of graphs... it is amazing how often messy applied problems have a simple description and solution in terms of classical graph properties</q> <cite>Skiena 2008</cite>.
</text>


<text>
0. Propaedeutic
1. Graph representation
2. Graph exploration
3. Optimization problems: MST, shortest paths, maximum flow
4. Hard problems, e.g. graph isomorphisms
5. A catalog of graph algorithms
</text>


<!-- Propaedeutic -->

<document>
<tag>graph-representation</tag>
<title>Propaedeutic</title>

<text>
An introduction to an art and science of algorithm analysis and design... as a prerrequisite for later sections.
</text>

<document>
<title>A primer on algorithm analysis</title>

<text>
Big-Oh notation...
</text>

<text>
Worst-case analysis...
</text>

<text>
Amortized analysis bounds the total amount of time used by any sequence of operations (e.g. coffee @ office or coffee shop). Used when single operations might prove costly but their cost is distributed among many fast operations... O(f(n)) in amortized analysis is worse than O(f(n)) in the worst case, but it might still be useful in practice. Provided that individual response time is not critical, amortized operations might help obtain a good throughput...
</text>

<text>
The dominance pecking order in algorithm analysis <cite>Skiena 2008</cite>:
</text>

<equation>
n! >> c^n >> n^3 >> n^2 >> n^{1+\epsilon} >> n \log n 
</equation>

<equation>
>> n >> sqrt(n) >> \log^2 n >> \log n
</equation>

<equation>
>> \log n / \log \log n >> \log \log n >> \alpha(n) >> 1
</equation>

<list>
<item><eqn>\alpha(n)</eqn>: Inverse Ackerman's function, which grows notoriously slowly (union-find data structure).</item>
<item><eqn>\log n / \log \log n</eqn>: Height of an n-leaf tree of degree <eqn>d = \log n</eqn>.</item>
<item><eqn>n^{1+\epsilon}</eqn>, e.g. <eqn>2^c*n^{1+1/c}</eqn> where <eqn>c</eqn> cannot be arbitrarily large (<eqn>2^c!</eqn>).</item>
</list>

<text>
++ polylogarithmic, polylogarithmic function in n is a polynomial in the logarithm of n,
a_k \log^k (n) + ... + a_1 \log(n) + a_0

</text>

</document>


<document>
<title>Fundamental data structures</title>


<text>
The importance of data structures @ <cite>Skiena 2008</cite>, chapter 3, page 65: <q>Changing the data structure does not change the correctness of the program, since we presumably replace a correct implementation with a different correct implementation. However, the new implementation of the data type realizes different tradeoffs in the time to execute various operations, so the total performance can improve dramatically</q>.
</text>

<text>
3 fundamental data structures, provided as ADTs whose implementation might vary: containers, dictionaries, and priority queues.

DESIGN IDEA: Isolate the interface of the data structure from its particular implementation, so that it can be changed if needed.
</text>


<document>
<title>Collections</title>

<text>
The base container in any collection framework...
</text>

<text>
- Unsorted linked lists or arrays: sequential vs. random access; memory fragmentation and cache performance (spatial locality)
- Sorted linked lists or arrays (-maintenance costs, only appropriate if there are not too many insertions and deletions, +logical deletions, +easy duplicate elimination, +binary search)
- Self-organizing lists: every time a key is accessed, the corresponding object is moved to the head of the list (temporal locality)
- Dynamic arrays (e.g. Java)
</text>

<text>
Standard collection frameworks: C++ Standard Template Library (STL), Java Collections Framework (JCF)...
</text>

</document>


<document>
<title>Dictionaries</title>

<text>
Managing collections of objects than can be identified by keys...
</text>

<text>
KEY OPERATIONS: Efficiently locate, insert, and delete the object associated to a particular key.
</text>

<text>
Multiple data structures have been used for implementing dictionaries, including different incarnations of hash tables and balanced trees:
</text>

<list>

<item>
Hash tables (a function maps keys to integers, which indicate positions within a collection). Well-tuned = O(1). Warning! Check its performance (it can slow down your application if not properly configured).
</item>

<item>
Binary and n-way search trees: O(log n) operations. Unbalanced search trees can degenerate into linked lists, hence rebalancing mechanisms are recommended (AVL-trees, red-black trees, and splay trees; B-trees for secondary storage). Splay trees, for instance, move any accessed key to the top of the tree, in order to exploit locality of reference (as self-organizing lists). B-trees collapse several levels of a BST into a single node in order to benefit from the system-specific page size (or block size in secondary storage) thus reducing cache misses, swapping, or disk accesses.  
</item>

<item>
Skip lists: A modern data structure that is easier to implement than balanced trees. Skip lists consist of a hierarchy of sorted linked lists. Roughly ln n lists, each one half as large as the one above it, support O(ln n) queries using amortized analysis.
</item>

</list>

<text>
Choosing the proper alternative: access patterns (e.g. relative frequencies of insert, delete, and search operations: hash table vs. sorted list). Efficient implementations always try to minimize the number of block data transfers. The so-called cache-oblivious data structures <cite>Frigo et al. 1999</cite> offer performance guarantees without explicit knowledge of the particular block size.
</text>

</document>


<document>
<title>Priority queues</title>

<text>
Managing collections of objects than can be identified by ordered keys...
</text>

<text>
KEY OPERATION: Efficiently access to the object corresponding to the smallest or largest key.
</text>

<text>
Objects are retrieved by their key priority, rather than by their insertion time (as happens with LIFO stacks or FIFO queues) or their key value (as in dictionaries).
</text>

<text>
When no insertions and deletions are performed, a sorted collection suffices. When such operations are mixed with queries, full-fledged priority queues can be implemented using:
</text>

<list>

<item>Binary heaps support insertion and minimum extraction in O(log n) time, but no deletion. Internally, they are implemented using arrays as implicit binary trees where the key of the root of every subtree is always lower than the key of all its descendants. Hence, the minimum is always readily avaiable at the root of the tree.</item>

<item>Fibonacci heaps <cite>Fredman and Tarjan 1987</cite> are unbalanced heaps designed to speed up decrease-key operations (i.e. when the priority of an item in the priority queue is reduced). They support insertion and decrease-key operations in constant amortized time. Applications: fasteer implementations of shortest paths, weighted bipartite matching, and minimum spanning trees.</item>

<item>Pairing heaps <cite>Stasko and Vitter 1987</cite> report the same bounds than Fibonacci heaps with less overhead associated to their maintenance.</item>

<item>Binary search trees can be used as priority queues, since the smallest element is always at the leftmost tree leaf, and they support extraction, insertion, and deletion in logarithmic time when balancing mechanisms are employed.</item>

<item>Bounded-height priority queues: The right priority queue implementation for small ranges of keys. When keys have n different values, say 1..n, we keep an array of n linked lists and a pointer to the smallest non-empty list. Good enough in practice (using amortized analysis), do not guarantee good worst-case behavior. Useful, e.g., for maintaining graph nodes ordered by degree.</item>

<item>Emde Boas priority queues <cite>van Emde Boas et al. 1977</cite> support O(log log n) insertion, deletion, search, max, and min operations when the key is a value from 1 to n.</item>
</list>

<text>
ISSUES:

- Changing priorities and specific operations...

- There is a linear-time merging algorithm for heap construction <cite>Floyd 1964</cite>; 1.625n comparisons are sufficient <cite>Gonnet and Munro 1986</cite> and 1.5n-O(log n) comparisons are necessary <cite>Carlsson and Chen 1992</cite>.
</text>

<text>
Example application: Simulation (a queue of forthcoming events is kept.
</text>


</document>


</document>

</document>

<!-- Graph representation -->

<document>
<tag>graph-representation</tag>
<title>Graph representation</title>

<text>
Graphs <eqn>G=(V,E)</eqn> of order <eqn>n</eqn> (number of vertices or nodes) and size <eqn>m</eqn> (number of edges or links)
</text>


<document>
<tag>graph-representation-matrix</tag>
<title>Adjacency matrix</title>

<text>
The most elementary representation of a graph is the adjacency matrix, also known as the connection matrix.
</text>

<text>
Using a <eqn>n \times n</eqn> square matrix <eqn>A</eqn>, where <eqn>A[i][j]=1</eqn> if there is a link from node i to node j, <eqn>A[i][j]=0</eqn> otherwise.
</text>

<text>
+ Easy to check whether a link exists or not, addition/removal of links: <eqn>O(1)</eqn> operations.
</text>

<text>
- Wasted space, specially for sparse networks: <eqn>O(n^2)</eqn> space vs. <eqn>O(n)</eqn> actual size 
- impractical for large networks unless an alternative representation scheme is employed...

Only for small and or very dense graphs!

- Efficient implementation: bit vectors to represent binary/unweighted graphs.
</text>

<text>
Incidence matrix: An alternative representation scheme, where we use a <eqn>n \times m</eqn> square matrix <eqn>I</eqn>, where <eqn>I[i][j]=1</eqn> if node i is involved in link j, <eqn>I[i][j]=0</eqn> otherwise. Incidence matrices might be useful for representing hypergraphs, i.e. generalized graphs where each edge can link subsets of more then two nodes (on standard graphs, there will be just two non-zero entries in each column).
</text>
</document>


<document>
<tag>graph-representation-lists</tag>
<title>Adjacency lists</title>

<text>
Linked lists are employed to store the neighbors of each node. 
i.e. a directed link (x,y) will appear in x's adjacency list.
i.e. undirected edges (x,y) will appear twice: in both x's and y's adjacency lists.
+ Implemented using pointers/references or dynamic arrays (to reduce memory fragmentation)
</text>

<text>
+ Faster computation of node degrees: <eqn>O(d)</eqn> (vs. <eqn>O(n)</eqn> for adjacency matrix). NOTE: Can be precomputed
</text>

<text>
+ Slower check of edge existence, insertion/deletion of edges: <eqn>O(d)</eqn> (vs. <eqn>O(1)</eqn> for adjacency matrix)

NOTE: Most algorithms can be easily designed so that they do not need to perform such operations, just by scanning all the links in each adjacency list (e.g. see breadth-first and depth-first traversal below).
</text>

<text>
+ Lower memory requirements: <eqn>O(n+m)</eqn> (vs. <eqn>O(n^2)</eqn> for adjacency matrix)
</text>

<text>
+ Faster graph traversal: <eqn>O(n+m)</eqn> (vs. <eqn>O(n^2)</eqn> for adjacency matrix)
</text>

<text>
+ Efficient implementation: dynamic arrays rather than linked lists (removing unnecessary pointers)
</text>

<text>
Preferred choice for most problems: Adjacency lists for sparse graphs, adjacency matrix only for dense graphs (uncommon in data mining problems).
</text>


<text>
Bipartite incidence structures are to adjacency lists as incidence matrices to adjacency matrices and, as incidence matrices, can be used to represent hypergraphs (i.e. generalized graphs where each edge can link subsets of more then two nodes). The incidence structure contains a vertex for each node and link in the hypergraph. An edge (i,j) will appear in the incidence structure if link j is incident on node i in the hypergraph.
</text>



<note>
<text>
Algorithmic libraries often provide general-purpose implementations of graph data structures, hence there is no need to implement them yourself unless your problem imposes very specific requirements, e.g. C++
</text>

<list>

<item>LEDA (Library of Efficient Data types and Algorithms), <url>http://www.algorithmic-solutions.com/</url>, originally from the Max Planck Institute for Informatics in Saarbrücken, Germany; since 2001, maintained by the Algorithmic Solutions Software GmbH.</item>

<item>Boost Graph Library (BGL), <url>http://www.boost.org</url>.</item>

<item>MatlabBGL, <url>http://www.cs.purdue.edu/homes/dgleich/packages/matlab_bgl/</url>, for Matlab: BGL port.</item>

<item>Combinatorica, <url>http://www.combinatorica.com</url>, for Mathematica, by Steven Skiena.</item>

<item>JUNG, <url>http://jung.sourceforge.net/</url>, Java graph library</item>

<item>Stanford Graphbase, implemented in CWEB, a literate version of the C programming language</item>
</list>

<text>
Implementation issues:
- Attribute representation... as fields in link/node records
- Large networks... hierarchical representation (vertices cluster subgraphs), e.g. VLSI circuits using cell libraries
</text>
</note>

</document>


<document>
<tag>graph-representation-compression</tag>
<title>Graph compression</title>

<text>
++ Compression (Liu, "Web Data Mining", 232-242)
++ SNAP
</text>

</document>

</document>



<!-- Graph exploration -->

<document>
<tag>graph-exploration</tag>
<title>Graph exploration</title>

<text>
A fundamental graph problem: Visiting every node (or link) in a network in a systematic way, i.e. graph traversal.
</text>

<text>
++ CS
Graph-searching procedures such as depth-first search (DFS)
<cite>Tarjan 1972</cite> <cite>Hopcroft and Tarjan 1973</cite>
and breadth-first search (BFS) 
<cite>Moore 1959</cite>
form the basic preprocessing steps for most graph algorithms. 
</text>

<text>
For efficiency, we should not visit the same node repeatedly. For correctness, we should guarantee that every node is eventually visited. IDEA: Keep track of which nodes we have already visited...
</text>


<text>
BFS/DFS define a tree on the vertices of the graph.
</text>

<note>
<title>The Visitor design pattern</title>

<text>
++ Visitor design pattern <cite>Gamma et al. 1994</cite>
</text>

</note>


<document>
<tag>graph-exploration-dfs</tag>
<title>Depth-first search</title>

<text>
Implementation: recursion/stack (LIFO: we explore a path until we meet a dead end (i.e. no unvisited neighbors) and then backtrack). Recursion eliminates the need of an explicit stack and provides a neat implementation of the algorithm.
</text>


<text>
EFFICIENCY: <eqn>O(n+m)</eqn> using adjacency lists
</text>



<text>
Time intervals: a clock ticks every time we enter or exit a node and we keep track of the entry and exit times for each node.
</text>

<text>
Properties:

- Time intervals are properly nested: if we enter x before y, we exit y before x; i.e. all nodes reachable from a given node are explored before we finish with it.

- The difference between exit and entry times indicate the number of descendants of any given node. Actually half the time difference is the number of descendants in the search tree.

- For undirected graphs, DFS partitions edges into tree edges and back edges. Tree edges appear in the DFS tree, whereas back edges always point to an ancestor in the DFS tree (never to a sibling nor cousin node, just because all nodes reachable from a given node are explored before we finish with it).
</text>

<!--
Classifying (x,y) edges:

- parent[y] == x  => Tree edge

- discovered[y] and not processed[y]  => Back edge

For directed graphs:

- discovered[y] and (entry_time[y] &gt; entry_time[x]) => Forward edge

- discovered[y] and (entry_time[y] &lt; entry_time[x]) => Cross edge
-->


<text>
APPLICATIONS
</text>

<text>
++ CS        Algorithms based on DFS have been known for a long time for the problem of searching
mazes. However, it was the work of Hopcroft and Tarjan (for which they
received the ACM Turing Award in 1986) that illustrated the full algorithmic
power of DFS <cite>Hopcroft and Tarjan 1973</cite>. They demonstrated efficient algorithms for several problems,
such as finding biconnected components and bridges of a graph and testing
triconnectivity and planarity. DFS on directed graphs can be used to
classify its vertices into strongly connected components, to detect cycles,
and to find a topological order of the vertices of a DAG.
</text>


<text>
Solving mazes (e.g. <cite>Moore 1959</cite>): A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Tremaux as a strategy for solving mazes.
</text>

<text>
ON UNDIRECTED GRAPHS
</text>

<text>
++ Connected components: Identify the different contextec components of a graph G, where x and y are members of different components if no path exists from x to y in G. O(m+n)
</text>


<text>
++ Cycle detection / tree testing: If there are no back edges, all edges belong to the DFS tree, hence the graph is actually a tree. When present, the back edge and the DFS tree define the cycle.
</text>

<text>
++ Graph biconnectedness: Articulation vertices (a.k.a. cut-nodes): Vertices whose deletion disconnects the graph connected component (i.e. single points of failure). Graphs without articulation points are said to be biconnected, since they have no single points of failure. A brute force algorithm would remove each vertex and check whether the graph is still connected (an <eqn>O(n(n+m))</eqn> algorithm), albeit a linear algorithm is possible based on DFS if we track the earliest reachable node for each node. If the DFS tree represented the whole graph, all internal nodes would be articulation points. Three situations have to be considered <cite>Skiena 2008</cite>: 

1) root cut-nodes (if the root of the DFS tree has more than one child, then it is an articulation point), 

2) bridge cut-nodes (if v is an internal node in the DFS tree and its earliest reachable node is v, then v is an articulation point)

3) parent cut-nodes (if v is an internal node in the DFS tree and its earliest reachable node is its parent in the tree, then the parent is an articulation point unless it is the root of the DFS tree, i.e. it cuts v and its descendants from the graph).
</text>

<text>
++ Edge biconnectedness: Bridges (in terms of link failures instead of node failures). A single edge whose deletion disconnects the graph is called a bridge. Any graph without bridges is said to be edge-biconnected. Edge (x,y) is a bridge when it is a DFS tree edge and no back edge connects from y or below to x or above.
</text>


<text>
The classic sequential algorithm for computing biconnected components in a connected undirected graph due to John Hopcroft and Robert Tarjan (1973) runs in linear time, and is based on depth-first search <cite>Hopcroft and Tarjan 1973</cite>

In the online version of the problem, vertices and edges are added (but not removed) dynamically, and a data structure must maintain the biconnected components. <cite>Westbrook and Tarjan 1992</cite> developed an efficient data structure for this problem based on disjoint-set data structures. Specifically, it processes n vertex additions and m edge additions in <eqn>O(m \alpha(m, n))</eqn> total time, where alpha is the inverse Ackermann function. This time bound is proved to be optimal.

<cite>Tarjan and Vishkin 1985</cite> designed a parallel algorithm on CRCW PRAM that runs in O(log n) time with n + m processors. 

<cite>Cong and Bader 2005</cite> developed an algorithm that achieves a speedup of 5 with 12 processors on SMPs.
</text>


<text>
ON DIRECTED GRAPHS
</text>

<text>
++ Cycle detection / DAG testing: If there are no back edges, all edges belong to the DFS tree, hence the graph is actually a DAG. As above, when present, the back edge and the DFS tree define the cycle.
</text>

<text>
++ Topological sorting: A common operation on directed acyclic graphs. Finding a linear ordering of the vertices so that for each arc (i,j), vertex i is to the left of vertex j (such ordering cannot exist if the graph contains a directed cycle, i.e. no back edges). Each DAG has, at least, one topological sort (often, many different ones, up to n! if there are no constraints), which gives an ordering to process each vertex before any of its successors (e.g. precedence constraints). Only DAGs can be topologically sorted. 

1) Considering the nodes in the reverse order they are processed by DFS returns a valid topological sort.

2) Using a DFS to identify all source vertices (of in-degree 0), which can appear at the start of the topological sort without violating any constraints. Deleting outgoing edges of the source vertices creates new source vertices. The process is repeated until all vertices are included in the topological sort. Proper use of data structures is sufficient to obtain a linear time O(n+m) algorithm.

 ref. first described by <cite>Kahn 1962</cite>: http://en.wikipedia.org/wiki/Topological_sorting
 Applications: scheduling tasks under precedence constraints (any topological sort, a.k.a. linear extension, defines an order to perform the tasks that satisfies all precedence constraints).
</text>


<text>
++ Identifying weakly-connected components: Just ignore edge direction.
</text>

<text>
++ Identifying strongly-connected components (first algorithm by <cite>Tarjan 1972</cite>): 1) Traverse the graph atarting from any given node to discover the nodes that are reachable from the node. 2) Build a graph G'=(V,E') with the same vertex set but all its arcs reversed; i.e. <eqn>(y,z) \in E' iff (x,y) \in E</eqn>. 3) Perform a traveral starting from v in G', which will result in discovering the set of nodes that can reach v. 

If the second traversal does not completely traverse G', each DFS performed on G' will correspond to a strongly-connected component.
</text>

<text>
Any graph can be partitioned into a set of strongly-connected components. Using DFS, we can easily identify cycles. If we take into account that all the nodes involved in a cycle must belong to the same strongly-connected component, we can collapse the nodes in the cycle into a single vertex and repeat the process. When no cycles remain, each vertex represents a different strongly-connected component.
</text>

<text>
Linear SCC algorithms:

- Kosaraju's algorithm, 1978: http://en.wikipedia.org/wiki/Kosaraju%27s_algorithm
   ref.  <cite>Aho et al. 1983</cite> credit it to an unpublished paper from 1978 by S. Rao Kosaraju and Micha Sharir. (Al

- Cheriyan-Mehlhorn/Gabow algorithm, 1996/1999: http://en.wikipedia.org/wiki/Cheriyan%E2%80%93Mehlhorn/Gabow_algorithm
   ref. <cite>Cheriyan and Mehlhorn 1996</cite> <cite>Gabow 2003</cite>

- Tarjan's strongly connected components algorithm, 1972: http://en.wikipedia.org/wiki/Tarjan's_strongly_connected_components_algorithm
   ref. <cite>Tarjan 1972</cite>
   


 all efficiently compute the strongly connected components of a directed graph, but Tarjan's and Gabow's are favoured in practice since they require only one depth-first search rather than two. http://en.wikipedia.org/wiki/Strongly_connected_component
</text> 


<text>
Parallelization of DFS 
- for dense graphs @ Section 10.6 <cite>Grama et al. 2003</cite>
- Parallel DFS @ Section 11.4  <cite>Grama et al. 2003</cite>
</text>


</document>



<document>
<tag>graph-exploration-bfs</tag>
<title>Breadth-first search</title>

<text>
Implementation: queue (FIFO: we explore the oldest unexplored vertices first).
</text>

<text>
EFFICIENCY: <eqn>O(n+m)</eqn> using adjacency lists
</text>

<text>
Properties (due to the fact that each path in the tree must be the shortest path in the graph):

- For undirected graphs, edges not appearing in the breadth-first search tree can only point to nodes on the same level or to the level directly below.
</text>



<text>
APPLICATIONS
</text>

<text>
++ Shortest paths can be found by performing a breadth-first search on unweighted/binary graphs <cite>Moore 1959</cite>


(more elaborate algorithms are required for weighted graphs, as we will see later): the tree resulting from BFS defines the shortest paths from the root to the remaining nodes in the graph. IMPLEMENTATION: parent[node] @ Visitor + reversal using recursion/stack
</text>

<text>
++ Connected components: Any node we visit is part of the same connected component. Starting the search from any unvisited node, we can obtain additional connected components.
</text>

<text>
++ Two-coloring graphs = Bipartite testing: A graph is bipartite if it can be colored without conflicts using two colors. Whenever we visit a new node, we color it using the opposite of its parent's color. If edges appear with the same color at both ends, the graph is not bipartite. Otherwise, we have partitioned the graph.


"Color the first vertex blue, and then do a depth-first search of the graph. Whenever we discover a new, uncolored
vertex, color it opposite of its parent, since the same color would cause a clash. The graph cannot be bipartite if we ever find an edge (x, y) where both x and y have been colored identically. Otherwise, the final coloring will be a 2-coloring, constructed in O(n + m) time."
</text>


<text>
++ Distributed algorithm @ MIT Lecture 2 (synchronous) + Lecture 9 (asynchronous) / Section 4.2 + Section 15.4 <cite>Lynch 1997</cite>
@ MIT Distributed algorithms Lecture 21
</text>

<text>
++ Sublinear approximate algorithm @ MIT Sublinear algorithms => Goal: Quickly distinguish inputs that have specific property from those that are far from having the property...
</text>


</document>



<document>
<tag>graph-exploration-best-first</tag>
<title>Best-first search</title>

<text>
Best-first search... heuristics

Judea Pearl described best-first search as estimating the promise of node n by a <q>heuristic evaluation function f(n) which, in general, may depend on the description of n, the description of the goal, the information gathered by the search up to that point, and most important, on any extra knowledge about the problem domain</q> <cite>Pearl 1984</cite>.

</text>

<text>
http://en.wikipedia.org/wiki/A*_search_algorithm
A* search algorithm

HISTORY: In 1964 Nils Nilsson invented a heuristic based approach to increase the speed of Dijkstra's algorithm. This algorithm was called A1. In 1967 Bertram Raphael made dramatic improvements upon this algorithm, but failed to show optimality. He called this algorithm A2. Then in 1968 Peter E. Hart introduced an argument that proved A2 was optimal when using a consistent heuristic with only minor changes. His proof of the algorithm also included a section that showed that the new A2 algorithm was the best algorithm possible given the conditions. He thus named the new algorithm in Kleene star syntax to be the algorithm that starts with A and includes all possible version numbers or A*. <cite>Hart et al. 1968</cite> <cite>Hart et al. 1972</cite>

Often used for path finding...
</text>


<text>
http://en.wikipedia.org/wiki/B*
B* algorithm <cite>Berliner 1979</cite>
</text>


<text>
++ Parallel best-first search @ Section 11.5 <cite>Grama et al. 2003</cite>
</text>

</document>

</document>


<!-- Minimum spanning trees -->

<document>
<tag>graph-spanning-trees</tag>
<title>Minimum spanning trees</title>

<text>
A spanning tree of a graph <eqn>G=(V,E)</eqn> is a subset of <eqn>n-1</eqn> edges from <eqn>E</eqn> that connect all the vertices in <eqn>V</eqn> and, therefore, form a tree. In weighted graphs, where edges have numerical values associated to them, known as weights, a minimum spanning tree is a spanning tree whose sum of edge values is as small as possible. A tree is always the smallest connected graph in terms of its number of edges (and also the most vulnerable to single failures), while a MST is the cheapest connected graph in term of edge costs.
</text>

<text>
The algorithms in this section can also be adapted to find maximum spanning trees (just negate the weights and the MST in the negated graph is the maximum spanning tree in the original graph), minimum product spanning trees (replace edge weights with their logarithms so that products are converted into sums), or minimum bottleneck spanning tree (i.e. a spanning tree that minimizes the maximum edge weight, by just employing Kruskal's algorithm). 
</text>

<text>
However, other related problems cannot be solved using the efficient algorithms described in this section:
- Steiner trees (when we can add intermediate vertices), NP-hard in general.
- Low-degree spanning tree (i.e. the minimum spanning tree whose maximum node degree is smaller; in case it where a simple path, this problem would be equivalent to finding a Hamiltonian path = TSP, NP-complete)
</text>

<text>
APPLICATIONS: 
- Whenever we need to connect a set of nodes with minimum cost (e.g. transportation networks, communication networks, distribution networks...). 
- Bottleneck spanning trees have applications when edge weights represent costs, capacities, or strengths
- Clustering (e.g. hierarchical clustering)
- They can be used to obtain approximate solutions to hard problems (Steiner trees or TSP below)
</text>


<text>
In unweighted graphs (or weighted graphs where all edges have the same cost), any tree is also a minimum spanning tree. Its cost will always be <eqn>n-1</eqn> times the cost of a single edge. In this case, a spanning tree can easily be found by performing any systematic graph traversal (e.g. DFS or BFS).
</text>

<text>
In weighted graphs, efficient greedy algorithms exist than can be employed to identify minimum spanning trees...
</text>


<document>
<tag>graph-spanning-trees-greedy</tag>
<title>Greedy MST algorithms</title>


<text>
@ MIT Graphs - Shortest paths
</text>

<text>
Greedy algorithms choose what to do next by selecting the best local candidate to be added to the solution and, when the optimality of the chosen heuristics can be proved, they provide efficient algorithms for solving problems such as finding the minimum spanning tree.
</text>

<text>
Candidates: 

- Edges (Kruskal's algorithm <cite>Kruskal 1956</cite>): Each vertex starts as a separate tree and separate trees are merged by adding the lowest-cost edge to the MST that does not create a cycle (i.e. connects two separate subtrees)

Kruskal(G):
Sort edges in order of increasing weight
edges_in_tree = 0
while edges_in_tree &lt; n-1
    get next edge (v,w)
    if component(v) != component(w)
       Add edge (v,w) to the MST
       component(v) = component(w)
       edges_in_tree++


- Nodes (Prim's algorithm <cite>Prim 1957</cite>, rediscovered by <cite>Dijkstra 1959</cite>): Start with an arbitrary vertex and iteratively grow the tree by adding the lowest-cost edge that links some new vertex to the tree.

Prim(G):
Select an arbitrary vertex
while...

- The oldest algorithm: Boruvka's algorithm, published in 1926 <cite>Boruvka 1926</cite> and rediscovered by Choquet in 1938 <cite>Choquet 1938</cite>

Since the lowest-weight edge incident on each vertex must be in the minimum spanning tree, the union of these edges will return a forest of at most n/2 trees. For each tree, select the edge (x,y) of lowest weight such that x \in T and y \notin T, which will also have to belong to the MST. Repeat until all vertices are connected. Each iteration at least halves the number of remaining trees, hence, after log n iterations, each taking linear time, we obtain the MST in O(m log n).


An inverse algorithm would also be possible...
  Algoritmo de  Algoritmo de borrado borrado inverso inverso::
  Comenzando con  Comenzando con T=A,  T=A, considerar considerar laslas aristas aristas en  en orden orden decreciente decreciente
  de coste de coste y y eliminar eliminar laslas aristas aristas de T salvo  de T salvo que que esoeso desconectase desconectase T.T.

</text>


<text>
MSTs are unique when all m edge weights are distinct. The way ties are broken leads to different MSTs...
</text>

<document>
<tag>graph-spanning-trees-greedy-prim</tag>
<title>Prim's algorithm</title>

<text>
Starts from any node and adds one edge at a time until all the vertices are connected... heuristics: select the edge with the smallest weight that will enlarge the number of vertices in the tree
</text>

<text>
No cycles are created since only edges between tree and non-tree nodes are added to the solution
</text>

<text>
EFFICIENCY:
<eqn>n</eqn> iterations through <eqn>m</eqn> edges = <eqn>O(mn)</eqn>
</text>

<text>
PROOF? by contradiction
</text>

<text>
Tree reconstruction: parent vector or dynamic structure...
</text>

</document>

<document>
<tag>graph-spanning-trees-greedy-kruskal</tag>
<title>Kruskal's algorithm</title>

<text>
Instead of starting on any particular node and growing a tree, Kruskal's algorithm builds the minimum spanning tree by connecting sets of vertices...
</text>

<text>
Initially: each vertex a separate component + Iteration: consider the least cost edge and test whether it connects two separate components (if its both endpoints are already in the same connected component, the edge is discarded, since it would create a cycle; if not, it is added to the MST and the components it connects are merged).
</text>

<text>
Since each connected component is always a tree, we do not have to check for cycles
</text>

<text>
EFFICIENCY:
Sorting edges <eqn>O(m \log m)</eqn> + <eqn>m</eqn> iterations / testing connectivity by BFS/DFS <eqn>O(n)</eqn> = <eqn>O(mn)</eqn>
</text>

<text>
PROOF? by contradiction
</text>

</document>

</document>


<document>
<tag>graph-spanning-trees-sequential</tag>
<title>Sequential implementation of MST algorithms</title>

<text>
Naive implementation:
<eqn>O(mn)</eqn> for Kruskal's algorithm 
<eqn>O(mn)</eqn> for Prim's algorithm 
(i.e. <eqn>O(n^3)</eqn> for dense graphs, <eqn>O(n^2)</eqn> for sparse graphs)
</text>

<text>
Prim's algorithm:
1) Maintaining a boolean flag to indicate whether a vertex is already in the tree or not, we can test whether the current edge links to a non-tree vertex in constant time, leading to a <eqn>O(n^2)</eqn> algorithm (both for dense and sparse graphs).
2) Optimization: <eqn>O(n \log k)</eqn> on graphs that have only <eqn>k</eqn> different edge costs.
3) The use of priority queues can lead to a more efficient implementation, by making it faster to find the minimum cost edge at each iteration.
- <eqn>O(m \log n)</eqn> using standard binary heaps.
- <eqn>O(m + n \log n)</eqn> using Fibonacci <cite>Fredman and Tarjan 1987</cite> or pairing heaps <cite>Stasko and Vitter 1987</cite>.
</text>

<text>
Kruskal's algorithm:
1) Union-find data structure for implementing the component test in <eqn>O(\log n)</eqn> = <eqn>O(m \log m)</eqn> time (i.e. needed to sort the edges).
+ Typically faster than Prim's algorithm for sparse graphs
</text>


<text>
Union-find data structure for representing set partitions (a partitioning of elements, say nodes, intro a collection of disjoint subsets):
1) Union: Component merging in <eqn>O(\log n)</eqn>
2) Find: Same component check in <eqn>O(\log n)</eqn>

Solution: Represent each component as a backwards tree, with pointers from a node to its parent (choosing how to merge in order to limit the height of the trees and reduce the effect of unbalanced trees: the smaller tree will always be a subtree of the larger tree; why? height of the nodes in the larger tree stay the same, heights in the smaller tree are increased by one)

Result: We must double the number of nodes to increase the height of the resulting tree, hence the <eqn>O(\log n)</eqn> result
</text>

<text>
Set data structures to represent unordered collections of objects. Representing subsets to efficiently test element \in subset, compute union/intersection of subsets, or insert/delete subset members,

1) Bit vectors: Space efficient, insertion/deletion by toggling single bits, union/intersection by or-ing/and-ing bit vectors.

2) Bloom filters <cite>Broder and Mitzenmacher 2005</cite> use hashing.

3) Dictionaries, which can be viewed as sets (no duplicate keys).

4) Standard collections, which can be viewed as multisets (may have multiple occurrences of the same element).

Set partitions: Pairwise-disjoint subsets, when each element is exactly in one subset...

1) Collection of sets (as a generalized bit vector, a collection of collections, or a dictionary with a subset sttribute): Costly union/intersection operations.

2) Union-find data structure *
++ Union-find can be done even faster: 
- Optimization: Path compression. Retraversing the path on each find operation and explicitly pointing all nodes to the root leads to almost constant-height trees.
- Limitation: Does not support breaking up unions
- Upper bound on m union-find operations on an n-element set: <eqn>O(m \alpha(m, n))</eqn>, where alpha is the inverse Ackermann function, which grows notoriously slowly, and leads to an almost-linear performance. <cite>Tarjan 1979</cite>.


NOTE:
Efficient implementation of basic graph algorithms led to the discovery of several data structures, such as the "union-find"
structure <cite>Tarjan 1979</cite> and Fibonacci heaps <cite>Fredman and Tarjan 1987</cite>. These data structures have been used to speed up many other algorithms.

</text>

<text>
A combination of Boruvka's algorhtm <cite>Boruvka 1926</cite> with Prim's algorithm <cite>Prim 1957</cite> yields an O(m log log n) algorithm. Running log log n iterations of Boruvka's algorhtm yields at most n/log n trees. Then create a graph G' with a vertex for each tree and an edge whose weight corresponds to the lightest edge between T_i and T_j. The MST of G' combined with the edges selected by  Boruvka's algorhtm yields the MST of G. Since Prim's algorithm will take O(n+m) time on this n/log n vertex, m edge graph (if implemented using Fibonacci or pairing heaps). <cite>Skiena 2008</cite>. Even tighter bounds, e.g. O(n \alpha(m,n)), can be achieved <cite>Karger et al. 1995</cite> <cite>Chazelle 2000</cite> <cite>Pettie and Ramachandran 2002</cite> <cite>Pettie and Ramachandran 2008</cite>.
</text>

<text>
Dynamic graph algorithms are incremental algorithms that maintain graph invariants (such as MSTs) under edge insertion and deletion operations. <cite>Holm et al. 2001</cite> describes and efficient algorithm to maintain MSTs (and several other invariant) in amortized polylogarithmic time per update.
</text>


</document>

<document>
<tag>graph-spanning-trees-parallel</tag>
<title>Parallel MST algorithms</title>

<text>
Parallel Prim's algorithm
- for dense graphs @ Section 10.2 <cite>Grama et al. 2003</cite>
- for sparse graphs @ Section 10.7.2 <cite>Grama et al. 2003</cite>
+ Bibliographic notes
</text>

</document>

<document>
<tag>graph-spanning-trees-distributed</tag>
<title>Distributed MST algorithms</title>

<text>
Distributed MST algorithms @ MIT 6.852: Distributed Algorithms - Lecture 4 (synchronous) + Lecture 9 (asynchronous) / Section 4.4 + Section 15.5 <cite>Lynch 1997</cite>
</text>

</document>

</document>




<!-- Shortest paths -->

<document>
<tag>graph-shortest-paths</tag>
<title>Shortest paths</title>


<text>
When graphs are weighted, i.e. each edge in the graph is associated to a numerical value, computing their shortest paths is more complicated than performing breadth-first traversals (recall that the BFS tree leads to minimum-number-of-links paths)...
</text>

<text>
Shortest paths in directed acyclic graphs can also be found in linear time: Perform a topological sorting and then process the vertices
from left to right. Observe that d(s, j) = min_(x,i) d(s, i) + w(i, j) since we already know the shortest path d(s, i) for all vertices to the left of j. The same algorithm (replacing min with max) also suffices to find the longest path in a DAG, which is useful in scheduling applications.
</text>


<note>
<title>The Strategy design pattern</title>

<text>
++ The Strategy design pattern <cite>Gamma et al. 1994</cite>
</text>
</note>


<text>
@ MIT Graphs - Shortest paths
</text>


<!-- Shortest paths: Single source -->

<document>
<tag>graph-shortest-paths-dijkstra</tag>
<title>Single source shortest paths</title>

<text>
Finding the shortest path from s to t in G.
Actually, finding the shortest path from a source node <eqn>s</eqn> to every other node in the network requires the same amount of work...
</text>

<text>
APPLICATIONS: Finding directions (note: hierarchical A*), transportation and communications networks, distinguishing
among homophones iin speech recognition systems, graph visualization algorithms...

Approximate shortest paths in road networks, for instance, typically employ a hierarchical variation of A* <cite>Goldberg et al. 2006</cite> <cite>Goldberg et al. 2007</cite>. Such problem differ from the shortest path problems in this Section because
(1) preprocessing costs can be amortized over many point-to-point queries, 
(2) the backbone of high-speed, long-distance highways can reduce the path problem to identifying the best place to get on and off this backbone, and (3) approximate or heuristic solutions suffice in practice
</text>



<text>
IDEA: Shortest path from s to t goes through x, then that path contains the shortest path from s to x
</text>

<text>
Similar to Prim's algorithm... each iteration determines the shortest path from s to a new vertex x by minimizing d(s,v)+w(v,x)

In Prim's algorithm, we just take into account the weighte of the edge we add to the MST; in Dijkstra's algorithm, we consider the cost of the whole path from s to x (i.e. the final edge weight plus the distance from s to the tree vertex that is adjacent to the new edge). Dijkstra's algorithm returns a shortest-path spanning tree from s to all the other nodes in the network.
</text>

<text>
http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm
Dijkstra's algorithm <cite>Dijkstra 1959</cite>: 

computes the shortest path from a given starting vertex x to all n . 1
other vertices. In each iteration, it identifies a new vertex v for which the shortest
path from x to v is known. We maintain a set of vertices S to which we currently
know the shortest path from v, and this set grows by one vertex in each iteration.
In each iteration, we identify the edge (u,v) so that
dist(x, u) + weight(u, v) = min_(w,y) { dist(x, w) + weight(w,y) }

This edge (u, v) gets added to a shortest path tree, whose root is x and describes
all the shortest paths from x.

If we just need to know the shortest
path from x to y, terminate the algorithm as soon as y enters S.
</text>

<text>
EFFICIENCY:
- <eqn>O(n^2)</eqn> using a naive implementation with arrays
- Using binary heaps: <eqn>O(m \log n)</eqn>
- Fastest implementation: <eqn>O(m + n \log n)</eqn> amortized time using Fibonacci heaps <cite>Fredman and Tarjan 1987</cite>
</text>

<text>
Related problem: Single-destination shortest path problem for directed graphs.
</text>

<text>
CORRECTNESS: Only on graphs without negative edges!!!

Dijkstra's algorithm assumes that all edges have positive cost. 
- Adding a fixed amount of weight to make each edge positive does
not solve the problem. Dijkstra's algorithm will then favor paths using a fewer
number of edges,

For graphs with edges of negative weight,
you must use the more general, but less efficient, Bellman-Ford algorithm.

NOTE: negative cost cycles =the shortest x to y path in such a graph is not defined because we can detour
from x to the negative cost cycle and repeatedly loop around it, making the total cost arbitrarily small.


</text>

<text>
In a geometric setting, more efficient geometric algorithms compute the shortest path directly from the arrangement of obstacles: motion planning algorithms <cite>Latombe 1991</cite> <cite>LaValle 2006</cite>
</text>


<text>
Parallelization of Dijkstra's algorithm
- for dense graphs @ Section 10.3 <cite>Grama et al. 2003</cite>
+ Bibliographic notes
</text>

</document>



<!-- Shortest paths: All pairs -->

<document>
<tag>graph-shortest-paths-all-pairs</tag>
<title>All-pairs shortest paths</title>


<text>
Some problems requite computing the shortest paths between all pairs of nodes, e.g. the central node (i.e. the node that minimizes the longest or average distance to every other node) or the network diameter (i.e. the longest shortest-path distance between two nodes in the network). 
</text>

<text>
EXAMPLE: 
- bounding the amount of time needed to deliver a packet through a network
</text>

<text>
+ Best on adjacency matrix (anyway we will need to store <eqn>n \times n</eqn> distances), i.e. one of the rare cases where algorithms on adjacency matrices work better
</text>


<text>
A first solution: Repeating Dijkstra's algorithm for each node
</text>

<!-- Floyd-Warshall -->

<text>
http://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm
Floyd-Warshall algorithm: <cite>Floyd 1962</cite>
often referred to as Floyd's algorithm, albeit the same algorithm was published by Bernard Roy in 1959 <cite>Roy 1959</cite> and by Stephen Warshall in 1962 <cite>Warshall 1962</cite>


<eqn>O(n^3)</eqn>

- Define the length of the shortest path from i to j using only the k first vertices as possible intermediate nodes: <eqn>W[i,j]^k</eqn>
- Initial shortest-path matrix = adjacency matrix
- <eqn>W[i,j]^k = min \{ W[i,j]^{k-1}, W[i,k]^{k-1} + W[k,j]^{k-1} \}</eqn>
- Compact implementation: 3 nested loops
- Ancillary matrix to reconstruct the paths
</text>

<code>
D0 = M
for k = 1 to n do
    for i = 1 to n do
        for j = 1 to n do
            Dk(i,j) = min{ Dk-1(ij), Dk-1(ik) + Dk-1(kj) }
return Dn


  for k := 1 to n
        for i := 1 to n
           for j := 1 to n
              if path[i][k] + path[k][j] &lt; path[i][j] then
                 path[i][j] := path[i][k]+path[k][j];
                 next[i][j] := k;

</code>

<text>
Transitive closure of directed graphs (Warshall's algorithm). In Warshall's original formulation of the algorithm, the graph is unweighted and represented by a Boolean adjacency matrix. Then the addition operation is replaced by logical conjunction (AND) and the minimum operation by logical disjunction (OR).
</text>

<!-- Bellman-Ford -->

<text>
http://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm
Bellman-Ford algorithm: <eqn>O(nm)</eqn> time, <eqn>O(n)</eqn> space

Bellman-Ford algorithm <cite>Bellman 1958</cite> <cite>Ford and Fulkerson 1962</cite>

+ Negative weights (arise when we reduce other problems to shortest-paths problems)
</text>

<code>
   // Step 1: initialize graph
   for each vertex v in vertices:
       if v is source then v.distance := 0
       else v.distance := infinity
       v.predecessor := null

   // Step 2: relax edges repeatedly
   for i from 1 to size(vertices)-1:
       for each edge uv in edges: // uv is the edge from u to v
           u := uv.source
           v := uv.destination
           if u.distance + uv.weight &lt; v.distance:
               v.distance := u.distance + uv.weight
               v.predecessor := u

   // Step 3: check for negative-weight cycles
   for each edge uv in edges:
       u := uv.source
       v := uv.destination
       if u.distance + uv.weight &lt; v.distance:
           error "Graph contains a negative-weight cycle"
</code>

<!-- Johnson -->

<text>
http://en.wikipedia.org/wiki/Johnson%27s_algorithm
Johnson's algorithm <cite>Johnson 1977</cite> 

1
First, a new node q is added to the graph, connected by zero-weight edges to each of the other nodes.

2
Second, the Bellman-Ford algorithm is used, starting from the new vertex q, to find for each vertex v the least weight h(v) of a path from q to v. If this step detects a negative cycle, the algorithm is terminated.

3
Next the edges of the original graph are reweighted using the values computed by the Bellman-Ford algorithm: an edge from u to v, having length w(u,v), is given the new length w(u,v) + h(u) - h(v).

4
Finally, q is removed, and Dijkstra's algorithm is used to find the shortest paths from each node s to every other vertex in the reweighted graph.


<eqn>O(n^2 \log n + nm)</eqn> using <eqn>O(nm)</eqn> time for the Bellman-Ford stage of the algorithm, and <eqn>O(n \log n + m)</eqn> for each of <eqn>n</eqn> instantiations of Dijkstra's algorithm. Faster than the Floyd-Warshal algorithm for sparse graphs.


It allows some of the edge weights to be negative
</text>


<!-- Applications -->

<text>
All-pairs shortest paths algorithms can be used to find the shortest cycle in a graph: its girth. Floyd's algorithm can be used to compute d_ii, which is the shortest way to get from vertex i to itself (i.e. the shortest cycle that goes through i). If you want the shortest simple cycle, the easiest
approach is to compute the lengths of the shortest paths from i to all other vertices, and then explicitly check whether there is an acceptable edge from each vertex back to i.
</text>


<text>
- Reachability questions (can I get to x from y?)

RELATED PROBLEMS 
+ Transitive closure, construct a graph G' = (V,E') with edge (i, j) in E' iff there is a directed path from i to j in G. 
+ Transitive reduction (also known as minimum equivalent digraph), the inverse operation of transitive closure: reducing the number of edges while maintaining identical reachability properties, i.e. construct a small graph G'' = (V,E'') with a directed path from i to j in G'' iff there is a directed path from i to j in G

- transitive closure 

1) using graph traversal algorithms O(n(n+m)), 
2) an all-pairs shortest path algorithms returns them all in a whole batch, albeit less efficiently, O(n^3) using Warshall's algorithm.
3) using matrix multiplication: O(log n) multiplications using a divide-and-conquer algorithm

- transitive reduction

Approximate solution: A linear-time, quick-and-dirty transitive reduction algorithm identifies the
strongly connected components of G, replaces each by a simple directed cycle,
and adds these edges to those bridging the different components.

+ Transitive reduction also arises in graph drawing, where it is important to eliminate as many unnecessary edges as possible to reduce the visual clutter
</text>


<text>
- pattern recognition problems, e.g. Viterbi algorithm, a dynamic programming algorithm that basically solves a shortest path problem on a DAG.
</text>


<!-- Parallelization -->

<text>
Parallelization of Floyd's algorithm 
- for dense graphs @ Section 10.4 <cite>Grama et al. 2003</cite>
+ Bibliographic notes
</text>


<text>
Wikipedia: A distributed variant of the Bellman-Ford algorithm is used in distance-vector routing protocols, for example the Routing Information Protocol (RIP). The algorithm is distributed because it involves a number of nodes (routers) within an Autonomous system, a collection of IP networks typically owned by an ISP. It consists of the following steps:
- Each node calculates the distances between itself and all other nodes within the AS and stores this information as a table.
- Each node sends its table to all neighboring nodes.
- When a node receives distance tables from its neighbors, it calculates the shortest routes to all other nodes and updates its own table to reflect any changes.
</text>


<text>
Distributed shortest paths @ MIT 6.852: Lectures 2-3 (synchronous) + Lectures 8-9 (asynchronous)  / Section 4.3 + Section 15.4 <cite>Lynch 1997</cite>
</text>

</document>

</document>




<!-- Network flows -->

<document>
<tag>graph-flow</tag>
<title>Network flows: Maximum flow and minimum cuts</title>

<text>
Interpreting weighted graphs as distribution networks where weights represent capacities
</text>

<text>
two primary classes of network flow problems:
maximum flow and minimum-cost flow:

- "The maximum-flow problem is that of finding a maximum flow of a
single commodity from a source vertex to a sink vertex that satisfies
capacity constraints on the edges and flow conservation constraints at the
vertices. "

- Associating costs with edges yields the minimum-cost flow problem. 
where we also have a targeted amount of flow f we want to send from s to t at minimum total cost.


Polynomial-time algorithms to solve maximum flow and minimum-cost flows are known.
"a surprising variety of linear programming
problems arising in practice can be modeled as network-flow problems,
and network-flow algorithms can solve these problems much faster than
general-purpose linear programming methods"

APPLICATIONS: cost-effective way to ship goods between a set of factories and a set of stores
defines a network-flow problem, as do many resource-allocation problems in communications
networks
</text>

<text>
Network flow problem: The maximum amount of flow that can be sent from s to t in a given graph.
</text>

<text>
The two major techniques for solving flow problems are the augmenting-path method and the preflow-push method.

- Augmenting paths: "These algorithms repeatedly find a path of positive
capacity from source to sink and add it to the flow. It can be shown that
the flow through a network is optimal if and only if it contains no augmenting
path. Since each augmentation adds something to the flow, we eventually
find the maximum. The difference between network-flow algorithms is in how
they select the augmenting path."

- "Preflow-push methods - These algorithms push flows from one vertex to another,
initially ignoring the constraint that in-flow must equal out-flow at each
vertex. Preflow-push methods prove faster than augmenting-path methods,
essentially because multiple paths can be augmented simultaneously. These
algorithms are the method of choice in practice"
</text>


<!-- Augmenting paths -->

<text>
IDEA: <b>Augmenting paths</b>
i.e. iteratively find a path of positive capacity from s to t and add it to the flow
- As in a traffic jam, the flow is limited by the most congested point.
- A flow through a network is optimal if and only if it contains no augmenting path.
</text>

<text>
MECHANISM: Directed <b>residual flow graph</b> R(G,f), where G is the graph and f the current flow
i.e. For each edge (i,j) in G with capacity c(i,j) and flow f(i,j), R can contain two edges
- an edge (i,j) with weight c(i,j)-f(i,j) if c(i,j)>f(i,j)
- an edge (j,i) with weight f(i,j)-c(i,j) if f(i,j)>0

An edge (i,j) in the residual flow graph indicates that flow can still be pushed from i to j (its weight determines the amount of its unused capacity). 

A path from s to t in the residual graph (and the minimum edge weight on this path) defines an augmenting path (and the amount of flow that can be pushed).
</text>

<text>
- For each edge in the residual graph: keep track of the amount of flow currently going through the edge as well as any unused capacity (the residual capacity)
- Initialization: create directed flow edges (i,j) and (j,i); initial flows set to 0; initial residual flows set to their capacity in the original graph.
</text>

<text>
ITERATIVE ALGORITHM
Ford-Fulkerson algorithm <cite>Ford and Fulkerson 1956</cite>
http://en.wikipedia.org/wiki/Ford%E2%80%93Fulkerson_algorithm

- Use the augmented path to update the residual graph (transfer the maximum possible volume from the residual capacity = the path edge with the smallest amount of residual capacity; NOTE: adding positive flow to (i,j) reduces the residual capacity of (i,j) but also increases the residual capacity of (j,i), i.e. augmenting a path requires updates to both forward and reverse links along the augmenting path ).
- Terminate when no such augmenting path exists.

As described, even though the algorithm converges to the optimal solution, it may take arbitrarily long (each augmenting path might add only a little to the flow).
</text>

<text>
IMPLEMENTATION (Edmonds-Karp algorithm <cite>Edmonds and Karp 1972</cite>):
http://en.wikipedia.org/wiki/Edmonds%E2%80%93Karp_algorithm
The algorithm is identical to the Ford-Fulkerson algorithm, except that the search order when finding the augmenting path is defined. The path found must be a shortest path that has available capacity

- BFS to look for any path from the source to the sink that increases the total flow (only consider edges with residual capacity, i.e. positive residual flow)
</text>

<text>

 <cite>Edmonds and Karp 1972</cite> proved that selecting the shortest unweighted augmenting path (a.k.a. shortest geodesic path) guarantees that <eqn>O(n^3)</eqn> augmentations suffice for optimization.

<eqn>O(nm^2)</eqn> running time: each augmenting path can be found in <eqn>O(m)</eqn> time, every time at least one of the <eqn>m</eqn> edges becomes saturated, the distance from the saturated edge to the source along the augmenting path must be longer than last time it was saturated, and that length is at most <eqn>n</eqn>.

Another property of this algorithm is that the length of the shortest augmenting path increases monotonically.
</text>

<text>
The standard reference for network flow algorithms and their history is <cite>Ahuja et al. 1993</cite>. 
The fastest known general network flow algorithm runs in <eqn>O(nm \log(n^2/m))</eqn> time <cite>Goldberg and Tarjan 1988</cite>.
</text>

<!-- Preflow-push -->

<text>
++ Kleinberg 7 + 13.2
</text>

<text>
++ @ MIT Graphs - Network flow
</text>


<text>
VARIANTS

- multiple sources and/or sinks? - No problem. We can handle
this by modifying the network to create a vertex to serve as a super-source
that feeds all the sources, and a super-sink that drains all the sinks.

- if all arc capacities are identical, either 0 or 1? - Faster algorithms
exist for 0-1 network flows.

- if all my edge costs are identical? - Use the simpler and faster algorithms
for solving maximum flow as opposed to minimum-cost flow. Max-flow
without edge costs arises in many applications, including edge/vertex connectivity
and bipartite matching.

- What if I have multiple types of material moving through the network? - In
a telecommunications network, every message has a given source and destination.
Each destination needs to receive exactly those calls sent to it, not
an equal amount of communication from arbitrary places. This can be modeled
as a multicommodity flow problem, where each call defines a different
commodity and we seek to satisfy all demands without exceeding the total
capacity of any edge. Linear programming will suffice for multicommodity flow if fractional flows
are permitted. Unfortunately, integral multicommodity flow is NP-complete,
even for only two commodities.

<text>
++ Kleinberg: Circulations with demands
</text>

</text>

<document>
<tag>graph-minimum-cuts</tag>
<title>Minimum cuts</title>

<text>
A set of edges whose deletion separates s from t is called an s-t cut. Since no s-t flow can exceed the weight of the minimum s-t cut, finding the maximum flow is equivalent to discovering the minimum cut. In fact, the maximum s-t flow equals the weight of the minimum s-t cut. This maximum-flow, minimum-cut theorem is due to <cite>Ford and Fulkerson 1962</cite>.
</text>

<text>
Therefore, flow algorithms can be used to solve edge and vertex connectivity problems that are employed to answer questions such as
What is the smallest subset of vertices (or edges) whose deletion will disconnect G? Or which will separate s from t? Such problem often appear when stuydying network reliability.
</text>

<text>
"The edge (vertex) connectivity of a graph G is the smallest number of edge
(vertex) deletions sufficient to disconnect G. There is a close relationship between
the two quantities. The vertex connectivity is always less than or equal to the
edge connectivity, since deleting one vertex from each edge in a cut set succeeds
in disconnecting the graph. But smaller vertex subsets may be possible. The minimum
vertex degree is an upper bound for both edge and vertex connectivity, since
deleting all its neighbors (or cutting the edges to all its neighbors) disconnects the
graph into one big and one single-vertex component"
</text>

<text>
- Simple depth-first or breadth-first traversals suffice to identify all connected components in linear time, including strongly-connected and weakly-connected components in directed graphs.

- Weak links: We say that G is biconnected if no single vertex deletion is sufficient to disconnect G. Any vertex that is such a
weak point is called an articulation vertex. A bridge is the analogous concept for edges, meaning a single edge whose deletion disconnects the graph. Linear-time algorithms for identifying articulation vertices and bridges also exist.

- Finding the smallest cut-set that separates a given pair of vertices can be solved using network flow algorithms. 

- Vertex connectivity is characterized by Menger's theorem, which states that a graph is k-connected if and only if every pair of vertices is joined by at least k vertex-disjoint paths. Network flow can again be used to perform this calculation, since a flow of k between a pair of vertices implies k edge-disjoint paths. we construct a graph G' such that any set of edge-disjoint paths in G' corresponds to vertex-disjoint paths in G. This is done by replacing each vertex vi of G with two vertices vi,1 and vi,2, such that edge (vi,1, vi,2)  G' for all vi G, and by replacing every edge (vi, x) ¸ G by edges (vi,j, xk), j = k {0, 1} in G'. Thus two edge-disjoint paths in G' correspond to each vertex-disjoint path
in G.

A non-flow-based algorithm for edge k-connectivity in O(kn2) is due to Matula <cite>Matula 1987</cite>.
Faster k-connectivity algorithms are known for certain small values of k. All three-connected components of a graph can be generated in linear time <cite>Hopcroft and Tarjan 1973b</cite>, while O(n2) suffices to test 4-connectivity <cite>Kanevsky and Ramachandran 1991</cite>

- Graph partitioning, i.e. splitting the graph into smaller components with specific properties, is, however, an NP-complete problem. Good heuristic techniques exist for this problem <cite>Karypis and Kumar 1999</cite>, which has important applications (e.g. in VLSI circuit layout).

<!-- Tools:
- Chaco: partition graphs for parallel computing applications http://www.cs.sandia.gov/¡«bahendr/chaco.html.
- METIS (http://glaros.dtc.umn.edu/gkhome/views/metis)successfully partitioned graphs with over 1,000,000 vertices.
- Scotch (http://www.labri.fr/perso/pelegrin/scotch/)
- JOSTLE (http://staffweb.cms.gre.ac.uk/¡«wc06/jostle/)
-->

</text>

</document>

<document>
<tag>graph-matching</tag>
<title>Matching</title>

<text>
A matching in a graph <eqn>G=(V,E)</eqn> is a subset of edges <eqn>E' \subset E</eqn> such that no two edges in <eqn>E'</eqn> share a vertex.
</text>

<text>
++ Kleinberg 404-414
</text>

<text>
PROBLEM
Find the largest set of edges E' from E such that each
vertex in V is incident to at most one edge of E'.
</text>

<text>
APPLICATION: Assignment problems, with/without weights (maximum cardinality matching/ maximum weight matching).
- Assignment problems (jobs to be done and people who can do them, male and female customers of a dating web site...)
</text>

<text>
Matching theory and algorithms <cite>Lovasz and Plummer 1986</cite>
</text>

<text>
++ CS 
      A matching in a graph is a subset of edges
that have no common incident vertices. It captures the notion of pairing
off compatible vertices. The maximum matching problem is that of finding
a matching of maximum cardinality. Hall and Tutte gave necessary and
sufficient conditions for a graph to have a perfect matching - a matching
in which all vertices are matched. Edmonds gave a polynomial-time algorithm
for finding a maximum matching. Edges may be assigned weights that denote
the advantage of including them in a matching. Maximum-weight bipartite
matching, also known as the assignment problem, arises in many applications.
Algorithms for maximum cardinality and maximum-weight matchings have been
well studied.  See <cite>Ahuja et al. 1993</cite>,<cite>Bondy and Murty 1976</cite>, 
, and <cite>Papadimitriou and Steiglitz 1982</cite>
for more details.
</text>

<text>
Standard algorithms for bipartite matching based on network flows:
</text>

<text>
Remember that a graph is bipartite (or two-colorable) if its vertices can be divided into two sets, L and R, so that every edge in the graph connects one vertex from the first set to a different vertex on the second set.
</text>
<text>
The largest possible bipartite matching can be found using network flow. Just add a source node s connected to every vertex in L with capacity 1, add a sink node t connected to every node in R, and assign each edge in the bipartite graph a capacity of 1. The maximum flow from s to t determines the largest matching in the bipartite graph.
</text>

<text>
The best algorithm for maximum bipartite matching, based on Hopcroft and Karp's algorithm <cite>Hopcroft and Karp 1973</cite>, repeatedly finds the shortest augmenting paths instead of using network flow, and runs in O(\sqrt{n}m) <cite>Micali and Vazirani 1980</cite>.
</text>

<text>
Classic algorithm: Hungarian algorithm for bipartite matching <cite>Kuhn 1955</cite>. The Hungarian algorithm runs in O(n(m + n log n)) time.
</text>

<text>
Matching based on preferences (e.g. matching medical residents to hospitals or students to university courses): O(n^2) time using Galey and Shapley's algorithm <cite>Gale and Shapley 1962</cite>.
</text>


<text>
@ MIT Graphs - Matching
</text>


</document>

</document>





<!-- Graph isomorphism -->

<document>
<tag>graph-isomorphism</tag>
<title>Graph isomorphism</title>

<text>
Find a (or all) mapping f from the vertices of G to the vertices of H such that G and H are identical; i.e. (x, y) is an edge of G iff (f(x), f(y)) is an edge of H.
</text>

<text>
"We need some terminology to settle what is meant when we say two graphs
are the same. Two labeled graphs G = (Vg,Eg) and H = (Vh,Eh) are identical
when (x, y) in Eg iff (x, y) in Eh. The isomorphism problem consists of finding a
mapping from the vertices of G to H such that they are identical. Such a mapping
is called an isomorphism; the problem of finding the mapping is sometimes called
graph matching."

"A mapping of a graph to itself is called an automorphism. For example, the complete graph Kn has n! automorphisms
(any mapping will do), while an arbitrary random graph is likely to have few or perhaps only one, since G is always identical to itself"
</text>

<text>
APPLICATIONS
e.g. 

- Pattern matching: The structure of chemical compounds are naturally described
by labeled graphs, with each atom represented by a vertex. Identifying all
molecules in a structure database containing a particular functional group is an
instance of subgraph isomorphism testing.

- Identifying symmetries (automorphism)

- indexing graph databases by small substructures (and doing expensive isomorphism
tests only against those containing the same substructures as the query graph).
</text>

<text>
@ MIT Interactive proofs and zero knowledge
</text>


<text>
VARIATIONS

- "Subgraph isomorphism asks whether there is a subset of edges and vertices of H that is
isomorphic to a smaller graph G."

- "Induced subgraph isomorphism asks whether there is a subset of vertices of H whose deletion leaves a subgraph isomorphic
to a smaller graph G. For induced subgraph isomorphism, (1) all edges of G must be present in H, and (2) no non-edges of G can be present in H."

e.g. Clique happens to be an instance of both subgraph isomorphism problems, while
Hamiltonian cycle is only an example of vanilla subgraph isomorphism

</text>

<text>
SOLUTION: Backtracking

The basic algorithm backtracks through
all n! possible relabelings of the vertices of graph h with the names of vertices
of graph g, and then tests whether the graphs are identical. Of course, we can
prune the search of all permutations with a given prefix as soon as we detect any
mismatch between edges whose vertices are both in the prefix.

++ Labels and related constraints can be factored into any backtracking algorithm.


++ the real key to efficient isomorphism testing is to preprocess the vertices
into "equivalence classes," partitioning them into sets of vertices so that two
vertices in different sets cannot possibly be mistaken for each other. All vertices
in each equivalence class must share the same value of some invariant that is independent
of labeling. Possibilities include:

- Vertex degree  This simplest way to partition vertices is based on their degreethe number of edges incident on the vertex. Two vertices of different degrees cannot be identical. This simple partition can be a big win, but won't do much for regular (equal degree) graphs.

- Shortest path matrix  For each vertex v, the all-pairs shortest path matrix defines a multiset of n-1 distances representing
the distances between v and each of the other vertices. Any two identical vertices must define the exact same multiset of distances, so we can partition the vertices into equivalence classes defining identical distance multisets.

- Counting length-k paths - Taking the adjacency matrix of G and raising it to the kth power gives a matrix where Gk[i, j] counts the number of (nonsimple) paths from i to j. For each vertex and each k, this matrix defines a multiset of path-counts, which can be used for partitioning as with distances above. You could try all 1 le k le n or beyond, and use any single deviation as an
excuse to partition.

Using these invariants, you should be able to partition the vertices of most graphs into a large number of small equivalence classes. Finishing the job off with backtracking should then be short work. We assign each vertex the name of its equivalence class as a label, and treat it as a labeled matching problem. It is harder to detect isomorphisms between highly-symmetric graphs than it is with random graphs because of the reduced effectiveness of these equivalence-class partitioning heuristics.



++ Faster algorithms exist for trees and planar graphs. 

- Trees (e.g. NLP, parsing):  Begin with the leaves of both trees and work inward toward the center. Each vertex in one tree is assigned a label representing the set of vertices in the second tree that might possibly be isomorphic to it, based on the constraints of labels and vertex degrees. For example, all the leaves in tree T1 are initially potentially equivalent to all
leaves of T2. Now, working inward, we can partition the vertices adjacent to leaves in T1 into classes based on how many leaves and non-leaves they are adjacent to. By carefully keeping track of the labels of the subtrees, we can make sure that we have the same distribution of labeled subtrees for T1 and T2. Any mismatch means T1 != T2, while completing the process partitions the
vertices into equivalence classes defining all isomorphisms. A linear-time tree isomorphism algorithm
for both labeled and unlabeled trees is presented in [AHU74].

-
</text>


<text>
Practical algorithms for graph isomorphism:
- <cite>McKay 1981</cite>
- <cite>Corneil and Gotlieb 1970</cite>
- <cite>Schmidt and Druffel 1976</cite>
- <cite>Ullman 1976</cite>
</text>


<text>
++ http://dabacon.org/pontiff/?p=4148
</text>

<text>
While they seem to perform well on random graphs, a major drawback of these algorithms is their exponential time performance in the worst case.
</text>

<text>
Best known algorithm: Luks (1983) has run time <eqn>2^O(\sqrt{n \log n})</eqn> for graphs with n vertices.
</text>

<text>
In <q>Canonical labeling of graph</q> <cite>Babai and Luks 1983</cite>:  subexponential time algorithm for a graph with n vertices.
</text>

<text>
Isomorphism for many special classes of graphs can be solved in polynomial time, and in practice graph isomorphism can often be solved efficiently:
</text>

<list>

<item>Trees <cite>Kelly 1957</cite> @ <cite>Aho et al. 1974</cite></item>


<item>Planar graphs <cite>Hopcroft and Wong 1974</cite></item>

<item>Maximal outerplanar graphs <cite>Beyer et al. 1979</cite></item>

<item>Interval graphs <cite>Lueker and Booth 1979</cite></item>

<item>Permutation graphs <cite>Colbourn 1981</cite></item>

<item>Trivalent graphs <cite>Galil et al. 1987</cite></item>

<item>Partial k-trees <cite>Bodlaender 1990</cite></item>


<item>Bounded-parameter graphs: 
- Graphs of bounded genus <cite>Miller 1980</cite> <cite>Filotti and Mayer 1980</cite>

- graphs of bounded degree <cite>Luks 1982</cite>

- graphs with bounded eigenvalue multiplicity <cite>Babai et al. 1982</cite>

- k-Contractible graphs (a generalization of bounded degree and bounded genus) <cite>Miller 1983a</cite> <cite>Miller 1983b</cite> 

- Color-preserving isomorphism of colored graphs with bounded color multiplicity (i.e., at most k vertices have the same color for a fixed k)
<cite>Luks 1986</cite>
</item>


</list>

<text>
SOFTWARE: ell now that you've gone and starting learning about some computational complexity in relationship to graph isomorphism, it's probably a good time to stop and look at actual practical algorithms for graph isomorphism.  The king of the hill here, as far as I know, is the program nauty (No AUTomorphism, Yes?) by Brendan D. McKay.  http://cs.anu.edu.au/people/bdm/nauty/ <cite>McKay 1981</cite>


"The best known isomorphism testing program is nauty (No
AUTomorphisms, Yes?) - set of very efficient C language procedures for determining
the automorphism group of a vertex-colored graph. Nauty is also able to
produce a canonically-labeled isomorph of the graph, to assist in isomorphism testing.


The VFLib graph-matching library contains implementations for several different
algorithms for both graph and subgraph isomorphism testing. This library
has been widely used and very carefully benchmarked [FSV01]. It is available at
http://amalfi.dis.unina.it/graph/.

GraphGrep [GS02] (http://www.cs.nyu.edu/shasha/papers/graphgrep/) is a representative
data mining tool for querying large databases of graphs.

Valiente [Val02] has made available the implementations of graph/subgraph
isomorphism algorithms for both trees and graphs in his book [Val02]. These C++
implementations run on top of LEDA (see Section 19.1.1 (page 658)), and are
available at http://www.lsi.upc.edu/¡«valiente/algorithm/.

</text>

<text>
REFERENCES @ SKIENA
Graph isomorphism is an important problem in complexity theory. Monographs
on isomorphism detection include [Hof82, KST93]. Valiente [Val02] focuses on algorithms
for tree and subgraph isomorphism. See [FSV01] for performance comparisons between different graph
and subgraph isomorphism algorithms.

Polynomial-time algorithms are known for planar graph isomorphism [HW74] and for
graphs where the maximum vertex degree is bounded by a constant [Luk80]. The all-pairs
shortest path heuristic is due to [SD76], although there exist nonisomorphic graphs that
realize the exact same set of distances [BH90]. 

A problem is said to be isomorphism-complete if it is provably as hard as isomorphism.
Testing the isomorphism of bipartite graphs is isomorphism-complete, since any graph
can be made bipartite by replacing each edge by two edges connected with a new vertex.
Clearly, the original graphs are isomorphic if and only if the transformed graphs are.
</text>
</document>



<!-- Algorithm catalog -->

<document>
<tag>graph-algorithm-catalog</tag>
<title>A catalog of graph algorithms</title>

<text>
Graphs of order n (nodes) and size m (links)...
</text>

<document>
<tag>graph-algorithm-catalog-linear</tag>
<title>Linear-time algorithms</title>


<text>
The following problems can be solved with algorithms that run in linear time (i.e. <eqn>O(n+m)</eqn>):
</text>

<list>

<item>Graph traversal, either BFS, DFS, or best-first search (as in A*).</item>

<item>Shortest paths in unweighted graphs (based on BFS): Finding the shortest paths and also counting the number of different shortest paths.</item>

<item>Identifying connected and strongly-connected components (based on graph traversal).</item>

<item>Bipartite testing (based on graph traversal) = 2-coloring.</item>

<item>Cycle detection (based on DFS).</item>

<item>Identification of articulation points, i.e. graph biconnectedness (based on DFS).</item>

<item>Identification of bridges, i.e. edge-biconnectedness (based on DFS).</item>

<item>Topological sorting (based on DFS).</item>

<item>Single-source shortest paths on DAGs, even with negative weights.</item>

<item>Shortest paths, longest paths, and hamiltonian paths on DAGs (topological sorting):
"The problem of finding the longest path in a DAG can be solved in linear time using
dynamic programming. Conveniently, the algorithm for finding the shortest
path in a DAG (presented in Section 15.4 (page 489)) does the job if we replace min with max.
+DAGs are the most interesting case of longest path for which efficient algorithms exist."
</item>

<item>Maximum matching in a tree.</item>

<item>The maximum independent set of a tree (equivalent to maximum tree matching on the complement graph): "(1)
stripping off the leaf nodes, (2) adding them to the independent set, (3) deleting all adjacent nodes, and then (4) repeating from the first step on the resulting trees until it is empty."</item>

<item>Tree partitioning: "Certain special graphs always have small separators, that partition the vertices
into balanced pieces. For any tree, there always exists a single vertex
whose deletion partitions the tree so that no component contains more than
n/2 of the original n vertices. These components need not always be connected;
consider the separating vertex of a star-shaped tree. This separating
vertex can be found in linear time using depth first-search."
<!--Every planar graph has a set of O(\sqrt{n}) vertices whose deletion leaves no component with
more than 2n/3 vertices. These separators provide a useful way to decompose geometric models, which are often defined by planar graphs.-->
</item>

<item>Tree isomorphism</item>

<item>Eulerian cycle: the shortest tour visiting each edge of G 
- Checking is an Eulerian cycle exists: verify that the graph is connected using DFS or BFS, and then count the number of odd-degree vertices.
- Constructing the cycle in linear time: Use DFS to find an arbitrary cycle in the graph. Delete this cycle and
repeat until the entire set of edges has been partitioned into a set of edge-disjoint cycles. These cycles will have common vertices (since the graph is connected), and so can be spliced together as an "eight" at a shared vertex. By so splicing all the extracted cycles together, we construct a single circuit containing all of the edges.
</item>


<item>Eulerian path</item>


</list>

</document>

<document>
<tag>graph-algorithm-catalog-almost-linear</tag>
<title>Almost linear-time algorithms</title>


<text>
The following algorithms are almost linear, i.e. <eqn>O(n \log n)</eqn>:
</text>

<list>

<item>Minimum spanning trees (using either Prim's or Kruskal's algorithm).</item>

<item>Single-source shortest paths (using Dijkstra's algorithm), only for positive weights.</item>

</list>


<text>
NOTE: When working with large data sets, only linear (<eqn>O(n)</eqn>) or near linear (e.g. <eqn>O(n \log n)</eqn>) are likely to be suitable.
</text>


</document>

<document>
<tag>graph-algorithm-catalog-polynomial</tag>
<title>Polynomial-time algorithms</title>

<text>
The following algorithms require polynomial time:
</text>

<list>

<item>All-pairs shortest paths (e.g. Floyd's algorithm).</item>

<item>Transitive closure (i.e. all-pairs reachability) and reduction.</item>

<item>Finding a directed cycle of minimum total weight, <eqn>O(n^3)</eqn>.</item>

<item>Maximum network flow.</item>

<item>Minimum cuts.</item>

<item>Maximum cuts in planar graphs:
"A polynomial-time algorithm to find maximum cuts in planar graphs exists. <cite>Hadlock 1975</cite>
The planar separator theorem and an efficient algorithm for finding such a separator
are due to Lipton and Tarjan [LT79, LT80]. For experiences in implementing planar
separator algorithms, see [ADGM04, HPS+05]."
</item>

<item>Maximum bipartite matching.</item>

<item>Minimum vertex cover problem and maximum independent set problems for bipartite graphs (since the maximum bipartite matching is equal in size to the minimum vertex cover in bipartite graphs...)</item>

<item>Minimum-size edge cover (i.e. a set of edges, of minimum size, such that each vertex in the graph is incident to at least one edge from the set). - "Cover all vertices using few edges (edge cover problem): Smallest set of edges such that each vertex is included in one of the edges. In fact, edge cover can be solved efficiently by finding a maximum cardinality matching and then selecting arbitrary edges to account for the unmatched vertices."</item>

<item>Chinese postman problem (a generalization of the Eulerian circuit) introduced by <cite>Kwan 1962</cite>, hence its name: the shortest tour visiting each edge of G at least once. This minimum
cycle will never visit any edge more than twice, so good tours exist for any road network. The optimal postman tour can be constructed by adding the appropriate edges to the graph G to make it Eulerian. Specifically, we find the shortest path between each pair of odd-degree vertices in G. Adding a path between two odd-degree vertices in G turns both of them to even-degree, moving G closer to becoming an Eulerian graph. Finding the best set of shortest paths to add to G reduces to identifying a minimum-weight perfect matching in a bipartite graph G' <cite>Edmonds and Johnson 1973</cite>. For undirected graphs, the vertices of G' correspond the odd-degree vertices of G, with the weight of edge (i, j) defined to be the length of the shortest path from i to j in G. For directed graphs, the vertices of G' correspond to the degree-imbalanced vertices from G, with the bonus that all edges in G' go from out-degree deficient vertices to in-degree deficient ones. Thus, bipartite matching algorithms suffice when G is directed. Once the graph is Eulerian, the actual cycle can be extracted in linear time.
</item>

<item>Bipartite graph edge coloring: Bipartite graphs can be edge-colored in polynomial time [Sch98].</item>

<item>Planar graph isomorphism</item>

</list>

</document>

<document>
<tag>graph-algorithm-catalog-hard</tag>
<title>Hard problems</title>

<text>
++ CS 
        Many optimization problems such as the well known traveling-salesman
problem are NP-hard. Polynomial-time algorithms are not known for these
problems, and many researchers believe that they do not exist. Hence
heuristics that produce suboptimal solutions have been a subject of much
research, especially in the last decade. Other problems that arise in
different contexts are the Steiner tree problem and the graph-coloring
problem. In spite of being NP-hard, large instances of the Steiner-tree
problem and the traveling-salesman problem can be solved in practice,
that is, solutions that are very close to optimal can be obtained. On the
other hand, even small instances of the graph-coloring problem are hard to
solve. Formal evidence for the hardness of the approximability of problems
such as clique, independent set, and coloring was obtained recently. The
complexity of graph isomorphism is a major open problem. Many other problems
on graphs are known to be NP-hard.
</text>

<note>
<text>
NP (non-deterministic polynomial) is the set of all decision problems whose solutions can be verified in polynomial time (or, alternatively, the set of decision problems that can be solved in polynomial time on a nondeterministic Turing machine). That is, even though any given solution to such a problem can be verified quickly (in polynomial time), there is no known efficient way to discover a solution to the problem.
</text>

<text>
In computational complexity theory, NP-hard problems are, at least as hard as the hardest problems in NP (i.e. any NP problem can be converted into L by a transformation of the inputs in polynomial time or, in theoretical parlance, every problem in NP is reducible to L in polynomial time). A problem is NP-complete when it is in NP and also NP-hard. Hence, NP-complete is a subset of NP, which is itself a subset of NP-hard.
</text>

<text>
This the P=NP? question is not resolved, and hence we do not know whether solution verification is really easier than solution discovery for NP problems, we could say that NP problems are <q>not-necessarily polynomial time</q> <cite>Skiena 2008</cite>.
</text>

<text>
More general classes exist, such as PSPACE: the set of all decision problems which can be solved by a Turing machine using a polynomial amount of space. Formally, <eqn>P \subseteq NP \subseteq PSPACE</eqn>. Some graph problems are known to be PSPACE-complete, such as the Canadian Traveler Problem (CTP) <cite>Papadimitriou and Yannakakis 1989</cite> <cite>Papadimitriou and Yannakakis 1991</cite>, a generalization of the shortest path problem to graphs that are partially observable (i.e. the graph is revealed while it is being explored) that has many practical applications.
</text>

</note>


<text>
The following problem occupies a singular position within the complexity hierarchy, since it is not known whether the problem has a fast algorithm that solves it ot is NP-complete:
</text>

<list>

<item>Isomorphism testing, i.e. checking whether the topological structure of two graphs are identical (typically solved using backtracking)</item>

</list>


<text>
The following algorithms are known to be NP-hard (most of them are, in fact, NP-complete): 
- Permutations/ORDERING, e.g. TSP, bandwidth, isomorphism
- Selection: COVERING/PARTITIONING, SUBGRAPHS
- Network design
- Partitioning
</text>

<list>

<item>Hamiltonian circuit on unweighted graphs (both directed and undirected), remains NP-complete even for directed planar graphs (graphs that can be embedded in the plane) and for cubic planar undirected graphs (graphs in which all vertices have degree three).

M. R. Garey, D. S. Johnson, and L. Stockmeyer. Some simplified NP-complete problems. Proceedings of the sixth annual ACM symposium on Theory of computing, p. 47-63. 1974.

<cite>Skiena 2008</cite>:

</item>


<item>The traveling salesman problem (TSP) on weighted graphs, one of the most intensively studied problems in optimization. http://en.wikipedia.org/wiki/Travelling_salesman_problem

APPLICATIONS: 
- transportation and routing problems.
- optimizing tool paths for manufacturing equipment.
- cheapest airfare between two point (Commercial air fares do not satisfy the triangle inequality; relaxed constraint: visiting the same hub several times: TSP with repeated vertices is easily solved by using any conventional TSP code on a new cost matrix D, where D(i,j) is the shortest path distance from i to j. This matrix can be constructed by solving an all-pairs shortest path and satisfies the triangle inequality.)

<cite>Skiena 2008</cite>:

OPTIMAL SOLUTION: Two different approaches if you insist
- Cutting plane methods model the problem as an integer program, then solve the linear programming relaxation of it. Additional constraints designed to force integrality are added if the optimal solution is not at an integer point. 
- Branch-and-bound algorithms perform a combinatorial search while maintaining careful upper and lower bounds on the cost of a tour. </item>


<item>Hamiltonian cycle (a special case of TSP): TSP tour of cost n

OPTIMIZATIONS: Sufficiently dense graphs always contain Hamiltonian cycles. Further, the cycles implied by such sufficiency conditions can be efficiently constructed

OPTIMAL SOLUTION: Backtracking
</item>

<item>Hamiltonian path on unweighted graphs (a special case of TSP). The Hamiltonian path problem for graph G is equivalent to the Hamiltonian cycle problem in a graph H obtained from G by adding a new vertex and connecting it to all vertices of G.</item>

<item>Longest simple path

APPLICATION: "pattern recognition problems. Let the vertices in the graph
correspond to possible symbols, with edges linking pairs of symbols that might
occur next to each other. The longest path through this graph is a good candidate
for the proper interpretation."
</item>

<item>Longest circuit</item>

<item>Paths with forbidden pairs or vertices</item>




<item>Minimum-size vertex cover: A vertex cover is a subset of vertices <eqn>V' \in V</eqn> such that every edge in E contains at least one vertex from V'. Obviously, the set of all vertices is a vertex cover. The problem of finding a minimum vertex cover is a classical optimization problem in computer science a, one of Karp's 21 NP-complete problems <cite>Karp 1972</cite>.

EQUIVALENCY: "Vertex cover and independent set are very closely related graph problems. Since
every edge in E is (by definition) incident on a vertex in any cover S, there can be
no edge both endpoints are in V - S. Thus, V - S must be an independent set.
Since minimizing S is the same as maximizing V -S, the problems are equivalent."

RELATED TO: Any program for computing the maximum clique in a graph
can be applied to vertex cover by complementing the input graph and selecting the
vertices which do not appear in the clique.
</item>

<item>Dominating set problem (a variant of the vertex cover problem): "Cover all vertices using few vertices (dominating set problem): The smallest set of vertices D such that every vertex in V - D is adjacent to at
least one vertex in the dominating set D. Every vertex cover of a nontrivial
connected graph is also a dominating set, but dominating sets can be much
smaller. Any single vertex represents the minimum dominating set of complete
graph Kn, while n-1 vertices are needed for a vertex cover. Dominating
sets tend to arise in communications problems, since they represent the hubs
or broadcast centers sufficient to communicate with all sites/users.
Dominating set problems can be easily expressed as instances of set cover, another NP problem:
Each vertex vi defines a subset of vertices consisting of itself plus all the vertices it is adjacent to. 
</item>




<item>Maximum-size independent set <cite>Karp 1972</cite>: An independent set is a set of vertices U such that no edge in E is incident on two vertices of U. A maximum independent set is a largest independent set and the maximum independent set problem is known to be NP-hard. A set is independent if and only if its complement is a vertex cover. The sum of the size of the maximum independent set and the size of a minimum vertex cover is the number of vertices in the graph.

RELATED TO graph matching (dual problem), maximal clique (equivalent on the complement graph), vertex coloring (each color defines an independent set; consequence: graphs with small chromatic numbers, such as planar and bipartite graphs, have large independent sets).

APPLICATIONS: 
- "The need to find large independent sets arises in facility dispersion
problems, where we seek a set of mutually separated locations"
- "Independent sets (also known as stable sets) avoid conflicts between elements,
and hence arise often in coding theory and scheduling problems."
</item>



<item>Maximum cliques, <eqn>O(3^{n/3}) = O(1.4422^n)</eqn>, since any n-vertex graph has at most any n-vertex graph has at most <eqn>3^{n/3}</eqn> maximal cliques;  http://en.wikipedia.org/wiki/Clique_problem,
+ Application: identifying clusters of related objects, ref. more on the chapter on network motifs.</item>

<item>Dense subgraph, i.e. a subgraph with exactly k vertices and at least y edges.</item>

<item>Planar subgraph, i.e. a subgraph with at least k edges that can be embedded on the plane.</item>

<item>Eulerian subgraph, i.e. a subgraph that contains an Eulerian cycle.</item>

<item>Bipartite subgraph, i.e. a subgraph with at least k edges that is bipartite.</item>

<item>Cubic subgraph, i.e. a cubic subgraph with at least k edges.</item>

<item>Degree-bounded connected subgraph, i.e. a subgraph with at least k edges so that the resulting subgraph is connected and no vertex degree exceeds d.</item>

<item>Transitive subgraph, i.e. (u,w) (w,v) implies (u,v).</item>

<item>Uniconnected subgraph, i.e. subgraph containing at most one directed path between any pair of vertices.</item>

<item>Minimum k-connected subgraph, i.e. k-connected subgraph (cannot be disconnected by removing fewer than k vertices).</item>



<item>Subgraph isomorphism, i.e. given two graphs G and H as input, determine whether G contains a subgraph that is isomorphic to H.</item>

<item>Largest common subgraph, i.e. given two graphs G and H as input, determine their largest isomorphic subgraphs.</item>

<item>Graph contractability, i.e. given two graphs G and H as input, can a graph isomorphic to H be obtained by a sequence of edge contractions? (when two adjacent vertices are replaced by a single one that is adjacent to exactly those vertices that were adjacent to at least one of the original vertices).</item>

<item>Minimum equivalent digraph. i.e. subgraph that contains a directed path from u to v only when the original graph does.</item>


<item>The Bandwidth problem, which seeks the linear ordering of the vertices that minimizes the length of the longest edge. which can be visualized as placing the vertices of a graph at distinct integer points along the x-axis so that the length of the longest edge is minimized.  Such placement is called linear graph arrangement, linear graph layout or linear graph placement.


e.g. the problem of placement of a set of standard cells in a singe row with the goal of minimizing the maximal propagation delay (which is assumed to be proportional to wire length).

e.g. solving linear systems (Gaussian elimination can be sped up when working with banded matrices, where the bandwidth is the distance from the furthest non-zero entry to the matrix diagonal)

Variants
- Linear arrangement: minimize the sum of the edge lengths
- Profile minimization: minimize the sum of one-way distances
- The pathwidth problem

++ applications in VLSI design, graph drawing, and computational linguistics.</item>

<item>Steiner trees, i.e. find the shortest interconnect for a given set of objects (as MST, but including the possibility of adding extra intermediate nodes).

++ Skien1 16.10
</item>

<item>Constrained spanninig trees: degree-constrained spanning tree (a spanning tree with no vertex with degree above k), maximum-leaf spanning trees, (a spanning tree with k or more leaves), shortest-total-path-length spanning tree (i.e. minimizing the sum of the length of the path in the tree between every pair of nodes), bounded-diameter spanning tree (no simple path with more than d edges)...</item>


<item>Matching on hypergraphs, i.e. when more than two vertices are involved in a single edge. In fact, 3-dimensional matching is known to be NP-hard whereas bipartite matching (a.k.a. 2-dimensional matching) is in P.</item>


<item>
Graph partitioning: a small cut that partitions
the vertices into roughly equal-sized pieces = "Partition the vertices into m roughly equal-sized subsets such that the total edge cost spanning the subsets is at most k."


APPLICATIONS:

- Clustering: Graph partition also arises when we need to cluster the vertices into logical components. If edges link similar pairs of objects, the clusters remaining after partition should reflect coherent groupings.

- Divide and conquer arises in many divide-and-conquer algorithms, which gain their efficiency by breaking problems into equal-sized pieces such that the respective solutions can easily be reassembled. Minimizing the number of edges cut in the partition usually simplifies the task of merging

- Parallel algorithms. Consider the finite element method, which is used to compute the physical properties (such
as stress and heat transfer) of  geometric models. Parallelizing such calculations requires partitioning the models into equal-sized pieces whose interface is small. This is a graph-partitioning problem, since the topology of a geometric model is
usually represented using a graph.

- Data locality: Large graphs are often partitioned into reasonable-sized pieces to improve data locality 

- Drawing: or make less cluttered drawings.
</item>


<item>Minimum cut into bounded sets, i.e. imposing restrictions on the size of the disjoint sets that partition the network.
"Minimum cut set  The smallest set of edges to cut that will disconnect a
graph can be efficiently found using network flow  algorithms. The smallest
cutset might split off only a single vertex, so the resulting partition could be
very unbalanced."
</item>

<item>Maximum cut (the max-cut problem), i.e. finding a cut whose size is at least the size of any other cut. <cite>Karp 1972</cite>

APPLICATION e.g. "Given an electronic circuit specified by a graph, the maximum
cut defines the largest amount of data communication that can simultaneously
occur in the circuit. The highest-speed communications channel should thus
span the vertex partition defined by the maximum edge cut."
</item>


<item>Clique cover (a.k.a. partition into cliques), i.e. determining whether the vertices of a graph can be partitioned into k cliques. </item>


<item>Feedback vertex set, i.e. a set of vertices whose removal leaves a DAG (a graph without cycles). ++ Skiena 16.11 </item>

<item>A feedback arc set (FAS) or feedback edge set is a set of edges which, when removed from the graph, leave a DAG.  A minimal feedback arc set has the additional property that, if the edges in it are reversed rather than removed, then the graph remains acyclic. Finding a small edge set with this property is a key step in layered graph drawing ++ Skiena 16.11 </item>


<item>Chromatic number / graph k-colorability / vertex coloring: assignment of labels traditionally called "colors" to elements of a graph subject to certain constraints. 

- The smallest number of colors sufficient to vertex-color a graph is its chromatic number

In its simplest form, it is a way of coloring the vertices of a graph such that no two adjacent vertices share the same color; this is called a vertex coloring. Using dynamic programming and a bound on the number of maximal independent sets, k-colorability can be decided in time and space O(2.445^n) <cite>Lawler 1976</cite> Using the principle of inclusion-exclusion and Yates's algorithm for the fast zeta transform, k-colorability can be decided in time O(n2^n) for any k <cite>Bjorklund et al. 2009</cite>. Faster algorithms are known for 3- and 4-colorability, which can be decided in time O(1.3289^n) <cite>Beigel and Eppstein 2005</cite> and O(1.7504^n) <cite>Byskov 2004</cite>, respectively. Note that 2-colorable graphs are bipartite graphs and bipartite testing can be performed in linear time.

APPLICATIONS
- many scheduling and clustering applications.
- Register allocation in compiler optimization.
- Printed circuit board testing

BOUND: Brook's theorem states that the chromatic number Xi(G) le Delta(G) + 1, where Delta(G) is
the maximum degree of a vertex of G. Equality holds only for odd-length cycles (which
have chromatic number 3) and complete graphs.

RESULT: The most famous problem in the history of graph theory is the four-color problem,
first posed in 1852 and finally settled in 1976 by Appel and Haken using a proof involving
extensive computation. Despite the four-color theorem, it is NP-complete to test whether a
particular planar graph requires four colors or if three suffice. 
</item>


<item>Edge coloring: assigns a color to each edge so that no two adjacent edges share the same color. an edge coloring of a graph is just a vertex coloring of its line graph (a graph that represents the adjacencies between edges of G)

APPLICATIONS
- scheduling applications, typically associated with minimizing the number of noninterfering rounds needed to complete
a given set of tasks.

BOUND
Edge coloring has a better (if less famous) theorem associated with it than vertex
coloring. Vizing's theorem states that any graph with a maximum vertex degree
of \Delta can be edge colored using at most \Delta + 1 colors. To put this in perspective,
note that any edge coloring must have at least \Delta colors, since all the edges incident
on any vertex must be distinct colors. (Vizing [Viz64] and Gupta [Gup66] independently proved that)

INFO
"The minimum number of colors needed to edge color a graph is called its edge-chromatic
number by some and its chromatic index by others. Note that an evenlength
cycle can be edge-colored with 2 colors, while odd-length cycles have an
edge-chromatic number of 3." it is NP-complete to compute the edge-chromatic
number [Hol81]

TRANSFORMATION
Any edge-coloring problem on G can be converted to the problem of finding a
vertex coloring on the line graph L(G), which has a vertex of L(G) for each edge
of G and an edge of L(G) if and only if the two edges of G share a common vertex.
Line graphs can be constructed in time linear to their size, and any vertex-coloring
code can be employed to color them
</item>

<item>Face coloring of a planar graph assigns a color to each face or region so that no two faces that share a boundary have the same color. a face coloring of a planar graph is just a vertex coloring of its planar dual (a vertex for each plane region of G, and an edge for each edge in G joining two neighboring regions). </item>

<item>Achromatic number: Does a graph G have an achromatic number K or greater? i.e. is there a partition of V into k disjoint sets so that each V_i is an independent set.</item>

<item>Other graph partitioning problems: into triangles, into cliques, into isomorphic subgraphs, into Hamiltonian subgraphs, into forests, into perfect matchings... are also known NP-complete problems.</item>

</list>

<text>
Contrast subtle differences: Eulerian path (easy) vs. Eulerian cycle (hard); bipartite matching (easy) vs. hypergraph matching (hard); shortest paths (easy) vs. longest paths (hard); Hamiltonian path on a graph (hard) vs. on a DAG (easy); the minimum spanning tree (easy), which is the undirected variant of the feedback arc set problem (hard)...
</text>

<text>
Even when a problem is NP-complete, algorithms can be devised that solve them in practice, either by employing a systematic search on the problem space (e.g. employing backtracking or branch-and-bound algorithms with substantial pruning) or heuristic methods that might not find the optimal solution but often return a good-enough one (e.g. simulated annealing). Heuristics are also employed by approximation algorithms that can get reasonable solutions with guaranteed bounds; i.e. the solution they provide is related to a lower bound on the optimal solution and imposes limits on how badly the approximation algorithm might perform.
</text>

<list>

<item>
Traveling salesman problem:

Geometric instances inherently satisfy the triangle inequality, so they can exploit performance guarantees from certain heuristics, e.g.

- Minimum spanning trees  This heuristic starts by finding the minimum
spanning tree (MST) of the sites, and then does a depth-first search of the
resulting tree. In the course of DFS, we walk over each of the n - 1 edges
exactly twice: once going down to discover a new vertex, and once going up
when we backtrack. Now define a tour by ordering the vertices by when they
were discovered. If the graph obeys the triangle inequality, the resulting tour
is at most twice the length of the optimal TSP tour. In practice, it is usually
better, typically 15% to 20% over optimal. Furthermore, the running time is
bounded by that of computing the MST, which is only O(n lg n) in the case
of points in the plane (see Section 15.3 (page 484)).

GREEDY ALGORITHMS (without guarantees)
- Incremental insertion methods: "A different class of heuristics starts from a
single vertex, and then inserts new points into this partial tour one at a time
until the tour is complete. The version of this heuristic that seems to work
best is furthest point insertion: of all remaining points, insert the point v into
a partial tour T such that  max_{v \in V} min_i (d(v, vi) + d(v, vi+1))
The gminh ensures that we insert the vertex in the position that adds the
smallest amount of distance to the tour, while the max ensures that we
pick the worst such vertex first... it 'roughs out' a partial tour first before filling in details"


OTHER ALTERNATIVES

- K-optimal tours (Kernighan-Lin, or kopt, class of heuristics): "Applies local refinements to an initially
arbitrary tour in the hopes of improving it. In particular, subsets of k edges
are deleted from the tour and the k remaining subchains rewired to form
a different tour with hopefully a better cost. A tour is k-optimal when no
subset of k edges can be deleted and rewired to reduce the cost of the tour.
Two-opting a tour is a fast and effective way to improve any other heuristic.

- Simulated annealing provides an alternate mechanism to employ edge flips to improve heuristic tours.

ref. book by Applegate, Bixby, Chvatal, and Cook [ABCC07] documents the
techniques they used in their record-setting TSP solvers, as well as the theory and history
behind the problem.
</item>

<item>The Euclidean Traveling Salesman Problem: When the triangle inequality holds, we can approximate the TSP solution by employing minimum spanning trees. Such trees, which connect all the nodes in a graph, can be traversed in depth. That traversal provides a non-Hamiltonian cycle whose length is at most twice the optimal cycle (why? deleting any link from the cycle returns a Hamiltonian path, i.e. a tree, whose weight cannot be greater than that of the MST.
</item>


<item>Vertex cover: Greedy algorithm that chooses an edge at random and adds both intervening nodes to the cover. Since one of the incident vertices must always be on the minimum vertex cover, this simple algorithm efficiently returns a cover that is at most twice as large as the optimal cover.


1) "The simplest heuristic for vertex cover selects the vertex with highest degree,
adds it to the cover, deletes all adjacent edges, and then repeats until the graph is
empty. With the right data structures, this can be done in linear time. However, this cover might be lg n times
worse than the optimal cover for certain input graphs. The example that the greedy algorithm can be as bad as lg n times optimal is due to [Joh74] and presented in [PS98]. "

2) "Fortunately, we can always find a vertex cover whose size is at most twice as
large as optimal. Find a maximal matching M in the graphi.e. , a set of edges no
two of which share a vertex in common and which cannot be enlarged by adding
additional edges. Such a maximal matching can be constructed incrementally, by
picking an arbitrary edge e in the graph, deleting any edge sharing a vertex with e,
and repeating until the graph is out of edges. Taking both of the vertices for each
edge in a maximal matching gives us a vertex cover. Why? Because any vertex
cover must contain at least one of the two vertices in each matching edge just to
cover the edges of M, this cover is at most twice as large as the minimum cover.
This heuristic can be tweaked to perform somewhat better in practice, if not
in theory. We can select the matching edges to kill off as many other edges as
possible, which should reduce the size of the maximal matching and hence the
number of pairs of vertices in the vertex cover. Also, some of the vertices from M
may in fact not be necessary, since all of their incident edges might also have been
covered using other selected vertices. We can identify and delete these losers by
making a second pass through our cover."

"Experimental studies of vertex cover heuristics include [GMPV06, GW97, RHG07]."
</item>


<item>Independent set: "The simplest reasonable heuristic is to find the lowest-degree vertex, add it to the independent set, and then delete it and all vertices adjacent to it. Repeating this process until the graph is empty gives a maximal independent set, in that it can't be made larger by just adding vertices.
</item>


<item>Maximum DAG: The largest possible subset of edges so that the resulting subgraph is acyclic. Any permutation of nodes can be interpreted as a topological sorting of the graph. Removing either left-to-right or right-to-left edges, we keep at least half of the edges in the original graph, hence the solution contains at least half as many edges as the optimum.
</item>


<item>
Graph partitioning:
"Any random vertex partition will expect to cut half of the edges in the graph, since the
probability that the two vertices defining an edge end up on different sides of the partition
is 1/2. Goemans and Williamson [GW95] gave an 0.878-factor approximation algorithm
for maximum-cut, based on semi-definite programming techniques. Tighter analysis of
this algorithm was followed by Karloff [Kar96]."
</item>

</list>

<text>
Alternative heuristics might even get better results for particular instances of the above problems, albeit more complex heuristics do not necessarily guarantee better results. In fact, the result if often the opposite. It should also be noted that some clever postprocessing of the results provided by approximation algorithms might also be worthwhile, since it might improve the approximate solution without weakening the approximation bound.
</text>

</document>


<!--
<figure>
<title>Image example (default file type)</title>
<image scale="20" file="image/cover/0321127420"/>
</figure>

<figure>
<title>Another image example (specific file type)</title>
<image scale="20" file="image/cover/0321117425.jpg" type="jpg"/>
</figure>

<text>
Inline image:
</text>

<image scale="20" file="image/cover/0321117425.jpg" type="jpg"/>
-->

</document>


<document>
<tag>graph-algorithms-notes</tag>
<title>Bibliographic notes</title>


<text>
Many textbooks describe at least part of the material mentioned in this chapter.

<cite>Skiena 2008</cite>, <cite>Kleinberg and Tardos 2005</cite>
<cite>Even 2011</cite> discusses many basic graph algorithms known before 1979.
<cite>Tarjan 1983</cite> book provides a survey of both algorithms and data structures.
<cite>Gibbons 1985</cite>
<cite>Sedgewick 1990</cite> in C, <cite>Sedgewick 2002</cite> and <cite>Sedgewick 2003</cite> in Java (2nd volume devoted to graph algorithms)

</text>


<text>
<cite>Grama et al. 2003</cite> includes a chapter on parallel graph algorithms.

+ <cite>Lynch 1997</cite> textbook on distributed algorithms.
</text>



<text>
Network flow; monographs <cite>Ahuja et al. 1993</cite> <cite>Ford and Fulkerson 1962</cite>; surveys <cite>Goldberg et al. 1990</cite> <cite>Schrijver 2002</cite>; original algorithms <cite> Ford and Fulkerson 1956</cite> <cite>Edmonds and Karp 1972</cite>
</text>

<text>
Matching:  The concept of a matching in a graph is very fundamental and forms
the cornerstone of combinatorial optimization, with an entire book devoted
to it <cite>Lovasz and Plummer 1986</cite>
</text>


<text>

Many graph problems (such as shortest paths, bipartite matching, or network flows) can also be solved as special cases of linear programming.

The books by <cite>Papadimitriou and Steiglitz 1982</cite> and <cite>Ahuja et al. 1993</cite> provide a
comprehensive coverage of combinatorial optimization problems. 

Hochbaum's edited collection <cite>Hochbaum 1996</cite> provides an extensive coverage of work on approximation
algorithms for graph problems. 

Software tools: lp_solve (<url>http://lpsolve.sourceforge.net/</url>), CLP (<url>http://www.coin-or.org</url>), NEOS = Network-Enabled Optimization System @ Argonne National Laboratory, Illinois (submit your own optimization jobs at <url>http://neos.mcs.anl.gov/</url>)

</text>

<text>
NP problems:
- 1972: Landmark paper, "Reducibility Among Combinatorial Problems"  <cite>Karp 1972</cite>, 21 combinatorial and graph theoretical NP-complete problems
- 1979: Catalog containing hundreds of NP problems<cite>Garey and Johnson 1979</cite>, 65 graph theoretical problems and 51 network design problems.

- A compendium of NP optimization problems edited by Pierluigi Crescenzi and Viggo Kann
<url>http://www.nada.kth.se/~viggo/problemlist/</url>

- Wikipedia: More than 3000 known NP-complete problems,
<url>http://en.wikipedia.org/wiki/List_of_NP-complete_problems</url>

</text>

<text>
Approximation algorithms:

- Crescenzi and Kann [ACG+03]
[ACG+03] G. Ausiello, P. Crescenzi, G. Gambosi, V. Kann, S. Marchetti-Spaccamela,
and M. Protasi. Complexity and Approximation: Combinatorial Optimization
Problems and Their Approximability Properties. Springer, 2003.

- Vazirani [Vaz04]
[Vaz04] V. Vazirani. Approximation Algorithms. Springer, 2004.

- Hochbaum [Hoc96]
[Hoc96] D. Hochbaum, editor. Approximation Algorithms for NP-hard Problems. PWS Publishing, Boston, 1996.

- Gonzalez [Gon07]
[Gon07] T. Gonzalez. Handbook of Approximation Algorithms and Metaheuristics. Chapman-Hall / CRC, 2007.
</text>


<text>
Conferences: Many international conferences carry articles on recent advances in graph
algorithms. Some of them are the IEEE Symposium on Foundations of Computer
Science (FOCS), the ACM Symposium on Theory of Computing (STOC), the ACM-SIAM
Symposium on Discrete Algorithms (SODA), and the International Colloquium on
Automata, Languages and Programming (ICALP).
</text>

</document>

</document>
