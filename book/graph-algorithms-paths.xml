<?xml version="1.0"  encoding="ISO-8859-1" ?> 
<!DOCTYPE sys-ents [ 
  <!ENTITY bibliography   SYSTEM "bibliography.xml">
]> 
<?xml-stylesheet type="text/xsl" href="../book.xsl"?>

<!-- Shortest paths -->

<document>
&bibliography;
<tag>graph-shortest-paths</tag>
<title>Shortest paths</title>


<text>
When graphs are weighted, i.e. each edge in the graph is associated to a numerical value, computing their shortest paths is more complicated than performing breadth-first traversals (recall that the BFS tree leads to minimum-number-of-links paths)...
</text>

<text>
Shortest paths in directed acyclic graphs can also be found in linear time: Perform a topological sorting and then process the vertices
from left to right. Observe that <eqn>d(s, j) = min_(x,i) d(s, i) + w(i, j)</eqn> since we already know the shortest path d(s, i) for all vertices to the left of j. The same algorithm (replacing min with max) also suffices to find the longest path in a DAG, which is useful in scheduling applications.
</text>


<note>
<title>The Strategy design pattern</title>

<text>
++ The Strategy design pattern <cite>Gamma et al. 1994</cite>
</text>
</note>


<text>
@ MIT Graphs - Shortest paths
</text>


<!-- Shortest paths: Single source -->

<document>
<tag>graph-shortest-paths-dijkstra</tag>
<title>Single source shortest paths</title>

<text>
Finding the shortest path from s to t in G.
Actually, finding the shortest path from a source node <eqn>s</eqn> to every other node in the network requires the same amount of work...
</text>

<text>
APPLICATIONS: Finding directions (note: hierarchical A*), transportation and communications networks, distinguishing
among homophones iin speech recognition systems, graph visualization algorithms...

Approximate shortest paths in road networks, for instance, typically employ a hierarchical variation of A* <cite>Goldberg et al. 2006</cite> <cite>Goldberg et al. 2007</cite>. Such problem differ from the shortest path problems in this Section because
(1) preprocessing costs can be amortized over many point-to-point queries, 
(2) the backbone of high-speed, long-distance highways can reduce the path problem to identifying the best place to get on and off this backbone, and (3) approximate or heuristic solutions suffice in practice
</text>


<document>
<tag>graph-shortest-paths-dijkstra</tag>
<title>Dijkstra's algorithm</title>

<text>
IDEA: Shortest path from s to t goes through x, then that path contains the shortest path from s to x
</text>

<text>
Similar to Prim's algorithm... each iteration determines the shortest path from s to a new vertex x by minimizing d(s,v)+w(v,x)

In Prim's algorithm, we just take into account the weighte of the edge we add to the MST; in Dijkstra's algorithm, we consider the cost of the whole path from s to x (i.e. the final edge weight plus the distance from s to the tree vertex that is adjacent to the new edge). Dijkstra's algorithm returns a shortest-path spanning tree from s to all the other nodes in the network.
</text>

<text>
<url>http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm</url>
Dijkstra's algorithm <cite>Dijkstra 1959</cite>: 

computes the shortest path from a given starting vertex x to all n . 1
other vertices. In each iteration, it identifies a new vertex v for which the shortest
path from x to v is known. We maintain a set of vertices S to which we currently
know the shortest path from v, and this set grows by one vertex in each iteration.
In each iteration, we identify the edge (u,v) so that
<eqn>dist(x, u) + weight(u, v) = min_(w,y) { dist(x, w) + weight(w,y) }</eqn>

This edge (u, v) gets added to a shortest path tree, whose root is x and describes
all the shortest paths from x.

If we just need to know the shortest path from x to y, terminate the algorithm as soon as y enters S.
</text>



<code>
procedure DIJKSTRA(V, E, w, s) 
2.   begin 
3.      VT := {s}; 
4.      for all v \in (V - V_T ) do 
5.         if (s, v) exists set l[v] := w(s, v); 
6.         else set l[v] := \infty; 
7.      while V_T \neq V do 
8.      begin 
9.         find a vertex u such that l[u] := min{l[v]|v \in (V - V_T)}; 
10.        VT := VT \cup {u}; 
11.        for all v \in (V - V_T ) do 
12.            l[v] := min{l[v], l[u] + w(u, v)}; 
13.     endwhile 
14.  end DIJKSTRA
</code>


Recall that Dijkstra's algorithm performs the following two steps in each iteration. 
- First, it extracts a vertex u \in (V - V_T) such that l[u] = min{l[v]|v \in (V - V_T)} and inserts it into set V_T. 
- Second, for each vertex v \in (V - V_T), it computes l[v] = min{l[v], l[u] + w(u, v)}. 

Note that, during the second step, only the vertices in the adjacency list of vertex u need to be considered. Since the graph is sparse, the number of vertices adjacent to vertex u is considerably smaller than \Theta(n); thus, using the adjacency-list representation improves performance.




<code>
1.   procedure JOHNSON(V, E, s) 
2.   begin 
3.      Q := V ; 
4.      for all v \in Q do 
5.          l[v] := \infty; 
6.      l[s] := 0; 
7.      while Q \neq \emptyset do 
8.      begin 
9.         u := extract min( Q); 
10.        for each v \in Adj [u] do 
11.           if v \in Q and l[u] + w(u, v) &lt; l[v] then 
12.               l[v] := l[u] + w(u, v); 
13.     endwhile 
14.  end JOHNSON
</code>

Johnson's algorithm = Dijkstra's algorithm using a priority queue Q to store the value l[v] for each vertex v \in (V - V_T). 
- The priority queue is constructed so that the vertex with the smallest value in l is always at the front of the queue. 
- A common way to implement a priority queue is as a binary min-heap. A binary min-heap allows us to update the value l[v] for each vertex v in time O (log n). 

Algorithm:
- Initially, for each vertex v other than the source, it inserts l[v] = \infty in the priority queue. 
- For the source vertex s it inserts l[s] = 0. 
- At each step of the algorithm, the vertex u \in (V - V_T) with the minimum value in l is removed from the priority queue. 
- The adjacency list for u is traversed, and for each edge (u, v) the distance l[v] to vertex v is updated in the heap. Updating vertices in the heap dominates the overall run time of the algorithm. The total number of updates is equal to the number of edges; thus, the overall complexity of Johnson's algorithm is <eqn>\Theta(m log n)</eqn>.


<text>
EFFICIENCY:
- <eqn>O(n^2)</eqn> using a naive implementation with arrays

- Using binary heaps: <eqn>O(m \log n)</eqn>

- Fastest implementation: <eqn>O(m + n \log n)</eqn> amortized time using Fibonacci heaps <cite>Fredman and Tarjan 1987</cite>
</text>



<text>
Related problem: Single-destination shortest path problem for directed graphs.
</text>


<text>
CORRECTNESS: Only on graphs without negative edges!!!

Dijkstra's algorithm assumes that all edges have positive cost. 
- Adding a fixed amount of weight to make each edge positive does
not solve the problem. Dijkstra's algorithm will then favor paths using a fewer
number of edges,

For graphs with edges of negative weight,
you must use the more general, but less efficient, Bellman-Ford algorithm.

NOTE: negative cost cycles =the shortest x to y path in such a graph is not defined because we can detour
from x to the negative cost cycle and repeatedly loop around it, making the total cost arbitrarily small.

</text>

<text>
In a geometric setting, more efficient geometric algorithms compute the shortest path directly from the arrangement of obstacles: motion planning algorithms <cite>Latombe 1991</cite> <cite>LaValle 2006</cite>
</text>

</document>

<document>
<tag>graph-shortest-paths-parallel-dense</tag>
<title>Parallel algorithm for dense networks</title>

<text>
PARALLELIZATION:
Parallelization of Dijkstra's algorithm
- for dense graphs @ Section 10.3 <cite>Grama et al. 2003</cite>

Dijkstra's algorithm is almost identical to Prim's minimum spanning tree algorithm. The main difference is that, for each vertex, Dijkstra's algorithm stores the minimum cost to reach the vertex from the source, whereas Prim's algorithm stores the cost of the minimum-cost edge connecting the vertex to the minimum spanning tree. 

"The parallel formulation of Dijkstra's single-source shortest path algorithm is very similar to the parallel formulation of Prim's algorithm for minimum spanning trees. The weighted adjacency matrix is partitioned using the 1-D block mapping. Each of the p processes is assigned n/p consecutive columns of the weighted adjacency matrix, and computes n/p values of the array l. During each iteration, all processes perform computation and communication similar to that performed by the parallel formulation of Prim's algorithm. Consequently, the parallel performance and scalability of Dijkstra's single-source shortest path algorithm is identical to that of Prim's minimum spanning tree algorithm."


Efficiency (for dense graphs, using their adjacency matrix)

1) Computation: \Theta(n/p)

2) Single-node accumulation = all-to-one reduction (global minimum): \Theta(log p) @ message-passing parallel computer

3) One-to-all broadcast: \Theta(log p) @ message-passing parallel computer

Overall

1) Computation: \Theta(n^2/p)

2) Communication: \Theta(n \log p)


Parallel run time:
t_p = \Theta \left ( \frac{n^2}{p} + n \log p \right ) 


Since the sequential run time is W=\Theta(n^2),

speedup

S \in \Theta \left ( \frac{n}{n/p + \log p} \right ) 


efficiency

E = \frac{1}{1 + \Theta ( (p \log p) / n ) }


i.e. 
A cost-optimal parallel implementation requires (p \log p) / n  \in O(1), hence the parallel implementation of Dijkstra's algorithm can use only p=O(n \log n) different processors.  

Isoefficiency function: \Theta(p^2 \log p^2)

tp = n^2/p + n log(p)
ts = n^2 = W

to = p*tp - W = n^2 + np log(p) - n^2 = n p log(p)

E = 1 / (1 + to/W)

Communication overhead: p^2 log^2(p)
	tc = n p log (p)
	W  = n^2
	W = K tc  ===  n^2 = n p log(p)  ===  n = p log(p)  === W=n^2= p^2 log^2(p)
+
Concurrency: p^2
	Since n must grow at least as fast as p... W = n^2 = p^2 
=
Isoefficiency: \Theta(p^2 \log p^2)
</text>


<text>
The performance of the parallel version of a sequential algorithm for dense networks:
- each processor performs roughly the same amount of work
- communication is localized (i.e. limited)
</text>


</document>


<document>
<tag>graph-shortest-paths-parallel-sparse</tag>
<title>Parallel algorithm for sparse networks</title>

@10.7.2 <cite>Grama et al. 2003</cite>

<text>
It is difficult to achieve an even work load distribution among processors and a low communication overhead for sparse graphs
- equal number of vertices? random number of edges incident on a given vertex, leading to work imbalance among processors
- equal number of edges? adjacency lists might be splitted among different processors and communication overhead might dramatically increase

Hard problem for arbitrary networks, albeit efficient parallel algorithms can be devised if the sparse graph has some structure (e.g. grid graphs, finite element meshes, planar graphs such as maps)...
</text>


1) Find the shortest paths to the vertices in the same order as the serial algorithm, and explores concurrently only safe vertices. However, this approach leads to complicated algorithms and limited concurrency. 

PRIORITY QUEUE
The maintainance of the priority queue is key for an efficient parallel version of Dijkstra's algorithm:

- A simple strategy is for a single process, for example, P_0, to maintain Q. All other processes will then compute new values of l[v] for v \in (V - V_T), and give them to P0 to update the priority queue. There are two main limitations of this scheme. 

	1) Because a single process is responsible for maintaining the priority queue, the overall parallel run time is O (m \log n) (there are O (m) queue updates and each update takes time O (\log n)). This leads to a parallel formulation with no asymptotic speedup, since O (|E| log n) is the sequential run time of the algorithm.

	2) During each iteration, the algorithm updates roughly m/n vertices. As a result, no more than m/n processes can be kept busy at any given time, which is very small for most of the interesting classes of sparse graphs, and to a large extent, independent of the size of the graphs.

The first limitation can be alleviated by distributing the maintainance of the priority queue to multiple processes. This is a non-trivial task, and can only be done effectively on architectures with low latency, such as shared-address-space computers. However, even in the best case, when each priority queue update takes only time O (1), the maximum speedup that can be achieved is O (log n), which is quite small. 

The second limitation can be alleviated by recognizing the fact that depending on the l value of the vertices at the top of the priority queue, more than one vertex can be extracted at the same time. In particular, if v is the vertex at the top of the priority queue, all vertices u such that l[u] = l[v] can also be extracted, and their adjacency lists processed concurrently. Such vertices are at the same minimum distance from the source and they can be processed in any order. Note that in order for this approach to work, all the vertices that are at the same minimum distance must be processed in lock-step. An additional degree of concurrency can be extracted if we know that the minimum weight over all the edges in the graph is m. In that case, all vertices u such that l[u] \leq l[v] + m can be processed concurrently (and in lock-step). We will refer to these as the safe vertices. However, this additional concurrency can lead to asymptotically better speedup than O (log n) only if more than one update operation of the priority queue can proceed concurrently, substantially complicating the parallel algorithm for maintaining the single priority queue.


2) Parallel algorithm that processes both safe and unsafe vertices concurrently, as long as these unsafe vertices can be reached from the source via a path involving vertices whose shortest paths have already been computed (i.e., their corresponding l-value in the priority queue is not infinite). 

Each one of the p processes extracts one of the p top vertices and proceeds to update the l values of the vertices adjacent to it. The algorithm does not ensure that the l values of the vertices extracted from the priority queue correspond to the cost of the shortest path. However, the correctness of the results can be ensured by detecting when we have incorrectly computed the shortest path to a particular vertex, and inserting it back into the priority queue with the updated value. 

Detection: Consider a vertex v that has just been extracted from the queue, and let u be a vertex adjacent to v that has already been extracted from the queue. If l[v] + w(v, u) is smaller than l[u], then the shortest path to u has been incorrectly computed, and u needs to be inserted back into the priority queue with l[u] = l[v] + w(v, u).

This approach for parallelizing Dijkstra's algorithm falls into the category of speculative decomposition (when a program may take one of many possible computationally significant branches depending on the output of other computations that precede it. In this situation, while one task is performing the computation whose output is used in deciding the next computation, other tasks can concurrently start the computations of the next stage). Essentially, the algorithm assumes that the l[] values of the top p vertices in the priority queue will not change as a result of processing some of these vertices, and proceeds to perform the computations required by Johnson's algorithm. However, if at some later point it detects that its assumptions were wrong, it goes back and recomputes the shortest paths of the affected vertices.

In order for such a speculative decomposition approach to be effective, we must also remove the bottleneck of working with a single priority queue:

Let p be the number of processes, and let G = (V, E) be a sparse graph. We partition the set of vertices V into p disjoint sets V1, V2, ..., Vp, and assign each set of vertices and its associated adjacency lists to one of the p processes.

Each process maintains a priority queue for the vertices assigned to it, and computes the shortest paths from the source to these vertices. Thus, the priority queue Q is partitioned into p disjoint priority queues Q1, Q2, ..., Qp, each assigned to a separate process. 

In addition to the priority queue, each process Pi also maintains an array sp such that sp[v] stores the cost of the shortest path from the source vertex to v for each vertex v  Vi. The cost sp[v] is updated to l[v] each time vertex v is extracted from the priority queue. Initially, sp[v] = \infty for every vertex v other than the source, and we insert l[s] into the appropriate priority queue for the source vertex s. Each process executes Dijkstra's algorithm on its local priority queue. At the end of the algorithm, sp[v] stores the length of the shortest path from source to vertex v.

When process Pi extracts the vertex u \in Vi with the smallest value l[u] from Qi, the l values of vertices assigned to processes other than Pi may need to be updated. Process Pi sends a message to processes that store vertices adjacent to u, notifying them of the new values. Upon receiving these values, processes update the values of l. For example, assume that there is an edge (u, v) such that u \in Vi and v \in Vj, and that process Pi has just extracted vertex u from its priority queue. Process Pi then sends a message to Pj containing the potential new value of l[v], which is l[u] + w(u, v). Process Pj, upon receiving this message, sets the value of l[v] stored in its priority queue to min{l[v], l[u] + w(u, v)}.

Since both processes Pi and Pj execute Dijkstra's algorithm, it is possible that process Pj has already extracted vertex v from its priority queue. This means that process Pj might have already computed the shortest path sp[v] from the source to vertex v. Then there are two possible cases: either sp[v] \leq l[u] + w(u, v), or sp[v] &gt; l[u] + w(u, v). The first case means that there is a longer path to vertex v passing through vertex u, and the second case means that there is a shorter path to vertex v passing through vertex u. For the first case, process Pj needs to do nothing, since the shortest path to v does not change. For the second case, process Pj must update the cost of the shortest path to vertex v. This is done by inserting the vertex v back into the priority queue with l[v] = l[u] + w(u, v) and disregarding the value of sp[v]. Since a vertex v can be reinserted into the priority queue, the algorithm terminates only when all the queues become empty.

Initially, only the priority queue of the process with the source vertex is non-empty. After that, the priority queues of other processes become populated as messages containing new l values are created and sent to adjacent processes. When processes receive new l values, they insert them into their priority queues and perform computations. 

Consider the problem of computing the single-source shortest paths in a grid graph where the source is located at the bottom-left corner. The computations propagate across the grid graph in the form of a wave. A process is idle before the wave arrives, and becomes idle again after the wave has passed. At any time during the execution of the algorithm, only the processes along the wave are busy. The other processes have either finished their computations or have not yet started them. 

<!--
Three mappings of grid graphs onto a p-process mesh <cite>Grama et al. 2003<\cite>:

- Block-Checkerboard Mapping (2D Block Mapping): p processors as a logical mesh and assign a different block of vertices to each process. At any time, the number of busy processes is equal to the number of processes intersected by the wave. Since the wave moves diagonally, no more than O (\sqrt{p}) processors are busy at any time. Ignoring  the overhead due to interprocess communication and extra work, then the maximum efficiency is poor, E = 1/\sqrt{n}, and becomes worse as the number of processors increases.

- Cyclic-Checkerboard Mapping (2D Cyclic Mapping): make each process responsible for scattered areas of the grid to increase the time during which a processor stays busy. In 2-D cyclic mapping, the n x n grid graph is divided into n^2/p blocks, each of size <eqn>\sqrt{p} \times \sqrt{p}</eqn>. Each block is mapped onto the <eqn>\sqrt{p} \times \sqrt{p}</eqn> process mesh. As the wave propagates through the graph, processors remain busy for most of the algorithm at the cost of a higher communication overhead than 2-D block mapping: since adjacent vertices reside on separate processes, every time a process extracts a vertex u from its priority queue it must notify other processes of the new value of l[u].

- Block-Striped Mapping (1D Block Mapping) assigns n/p stripes of the grid graph to each processor. If we assume that the wave propagates at a constant rate, then p/2 processes (on the average) are busy. Ignoring any overhead, the speedup and efficiency of this mapping are p/2 and 1/2, respctively. The 1-D block mapping is substantially better than the 2-D block mapping but cannot use more than O(n) processes.


The 2-D block mapping, 2-D block-cyclic mapping, and 1-D block mapping formulation of Johnson's algorithm are due to Wada and Ichiyoshi [WI89]. They also presented theoretical and experimental evaluation of these schemes on a mesh-connected parallel computer.

[WI89] K. Wada and N. Ichiyoshi. A distributed shortest path algorithm and its mapping on the Multi-PSI. In Proceedings of International Conference of Parallel Processing, 1989.

-->

</document>

</document>




<!-- Shortest paths: All pairs -->

<document>
<tag>graph-shortest-paths-all-pairs</tag>
<title>All-pairs shortest paths</title>


<text>
Some problems requite computing the shortest paths between all pairs of nodes, e.g. the central node (i.e. the node that minimizes the longest or average distance to every other node) or the network diameter (i.e. the longest shortest-path distance between two nodes in the network). 
</text>

<text>
EXAMPLE: 
- bounding the amount of time needed to deliver a packet through a network
</text>

<!-- Applications -->

<text>
All-pairs shortest paths algorithms can be used to find the shortest cycle in a graph: its girth. Floyd's algorithm can be used to compute <eqn>d_ii</eqn>, which is the shortest way to get from vertex i to itself (i.e. the shortest cycle that goes through i). If you want the shortest simple cycle, the easiest
approach is to compute the lengths of the shortest paths from i to all other vertices, and then explicitly check whether there is an acceptable edge from each vertex back to i.
</text>


<text>
- Reachability questions (can I get to x from y?)

RELATED PROBLEMS 
+ Transitive closure, construct a graph G' = (V,E') with edge (i, j) in E' iff there is a directed path from i to j in G. 
+ Transitive reduction (also known as minimum equivalent digraph), the inverse operation of transitive closure: reducing the number of edges while maintaining identical reachability properties, i.e. construct a small graph G'' = (V,E'') with a directed path from i to j in G'' iff there is a directed path from i to j in G

- transitive closure 

1) using graph traversal algorithms O(n(n+m)), 
2) an all-pairs shortest path algorithms returns them all in a whole batch, albeit less efficiently, <eqn>O(n^3)</eqn> using Warshall's algorithm.
3) using matrix multiplication: O(log n) multiplications using a divide-and-conquer algorithm

- transitive reduction

Approximate solution: A linear-time, quick-and-dirty transitive reduction algorithm identifies the
strongly connected components of G, replaces each by a simple directed cycle,
and adds these edges to those bridging the different components.

+ Transitive reduction also arises in graph drawing, where it is important to eliminate as many unnecessary edges as possible to reduce the visual clutter
</text>


<text>
- pattern recognition problems, e.g. Viterbi algorithm, a dynamic programming algorithm that basically solves a shortest path problem on a DAG.
</text>


<!-- Algorithms -->

<document>
<tag>graph-all-shortest-paths-sequential</tag>
<title>Sequential algorithms</title>

<text>
A first solution: Repeating Dijkstra's algorithm for each node
</text>

<text>
+ Best on adjacency matrix (anyway we will need to store <eqn>n \times n</eqn> distances), i.e. one of the rare cases where algorithms on adjacency matrices work better
</text>

<!-- Floyd-Warshall -->

<document>
<tag>graph-shortest-paths-floyd-warshall</tag>
<title>Floyd-Warshall algorithm</title>

<text>
<url>http://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm</url>
Floyd-Warshall algorithm: <cite>Floyd 1962</cite>
often referred to as Floyd's algorithm, albeit the same algorithm was published by Bernard Roy in 1959 <cite>Roy 1959</cite> and by Stephen Warshall in 1962 <cite>Warshall 1962</cite>
</text>


- Define the length of the shortest path from i to j using only the k first vertices as possible intermediate nodes: <eqn>W[i,j]^k</eqn>

- Initial shortest-path matrix = adjacency matrix

- <eqn>W[i,j]^k = min \{ W[i,j]^{k-1}, W[i,k]^{k-1} + W[k,j]^{k-1} \}</eqn>


IDEA: 

- Minimum-weight path p^{(k)}_{i,j} from v_i to v_j among the subset of k vertices
- Minimum distance d^{(k)}_{i,j} = weight of p^{(k)}_{i,j}

When a vertex v_k is not in the shortest path from v_i to v_j, then p^{(k)}_{i,j} = p^{(k-1)}_{i,j}
When the vertex v_k is in the shortest path from v_i to v_j, then p^{(k)}_{i,j} = p^{(k-1)}_{i,k} + p^{(k-1)}_{k,j}

<equation>
d^{(k)}_{i,j} = min \{ d^{(k-1)}_{i,j}, d^{(k-1)}_{i,k} + d^{(k-1)}_{k,j} \}
</equation>

given that

<equation>
d^{(0)}_{i,j} = w (v_i, v_j)
</equation>


Simple <eqn>\Theta(n^3)</eqn> implementation (for dense graphs)
- Compact implementation: 3 nested loops
- Ancillary matrix to reconstruct the paths

<eqn>\Theta(n^2)</eqn> space complexity, since <eqn>D^(k)</eqn> can be computed from <eqn>D^(k-1)</eqn>


<code>
D0 = M
for k = 1 to n do
    for i = 1 to n do
        for j = 1 to n do
            Dk(i,j) = min{ Dk-1(ij), Dk-1(ik) + Dk-1(kj) }
return Dn


  for k := 1 to n
        for i := 1 to n
           for j := 1 to n
              if path[i][k] + path[k][j] &lt; path[i][j] then
                 path[i][j] := path[i][k]+path[k][j];
                 next[i][j] := k;

</code>


<!-- Transitive closure -->

<note>
<title>Warshall's algorithm: The transitive closure of directed graphs</title>

<text>
Formally, given a graph G = (V, E), the transitive closure of the graph G is defined as the graph G* = (V, E*), where E* = {(vi, vj)| there is a path from vi to vj in G}.
</text>

<text>
The transitive closure of a graph is obtained by computing the connectivity matrix A*. The connectivity matrix of G is a matrix <eqn>A^* = (a^*_{i,j})</eqn> such that <eqn>a^*_{i,j} = 1</eqn> if there is a path from vi to vj or i = j, and <eqn>a^*_{i,j} = \infty</eqn> otherwise.
</text>

<text>
Warshall's algorithm can be used to compute the transitive closure of a graph and, therefore, determine whether two vertices are connected or not.

In Warshall's original formulation of the algorithm, the graph is unweighted and represented by a Boolean adjacency matrix. 

Warshall's transitive closure algorithm is actually equivalent to Floyd's shortest path algorithm.

The only difference is that the addition operation in Floyd's algorithm is replaced by a logical conjunction (and), while the minimum operation in Floyd's algorithm is replaced by logical disjunction (or). Initially, we would set <eqn>d_{i,j} = 1</eqn> if <eqn>i = j</eqn> or <eqn>(v_i, v_j) \in E</eqn>, and <eqn>d_{i,j} = 0</eqn> otherwise. Matrix A* would be obtained by setting <eqn>a^*_{i,j} = \infty</eqn> when <eqn>d_{i,j} = 0</eqn> and  <eqn>a^*_{i,j} = 1</eqn> otherwise.
</text>

<text>
As you can easily check, the connectivity matrix A* can also be obtained from the distance matrix D returned by any of the all-pairs shortest paths algorithms discussed in this chapter. Given a shortest-path distance matrix, the connectivity matrix can then be defined as
</text>

<equation>
a^*_{ij} = 
\left \{
\begin{matrix}
 \infty &amp; if &amp; d_{i,j} = \infty \\ 
 1 &amp; if &amp; d_{i,j} \geq 0
\end{matrix}
\right.
</equation>

</note>



</document>


<!-- Bellman-Ford -->

<document>
<tag>graph-shortest-paths-bellman-ford</tag>
<title>Bellman-Ford algorithm</title>

<text>
<url>http://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm</url>
Bellman-Ford algorithm: <eqn>O(nm)</eqn> time, <eqn>O(n)</eqn> space

Bellman-Ford algorithm <cite>Bellman 1958</cite> <cite>Ford and Fulkerson 1962</cite>

+ Negative weights (arise when we reduce other problems to shortest-paths problems)
</text>

<code>
   // Step 1: initialize graph
   for each vertex v in vertices:
       if v is source then v.distance := 0
       else v.distance := infinity
       v.predecessor := null

   // Step 2: relax edges repeatedly
   for i from 1 to size(vertices)-1:
       for each edge uv in edges: // uv is the edge from u to v
           u := uv.source
           v := uv.destination
           if u.distance + uv.weight &lt; v.distance:
               v.distance := u.distance + uv.weight
               v.predecessor := u

   // Step 3: check for negative-weight cycles
   for each edge uv in edges:
       u := uv.source
       v := uv.destination
       if u.distance + uv.weight &lt; v.distance:
           error "Graph contains a negative-weight cycle"
</code>

</document>


<!-- Johnson -->

<document>
<tag>graph-shortest-paths-johnson</tag>
<title>Johnson's algorithm</title>

<text>
<url>http://en.wikipedia.org/wiki/Johnson%27s_algorithm</url>
Johnson's algorithm <cite>Johnson 1977</cite> 

1
First, a new node q is added to the graph, connected by zero-weight edges to each of the other nodes.

2
Second, the Bellman-Ford algorithm is used, starting from the new vertex q, to find for each vertex v the least weight h(v) of a path from q to v. If this step detects a negative cycle, the algorithm is terminated.

3
Next the edges of the original graph are reweighted using the values computed by the Bellman-Ford algorithm: an edge from u to v, having length w(u,v), is given the new length w(u,v) + h(u) - h(v).

4
Finally, q is removed, and Dijkstra's algorithm is used to find the shortest paths from each node s to every other vertex in the reweighted graph.


<eqn>O(n^2 \log n + nm)</eqn> using <eqn>O(nm)</eqn> time for the Bellman-Ford stage of the algorithm, and <eqn>O(n \log n + m)</eqn> for each of <eqn>n</eqn> instantiations of Dijkstra's algorithm. Faster than the Floyd-Warshal algorithm for sparse graphs.


It allows some of the edge weights to be negative
</text>


</document>

</document>



<!-- Parallelization -->

<document>
<tag>graph-all-shortest-paths-parallel</tag>
<title>Parallel algorithms</title>



<text>
PARALLELIZATION:
- for dense graphs @ Section 10.4 <cite>Grama et al. 2003</cite>
+ Bibliographic notes
</text>

3 algorithms: matrix multiplication, Dijkstra's single-source shortest paths, Floyd's algorithm

Note: Dijkstra's algorithm requires non-negative edge weights, the other two algorithms work with graphs containing negative-weight edges provided that there are not negative-weight cycles.

<document>
<title>Parallel algorithm based on matrix multiplication</title>

<text>
Matrix-multiplication-based algorithm:
</text>

- Minimum-weight path p^{(k)}_{i,j} from v_i to v_j containing k edges

- Minimum distance d^{(k)}_{i,j} = weight of p^{(k)}_{i,j}

- Path decomposition: p^{(k)}_{i,j} = p^{(k-1)}_{i,m} + (v_m, v_j)

- Therefore,  d^{(k)}_{i,j} = min { d^{(k-1)}_{i,j}, d^{(k-1)}_{i,m} + w(v_m,v_j) }


- Taking into account that w(v_j,v_j)=0

<equation>
d^{(k)}_{i,j} = min \{ d^{(k-1)}_{i,m} + w(v_m,v_j) \}
</equation>

If we consider the matrix <eqn>D^(k) = ( d^{(k)}_{i,j} )</eqn>, then <eqn>D^(n-1)</eqn> contains the weights of the shortest paths between all pairs of vertices in G. Given that <eqn>D^(k)</eqn> can be computed from <eqn>D^(k-1)</eqn> using a modified matrix multiplication algorithm (performing addition and minimization instead of multiplication and addition), we can design a divide-and-conquer algorithm that obtains the desired result after <eqn>\lceil \log (n-1) \rceil</eqn> modified multiplications, i.e. <eqn>D^(n-1)</eqn> is computed by computing <eqn>D^2</eqn>, <eqn>D^2</eqn>, <eqn>D^8</eqn>...

The conventional <eqn>\Theta(n^3)</eqn> matrix-multiplication algorithm results in an <eqn>\Theta(n^3 \log n)</eqn> sequential all-pairs shortest paths algorithm that is easily parallelizable using parallel matrix multiplication algorithms such as Cannon's algorithm or the DNS algorithm, which are specially useful in massively-parallel architectures. 

- Cannon's algorithm <cite>Cannon 1969</cite> partitions matrices into p square blocks and schedules the computations so that each processor performs <eqn>\sqrt{p}</eqn> multiplications of <eqn>n/\sqrt{p} \times n/\sqrt{p}</eqn> matrices in <eqn>O(n^3/p)</eqn>. This parallel algorithm is optimal for <eqn>O(n^2)</eqn> processors and the isoefficiency function is <eqn>\Theta(p^{3/2)}</eqn>.

- The DNS algorithm, due to Dekel, Nassimi and Sahni <cite>Dekel et al. 1981</cite>, can use up to <eqn>n^3</eqn> processors in parallel and performs matrix multiplication in <eqn>\Theta(\log n)</eqn> time by using <eqn>\Theta(n^3 / \log n)</eqn> processors. The algorithm is cost-optimal for <eqn>O(p \log^3 p)</eqn> processors and its isoefficiency function is <eqn>\Theta(p \log^3 p)</eqn>.

ref. L. E. Cannon. A cellular computer to implement the Kalman Filter Algorithm. Ph.D. Thesis, Montana State University, Bozman, MT, 1969.
ref. E. Dekel, D. Nassimi, and S. Sahni. Parallel matrix and graph algorithms. SIAM Journal on Computing, 10:657–673, 1981
ref. G. C. Fox, S. W. Otto, and A. J. G. Hey. Matrix algorithms on a hypercube I: Matrix multiplication. Parallel Computing, 4:17–31, 1987.

++ Gupta and Kumar [GK91] present a detailed scalability analysis of several matrix multiplication algorithms
ref. A. Gupta and V. Kumar. The scalability of matrix multiplication algorithms on parallel computers. Technical Report TR 91-54, Department of Computer Science, University of Minnesota, Minneapolis, MN, 1991. A short version appears in Proceedings of 1993 International Conference on Parallel Processing, pages III-115–III-119, 1993.

Therefore, if we use the DNS algorithm with <eqn>n^3</eqn> processors, which requires <eqn>\Theta(\log n)</eqn> time for matrix multiplication, we can obtain a <eqn>\Theta(\log^2 n)</eqn> algorithm for solving the all-pairs shortest path problem. Since the best-known sequential algorithm has <eqn>\Theta(n^3)</eqn> time complexity, the speedup and efficiency of the parallel algorithm are:

<equation>
S \in \Theta \left ( \frac{n^3}{\log^2 n} \right ) 
</equation>

<equation>
E \in \Theta \left ( \frac{1}{\log^2 n} \right ) 
</equation>

<text>
When we use <eqn>\Theta(n^3 / \log n)</eqn> processors, the efficiency of this algorithm can be improved to <eqn>\Theta(1 / \log n)</eqn>, yet the algorithm is still unscalable. This is the fastest-known parallel algorithm for solving the all-pairs shortest paths problem. However, slower cost-optimal algorithms are employed in practice.
</text>

</document>

<document>
<title>Parallelization based on Dijkstra's algorithm</title>

<text>
Dijkstra's single-source shortest paths algorithms can be used to solve the all-pairs shortest paths problem just by executing it for each vertex. Dijkstra's all-pairs shortest paths algorithm can be parallelized in two different ways:
</text>

<list>

<item>Source-partitioned Dijkstra's all-pairs shortest paths: Vertices are partitioned among different processors and each processor computes the single-source shortest paths for its assigned vertices.</item>

<item>Source-parallel Dijkstra's all-pairs shortest paths: Each vertex is assigned to a set of processors and the parallel version of Dijkstra's single-source shortest paths algorithm is used.</item>

</list>


<text>
The source-partitioned version requires no interprocessor communication, hence the parallel run time is <eqn>t_p \in \Theta(n^2)</eqn> for dense graphs. Since the sequential time for the all-pairs shortest paths problem is <eqn>W=\Theta(n^3)</eqn> for dense graphs, the source-partitioned version achieves a linear speedup <eqn>\Theta(n)</eqn> and optimal efficiency <eqn>\Theta(1)</eqn>. However, the algorithm is poorly scalable, since it can use <eqn>n</eqn> processors at most and its isoefficiency function due to concurrency is <eqn>\Theta(p^3)</eqn>.
</text>

<text>
When the number of available processors p is greater than the number of nodes n, the source-parallel version can improve the performance of the source-partitioned version by partitioning the p processors into n partitions with p/n processors. Parallelizing both the all-pairs algorithm by assigning each vertex to a separate set of processors and the single-source algorithm by using sets of p/n processors, we could use up to n^2 processors in parallel. Assuming an hypercube interconnection network or an equivalent message-passing parallel computer, the source-parallel algorithm can use <eqn>O(n^2 / \log n)</eqn> processors efficiently, requires just <eqn>O(n \log n)</eqn> parallel run time, and its isoefficiency function drops to <eqn>O(p^{3/2} \log^{3/2} p )</eqn>, a substantial improvement over the source-partitioned version.
</text>

</document>

<document>
<title>Parallelization of Floyd's algorithm</title>

<text>
Parallelization strategy for computing the <eqn>D^{(k)}</eqn> matrix using <eqn>p</eqn> processors: Matrix <eqn>D^{(k)}</eqn> is partitioned into <eqn>p</eqn> parts and each part is assigned to a different processor. In order to perform this, each processor need access to segments of rows and columns of the <eqn>D^{(k-1)}</eqn> matrix.
</text>

<text>
Techniques for partitioning the <eqn>D^{(k)}</eqn> matrix:
</text>

<list>

<item>
Block-striped mapping: Each of the <eqn>p</eqn> processors is assigned <eqn>n/p</eqn> consecutive columns of the <eqn>D^{(k)}</eqn> matrix.
</item>

<item>
Block-checkerboard mapping: The <eqn>D^{(k)}</eqn> matrix is divided into <eqn>p</eqn> squares of size <eqn>n/\sqrt{p} \times n/\sqrt{p}</eqn> and each square is assigned to a different processor.
</item>

<item>
Pipelined block-checkerboard mapping: Rather than waiting for the (k-1)-th iteration to complete and ensure that all processors have the appropriate segments of the <eqn>D^{(k-1)}</eqn> matrix, a processor can start its work on the k-th iteration as soon as it has computed its part of the <eqn>D^{(k-1)}</eqn> matrix and has received the relevant parts of the <eqn>D^{(k-1)}</eqn> matrix.
</item>

</list>


<text>
Using the last parallelization strategy, let us consider what happens on a p-processor 2D mesh. In each algorithm step, <eqn>n/\sqrt{p}</eqn> elements from a row are sent from processor <eqn>P_{i,j}</eqn> to processors <eqn>P_{i+1,j}</eqn> and <eqn>P_{i-1,j}</eqn>. Likewise, <eqn>n/\sqrt{p}</eqn> elements from a column are sent from processor <eqn>P_{i,j}</eqn> to processors <eqn>P_{i,j+1}</eqn> and <eqn>P_{i,j-1}</eqn>. Each step requires <eqn>\Theta(n/\sqrt{p})</eqn> time and after <eqn>\Theta(\sqrt{p})</eqn> steps, i.e. in <eqn>\Theta(n)</eqn> time, every processor gets the relevant elements it needs from the first row (and column). Successive rows and columns are received every <eqn>\Theta(n^2/p)</eqn> time in pipeline mode. Hence, every processor can complete its computation in time <eqn>\Theta(n^3/p)</eqn> and send the relevant information to other processors in <eqn>\Theta(n)</eqn> time. The parallel run time required by the algorithm is, therefore,
</text>

<equation>
t_p \in \theta \left( \frac{n^3}{p} + n \right )
</equation>

<text>
Since the sequential run time for dense networks is <eqn>W \in \Theta(n^3)</eqn>, the parallel algorithm speedup and efficiency are
</text>

<equation>
S \in \Theta \left ( \frac{n^2}{n^2/p + 1} \right )
</equation>

<equation>
E = \frac{1}{1 + \Theta(p/n^2)}
</equation>

<text>
This algorithm is cost-optimal for <eqn>p/n^2 \in O(1)</eqn>, hence the pipelined algorithm can use up to <eqn>O(n^2)</eqn> processors efficiently and its isoefficiency function is <eqn>\Theta(p^{3/2})</eqn>.
</text>

tp = n^3/p + n
ts = n^3 = W

to = p*tp - W = n^3 + np - n^3 = np

E = 1 / (1 + to/W) = 1 / (1 + p/n^2)

	p = n^2 === n = p^{1/2}
	W  = n^3
	W  = K to  ===  n^3 = K p^{3/2}

Communication overhead: p^{3/2}
	tc = np
	W = n^3
	W = K to  ===  n^3 = np  ===  n = sqrt(p)  === W = n^3 = p^{3/2}
+
Concurrency: p
	Since n must grow at least as fast as p... W = n^3 = p^3
=
Isoefficiency: \Theta(p^{3/2})


<text>
Table <ref>parallel-all-pairs</ref> compares the performance and scalability properties of different algorithms for solving the all-pairs shortest paths problem for dense networks on architectures with <eqn>O(p)</eqn> bisection bandwidth. The pipelined parallel implementation of Floyd's algorithm is the most scalable: it can use up to <eqn>\Theta(n^2)</eqn> processors to compute the all-pairs shortest paths in linear <eqn>\Theta(n)</eqn> time and it performs equally well on hypercubes with <eqn>O(p)</eqn> bisection bandwidth and meshes with <eqn>O(\sqrt{p})</eqn> bisection bandwidth, both with store-and-forward and cut-through routing <cite>Grama et al. 2003</cite>.
</text>


<dataset>
 <tag>parallel-all-pairs</tag>
 <title>Performance and scalability of parallel all-pairs shortest paths algorithms</title>
 <metadata>
  <field width="25">Algorithm</field>
  <field width="25">Maximum number of processors for <eqn>E \in \Theta(1)</eqn></field>
  <field width="25">Parallel run time</field>
  <field width="25">Isoefficiency function</field>
 </metadata>
 <record>
  <field>Matrix multiplication</field>
  <field><eqn>-</eqn></field>
  <field><eqn>\Theta(\log^2 n)</eqn></field>
  <field><eqn>-</eqn></field>
 </record>
 <record>
  <field>Dijkstra (source-partitioned)</field>
  <field><eqn>\Theta(n)</eqn></field>
  <field><eqn>\Theta(n^2)</eqn></field>
  <field><eqn>\Theta(p^3)</eqn></field>
 </record>
 <record>
  <field>Dijkstra (source-parallel)</field>
  <field><eqn>\Theta(n^2 / \log n)</eqn></field>
  <field><eqn>\Theta(n \log n)</eqn></field>
  <field><eqn>\Theta(p^{3/2} \log^{3/2} p)</eqn></field>
 </record>

 <record>
  <field>Floyd (block-striped)</field>
  <field><eqn>\Theta(n / \log n)</eqn></field>
  <field><eqn>\Theta(n^2 \log n)</eqn></field>
  <field><eqn>\Theta(p^3 \log^3 p)</eqn></field>
 </record>
 <record>
  <field>Floyd (block-checkerboard)</field>
  <field><eqn>\Theta(n^2 / \log^2 n)</eqn></field>
  <field><eqn>\Theta(n \log^2 n)</eqn></field>
  <field><eqn>\Theta(p^{3/2} \log^3 p)</eqn></field>
 </record>
 <record>
  <field>Floyd (pipelined block-checkerboard)</field>
  <field><eqn>\Theta(n^2)</eqn></field>
  <field><eqn>\Theta(n)</eqn></field>
  <field><eqn>\Theta(p^{3/2})</eqn></field>
 </record>

</dataset>


</document>

</document>


<!-- Distributed algorithm -->

<document>
<tag>graph-all-shortest-paths-distributed</tag>
<title>Distributed algorithms</title>

<text>
DISTRIBUTED ALGORITHM:
Wikipedia: A distributed variant of the Bellman-Ford algorithm is used in distance-vector routing protocols, for example the Routing Information Protocol (RIP). The algorithm is distributed because it involves a number of nodes (routers) within an Autonomous system, a collection of IP networks typically owned by an ISP. It consists of the following steps:
- Each node calculates the distances between itself and all other nodes within the AS and stores this information as a table.
- Each node sends its table to all neighboring nodes.
- When a node receives distance tables from its neighbors, it calculates the shortest routes to all other nodes and updates its own table to reflect any changes.
</text>


<text>
Distributed shortest paths @ MIT 6.852: Lectures 2-3 (synchronous) + Lectures 8-9 (asynchronous)  / Section 4.3 + Section 15.4 <cite>Lynch 1997</cite>
</text>

</document>

</document>

<document>
<tag>graph-shortest-paths-notes</tag>
<title>Bibliographic notes</title>


From <cite>Grama et al. 2003</cite>:


SINGLE-SOURCE

- Algorithm discovered by Dijkstra [Dij59]. Due to the similarity between Dijkstra's algorithm and Prim's MST algorithm, all the parallel formulations of Prim's algorithm discussed in a previous chapter can also be applied to the single-source shortest paths problem. 

[Dij59] E. W. Dijkstra. A note on two problems in connection with graphs. Numerische Mathematik, 1:269–271, 1959.


- Bellman [Bel58] and Ford [FR62] independently developed a single-source shortest paths algorithm that operates on graphs with negative weights but without negative-weight cycles. The Bellman-Ford single-source algorithm has a sequential complexity of O (nm). 

[Bel58] R. Bellman. On a routing problem. Quarterly of Applied Mathematics, 16(1):87–90, 1958.

[FR62] L. R. Ford and R. L. Rivest. Flows in Networks. Princeton University Press, Princeton, NJ, 1962.


- Paige and Kruskal [PK89] present parallel formulations of both the Dijkstra and Bellman-Ford single-source shortest paths algorithm. Their formulation of Dijkstra's algorithm runs on an EREW PRAM of Q(n) processes and runs in time Q(n log n). Their formulation of Bellman-Ford's algorithm runs in time Q(nmp + n log p) on a p-process EREW PRAM where p \leq m. They also present algorithms for the CRCW PRAM [PK89].

[PK89] R. C. Paige and C. P. Kruskal. Parallel algorithms for shortest path problems. In Proceedings of 1989 International Conference on Parallel Processing, 14– 19, 1989.


... for sparse graphs

The single-source shortest paths algorithm for sparse graphs was discovered by Johnson [Joh77]. 

[Joh77] D. B. Johnson. Efficient algorithms for shortest paths in sparse networks. Journal of the ACM, 24(1):1–13, March 1977.


Paige and Kruskal [PK89] discuss the possibility of maintaining the queue Q in parallel. Rao and Kumar [RK88a] presented techniques to perform concurrent insertions and deletions in a priority queue. 

[PK89] R. C. Paige and C. P. Kruskal. Parallel algorithms for shortest path problems. In Proceedings of 1989 International Conference on Parallel Processing, 14– 19, 1989.

[RK88a] V. N. Rao and V. Kumar. Concurrent access of priority queues. IEEE Transactions on Computers, C–37 (12), 1988.








ALL-PAIRS


The source-partitioning formulation of Dijkstra's all-pairs shortest paths is discussed by Jenq and Sahni [JS87] and Kumar and Singh [KS91b]. 

[JS87] J.-F. Jenq and S. Sahni. All pairs shortest paths on a hypercube multiprocessor. In Proceedings of the 1987 International Conference on Parallel Processing, 713–716, 1987.

[KS91b] V. Kumar and V. Singh. Scalability of parallel algorithms for the all-pairs shortest path problem. Journal of Parallel and Distributed Computing, 13(2):124–138, October 1991. A short version appears in the Proceedings of the International Conference on Parallel Processing, 1990.


The source parallel formulation of Dijkstra's all-pairs shortest paths algorithm is discussed by Paige and Kruskal [PK89] and Kumar and Singh [KS91b]. 

[PK89] R. C. Paige and C. P. Kruskal. Parallel algorithms for shortest path problems. In Proceedings of 1989 International Conference on Parallel Processing, 14– 19, 1989.

[KS91b] V. Kumar and V. Singh. Scalability of parallel algorithms for the all-pairs shortest path problem. Journal of Parallel and Distributed Computing, 13(2):124–138, October 1991. A short version appears in the Proceedings of the International Conference on Parallel Processing, 1990.


The Floyd's all-pairs shortest paths algorithm discussed is due to Floyd [Flo62]. 

[Flo62] R. W. Floyd. Algorithm 97: Shortest path. Communications of the ACM, 5(6):345, June 1962.


The 1-D and 2-D block mappings are presented by Jenq and Sahni [JS87], and the pipelined version of Floyd's algorithm is presented by Bertsekas and Tsitsiklis [BT89] and Kumar and Singh [KS91b].

[JS87] J.-F. Jenq and S. Sahni. All pairs shortest paths on a hypercube multiprocessor. In Proceedings of the 1987 International Conference on Parallel Processing, 713–716, 1987.

[BT89] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Prentice-Hall, NJ, 1989.

[BT97] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Athena Scientific, 1997.

[KS91b] V. Kumar and V. Singh. Scalability of parallel algorithms for the all-pairs shortest path problem. Journal of Parallel and Distributed Computing, 13(2):124–138, October 1991. A short version appears in the Proceedings of the International Conference on Parallel Processing, 1990.



++ <cite>Grama et al. 2003</cite>: Kumar and Singh [KS91b] present isoefficiency analysis and performance comparison of different parallel formulations for the all-pairs shortest paths on hypercube- and mesh-connected computers. The discussion in Section 10.4.3 is based upon the work of Kumar and Singh [KS91b] and of Jenq and Sahni [JS87]. 


Levitt and Kautz [LK72] present a formulation of Floyd's algorithm for two-dimensional cellular arrays that uses n^2 processors and runs in time Q(n). 

[LK72] K. N. Levitt and W. T. Kautz. Cellular arrays for the solution of graph problems. Communications of the ACM, 15(9):789–801, September 1972.


Deo, Pank, and Lord have developed a parallel formulation of Floyd's algorithm for the CREW PRAM model that has complexity Q(n) on n^2 processes.



Chandy and Misra [CM82] present a distributed all-pairs shortest-path algorithm based on diffusing computation.

[CM82] K. M. Chandy and J. Misra. Distributed computation on graphs: Shortest path algorithms. Communications of the ACM, 25(11):833–837, November 1982.



</document>

</document>