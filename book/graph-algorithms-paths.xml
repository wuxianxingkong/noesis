<?xml version="1.0"  encoding="ISO-8859-1" ?> 
<!DOCTYPE sys-ents [ 
  <!ENTITY bibliography   SYSTEM "bibliography.xml">
]> 
<?xml-stylesheet type="text/xsl" href="../book.xsl"?>

<!-- Shortest paths -->

<document>
&bibliography;
<tag>graph-shortest-paths</tag>
<title>Shortest paths</title>


<text>
When graphs are weighted, i.e. each edge in the graph is associated to a numerical value, computing their shortest paths is more complicated than performing breadth-first traversals (recall that the BFS tree leads to minimum-number-of-links paths)...
</text>

<text>
Shortest paths in directed acyclic graphs can also be found in linear time: Perform a topological sorting and then process the vertices
from left to right. Observe that <eqn>d(s, j) = min_(x,i) d(s, i) + w(i, j)</eqn> since we already know the shortest path d(s, i) for all vertices to the left of j. The same algorithm (replacing min with max) also suffices to find the longest path in a DAG, which is useful in scheduling applications.
</text>


<note>
<title>The Strategy design pattern</title>

<text>
++ The Strategy design pattern <cite>Gamma et al. 1994</cite>
</text>
</note>


<text>
@ MIT Graphs - Shortest paths
</text>


<!-- Shortest paths: Single source -->

<document>
<tag>graph-shortest-paths-dijkstra</tag>
<title>Single source shortest paths</title>

<text>
Finding the shortest path from s to t in G.
Actually, finding the shortest path from a source node <eqn>s</eqn> to every other node in the network requires the same amount of work...
</text>

<text>
APPLICATIONS: Finding directions (note: hierarchical A*), transportation and communications networks, distinguishing
among homophones iin speech recognition systems, graph visualization algorithms...

Approximate shortest paths in road networks, for instance, typically employ a hierarchical variation of A* <cite>Goldberg et al. 2006</cite> <cite>Goldberg et al. 2007</cite>. Such problem differ from the shortest path problems in this Section because
(1) preprocessing costs can be amortized over many point-to-point queries, 
(2) the backbone of high-speed, long-distance highways can reduce the path problem to identifying the best place to get on and off this backbone, and (3) approximate or heuristic solutions suffice in practice
</text>



<text>
IDEA: Shortest path from s to t goes through x, then that path contains the shortest path from s to x
</text>

<text>
Similar to Prim's algorithm... each iteration determines the shortest path from s to a new vertex x by minimizing d(s,v)+w(v,x)

In Prim's algorithm, we just take into account the weighte of the edge we add to the MST; in Dijkstra's algorithm, we consider the cost of the whole path from s to x (i.e. the final edge weight plus the distance from s to the tree vertex that is adjacent to the new edge). Dijkstra's algorithm returns a shortest-path spanning tree from s to all the other nodes in the network.
</text>

<text>
<url>http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm</url>
Dijkstra's algorithm <cite>Dijkstra 1959</cite>: 

computes the shortest path from a given starting vertex x to all n . 1
other vertices. In each iteration, it identifies a new vertex v for which the shortest
path from x to v is known. We maintain a set of vertices S to which we currently
know the shortest path from v, and this set grows by one vertex in each iteration.
In each iteration, we identify the edge (u,v) so that
<eqn>dist(x, u) + weight(u, v) = min_(w,y) { dist(x, w) + weight(w,y) }</eqn>

This edge (u, v) gets added to a shortest path tree, whose root is x and describes
all the shortest paths from x.

If we just need to know the shortest
path from x to y, terminate the algorithm as soon as y enters S.
</text>

<text>
EFFICIENCY:
- <eqn>O(n^2)</eqn> using a naive implementation with arrays
- Using binary heaps: <eqn>O(m \log n)</eqn>
- Fastest implementation: <eqn>O(m + n \log n)</eqn> amortized time using Fibonacci heaps <cite>Fredman and Tarjan 1987</cite>
</text>

<text>
Related problem: Single-destination shortest path problem for directed graphs.
</text>

<text>
CORRECTNESS: Only on graphs without negative edges!!!

Dijkstra's algorithm assumes that all edges have positive cost. 
- Adding a fixed amount of weight to make each edge positive does
not solve the problem. Dijkstra's algorithm will then favor paths using a fewer
number of edges,

For graphs with edges of negative weight,
you must use the more general, but less efficient, Bellman-Ford algorithm.

NOTE: negative cost cycles =the shortest x to y path in such a graph is not defined because we can detour
from x to the negative cost cycle and repeatedly loop around it, making the total cost arbitrarily small.


</text>

<text>
In a geometric setting, more efficient geometric algorithms compute the shortest path directly from the arrangement of obstacles: motion planning algorithms <cite>Latombe 1991</cite> <cite>LaValle 2006</cite>
</text>


<text>
PARALLELIZATION:
Parallelization of Dijkstra's algorithm
- for dense graphs @ Section 10.3 <cite>Grama et al. 2003</cite>

Dijkstra's algorithm is almost identical to Prim's minimum spanning tree algorithm. The main difference is that, for each vertex, Dijkstra's algorithm stores the minimum cost to reach the vertex from the source, whereas Prim's algorithm stores the cost of the minimum-cost edge connecting the vertex to the minimum spanning tree. 

"The parallel formulation of Dijkstra's single-source shortest path algorithm is very similar to the parallel formulation of Prim's algorithm for minimum spanning trees. The weighted adjacency matrix is partitioned using the 1-D block mapping. Each of the p processes is assigned n/p consecutive columns of the weighted adjacency matrix, and computes n/p values of the array l. During each iteration, all processes perform computation and communication similar to that performed by the parallel formulation of Prim's algorithm. Consequently, the parallel performance and scalability of Dijkstra's single-source shortest path algorithm is identical to that of Prim's minimum spanning tree algorithm."


Efficiency (for dense graphs, using their adjacency matrix)

1) Computation: \Theta(n/p)

2) Single-node accumulation = all-to-one reduction (global minimum): \Theta(log p) @ message-passing parallel computer

3) One-to-all broadcast: \Theta(log p) @ message-passing parallel computer

Overall

1) Computation: \Theta(n^2/p)

2) Communication: \Theta(n \log p)


Parallel run time:
t_p = \Theta \left ( \frac{n^2}{p} + n \log p \right ) 


Since the sequential run time is W=\Theta(n^2),

speedup

S \in \Theta \left ( \frac{n}{n/p + \log p} \right ) 


efficiency

E = \frac{1}{1 + \Theta ( (p \log p) / n ) }


i.e. 
A cost-optimal parallel implementation requires (p \log p) / n  \in O(1), hence the parallel implementation of Dijkstra's algorithm can use only p=O(n \log n) different processors.  

Isoefficiency function: \Theta(p^2 \log p^2)

tp = n^2/p + n log(p)
ts = n^2 = W

to = p*tp - W = n^2 + np log(p) - n^2 = n p log(p)

E = 1 / (1 + to/W)

Communication overhead: p^2 log^2(p)
	tc = n p log (p)
	W  = n^2
	W = K tc  ===  n^2 = n p log(p)  ===  n = p log(p)  === W=n^2= p^2 log^2(p)
+
Concurrency: p^2
	Since n must grow at least as fast as p... W = n^2 = p^2 
=
Isoefficiency: \Theta(p^2 \log p^2)

+ Bibliographic notes
</text>



</document>



<!-- Shortest paths: All pairs -->

<document>
<tag>graph-shortest-paths-all-pairs</tag>
<title>All-pairs shortest paths</title>


<text>
Some problems requite computing the shortest paths between all pairs of nodes, e.g. the central node (i.e. the node that minimizes the longest or average distance to every other node) or the network diameter (i.e. the longest shortest-path distance between two nodes in the network). 
</text>

<text>
EXAMPLE: 
- bounding the amount of time needed to deliver a packet through a network
</text>

<text>
+ Best on adjacency matrix (anyway we will need to store <eqn>n \times n</eqn> distances), i.e. one of the rare cases where algorithms on adjacency matrices work better
</text>


<text>
A first solution: Repeating Dijkstra's algorithm for each node
</text>

<!-- Floyd-Warshall -->

<text>
<url>http://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm</url>
Floyd-Warshall algorithm: <cite>Floyd 1962</cite>
often referred to as Floyd's algorithm, albeit the same algorithm was published by Bernard Roy in 1959 <cite>Roy 1959</cite> and by Stephen Warshall in 1962 <cite>Warshall 1962</cite>
</text>


- Define the length of the shortest path from i to j using only the k first vertices as possible intermediate nodes: <eqn>W[i,j]^k</eqn>

- Initial shortest-path matrix = adjacency matrix

- <eqn>W[i,j]^k = min \{ W[i,j]^{k-1}, W[i,k]^{k-1} + W[k,j]^{k-1} \}</eqn>


IDEA: 

- Minimum-weight path p^{(k)}_{i,j} from v_i to v_j among the subset of k vertices
- Minimum distance d^{(k)}_{i,j} = weight of p^{(k)}_{i,j}

When a vertex v_k is not in the shortest path from v_i to v_j, then p^{(k)}_{i,j} = p^{(k-1)}_{i,j}
When the vertex v_k is in the shortest path from v_i to v_j, then p^{(k)}_{i,j} = p^{(k-1)}_{i,k} + p^{(k-1)}_{k,j}

<equation>
d^{(k)}_{i,j} = min \{ d^{(k-1)}_{i,j}, d^{(k-1)}_{i,k} + d^{(k-1)}_{k,j} \}
</equation>

given that

<equation>
d^{(0)}_{i,j} = w (v_i, v_j)
</equation>


Simple <eqn>\Theta(n^3)</eqn> implementation (for dense graphs)
- Compact implementation: 3 nested loops
- Ancillary matrix to reconstruct the paths

<eqn>\Theta(n^2)</eqn> space complexity, since <eqn>D^(k)</eqn> can be computed from <eqn>D^(k-1)</eqn>


<code>
D0 = M
for k = 1 to n do
    for i = 1 to n do
        for j = 1 to n do
            Dk(i,j) = min{ Dk-1(ij), Dk-1(ik) + Dk-1(kj) }
return Dn


  for k := 1 to n
        for i := 1 to n
           for j := 1 to n
              if path[i][k] + path[k][j] &lt; path[i][j] then
                 path[i][j] := path[i][k]+path[k][j];
                 next[i][j] := k;

</code>

<text>
Transitive closure of directed graphs (Warshall's algorithm). In Warshall's original formulation of the algorithm, the graph is unweighted and represented by a Boolean adjacency matrix. Then the addition operation is replaced by logical conjunction (AND) and the minimum operation by logical disjunction (OR).
</text>

<!-- Bellman-Ford -->

<text>
<url>http://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm</url>
Bellman-Ford algorithm: <eqn>O(nm)</eqn> time, <eqn>O(n)</eqn> space

Bellman-Ford algorithm <cite>Bellman 1958</cite> <cite>Ford and Fulkerson 1962</cite>

+ Negative weights (arise when we reduce other problems to shortest-paths problems)
</text>

<code>
   // Step 1: initialize graph
   for each vertex v in vertices:
       if v is source then v.distance := 0
       else v.distance := infinity
       v.predecessor := null

   // Step 2: relax edges repeatedly
   for i from 1 to size(vertices)-1:
       for each edge uv in edges: // uv is the edge from u to v
           u := uv.source
           v := uv.destination
           if u.distance + uv.weight &lt; v.distance:
               v.distance := u.distance + uv.weight
               v.predecessor := u

   // Step 3: check for negative-weight cycles
   for each edge uv in edges:
       u := uv.source
       v := uv.destination
       if u.distance + uv.weight &lt; v.distance:
           error "Graph contains a negative-weight cycle"
</code>

<!-- Johnson -->

<text>
<url>http://en.wikipedia.org/wiki/Johnson%27s_algorithm</url>
Johnson's algorithm <cite>Johnson 1977</cite> 

1
First, a new node q is added to the graph, connected by zero-weight edges to each of the other nodes.

2
Second, the Bellman-Ford algorithm is used, starting from the new vertex q, to find for each vertex v the least weight h(v) of a path from q to v. If this step detects a negative cycle, the algorithm is terminated.

3
Next the edges of the original graph are reweighted using the values computed by the Bellman-Ford algorithm: an edge from u to v, having length w(u,v), is given the new length w(u,v) + h(u) - h(v).

4
Finally, q is removed, and Dijkstra's algorithm is used to find the shortest paths from each node s to every other vertex in the reweighted graph.


<eqn>O(n^2 \log n + nm)</eqn> using <eqn>O(nm)</eqn> time for the Bellman-Ford stage of the algorithm, and <eqn>O(n \log n + m)</eqn> for each of <eqn>n</eqn> instantiations of Dijkstra's algorithm. Faster than the Floyd-Warshal algorithm for sparse graphs.


It allows some of the edge weights to be negative
</text>


<!-- Applications -->

<text>
All-pairs shortest paths algorithms can be used to find the shortest cycle in a graph: its girth. Floyd's algorithm can be used to compute <eqn>d_ii</eqn>, which is the shortest way to get from vertex i to itself (i.e. the shortest cycle that goes through i). If you want the shortest simple cycle, the easiest
approach is to compute the lengths of the shortest paths from i to all other vertices, and then explicitly check whether there is an acceptable edge from each vertex back to i.
</text>


<text>
- Reachability questions (can I get to x from y?)

RELATED PROBLEMS 
+ Transitive closure, construct a graph G' = (V,E') with edge (i, j) in E' iff there is a directed path from i to j in G. 
+ Transitive reduction (also known as minimum equivalent digraph), the inverse operation of transitive closure: reducing the number of edges while maintaining identical reachability properties, i.e. construct a small graph G'' = (V,E'') with a directed path from i to j in G'' iff there is a directed path from i to j in G

- transitive closure 

1) using graph traversal algorithms O(n(n+m)), 
2) an all-pairs shortest path algorithms returns them all in a whole batch, albeit less efficiently, <eqn>O(n^3)</eqn> using Warshall's algorithm.
3) using matrix multiplication: O(log n) multiplications using a divide-and-conquer algorithm

- transitive reduction

Approximate solution: A linear-time, quick-and-dirty transitive reduction algorithm identifies the
strongly connected components of G, replaces each by a simple directed cycle,
and adds these edges to those bridging the different components.

+ Transitive reduction also arises in graph drawing, where it is important to eliminate as many unnecessary edges as possible to reduce the visual clutter
</text>


<text>
- pattern recognition problems, e.g. Viterbi algorithm, a dynamic programming algorithm that basically solves a shortest path problem on a DAG.
</text>


<!-- Parallelization -->

<text>
PARALLELIZATION:
- for dense graphs @ Section 10.4 <cite>Grama et al. 2003</cite>
+ Bibliographic notes
</text>

3 algorithms: matrix multiplication, Dijkstra's single-source shortest paths, Floyd's algorithm

Note: Dijkstra's algorithm requires non-negative edge weights, the other two algorithms work with graphs containing negative-weight edges provided that there are not negative-weight cycles.

<document>
<title>Parallel algorithm based on matrix multiplication</title>

<text>
Matrix-multiplication-based algorithm:
</text>

- Minimum-weight path p^{(k)}_{i,j} from v_i to v_j containing k edges

- Minimum distance d^{(k)}_{i,j} = weight of p^{(k)}_{i,j}

- Path decomposition: p^{(k)}_{i,j} = p^{(k-1)}_{i,m} + (v_m, v_j)

- Therefore,  d^{(k)}_{i,j} = min { d^{(k-1)}_{i,j}, d^{(k-1)}_{i,m} + w(v_m,v_j) }


- Taking into account that w(v_j,v_j)=0

<equation>
d^{(k)}_{i,j} = min \{ d^{(k-1)}_{i,m} + w(v_m,v_j) \}
</equation>

If we consider the matrix <eqn>D^(k) = ( d^{(k)}_{i,j} )</eqn>, then <eqn>D^(n-1)</eqn> contains the weights of the shortest paths between all pairs of vertices in G. Given that <eqn>D^(k)</eqn> can be computed from <eqn>D^(k-1)</eqn> using a modified matrix multiplication algorithm (performing addition and minimization instead of multiplication and addition), we can design a divide-and-conquer algorithm that obtains the desired result after <eqn>\lceil \log (n-1) \rceil</eqn> modified multiplications, i.e. <eqn>D^(n-1)</eqn> is computed by computing <eqn>D^2</eqn>, <eqn>D^2</eqn>, <eqn>D^8</eqn>...

The conventional <eqn>\Theta(n^3)</eqn> matrix-multiplication algorithm results in an <eqn>\Theta(n^3 \log n)</eqn> sequential all-pairs shortest paths algorithm that is easily parallelizable using parallel matrix multiplication algorithms such as Cannon's algorithm or the DNS algorithm, which are specially useful in massively-parallel architectures. 

- Cannon's algorithm <cite>Cannon 1969</cite> partitions matrices into p square blocks and schedules the computations so that each processor performs <eqn>\sqrt{p}</eqn> multiplications of <eqn>n/\sqrt{p} \times n/\sqrt{p}</eqn> matrices in <eqn>O(n^3/p)</eqn>. This parallel algorithm is optimal for <eqn>O(n^2)</eqn> processors and the isoefficiency function is <eqn>\Theta(p^{3/2)}</eqn>.

- The DNS algorithm, due to Dekel, Nassimi and Sahni <cite>Dekel et al. 1981</cite>, can use up to <eqn>n^3</eqn> processors in parallel and performs matrix multiplication in <eqn>\Theta(\log n)</eqn> time by using <eqn>\Theta(n^3 / \log n)</eqn> processors. The algorithm is cost-optimal for <eqn>O(p \log^3 p)</eqn> processors and its isoefficiency function is <eqn>\Theta(p \log^3 p)</eqn>.

ref. L. E. Cannon. A cellular computer to implement the Kalman Filter Algorithm. Ph.D. Thesis, Montana State University, Bozman, MT, 1969.
ref. E. Dekel, D. Nassimi, and S. Sahni. Parallel matrix and graph algorithms. SIAM Journal on Computing, 10:657–673, 1981
ref. G. C. Fox, S. W. Otto, and A. J. G. Hey. Matrix algorithms on a hypercube I: Matrix multiplication. Parallel Computing, 4:17–31, 1987.

++ Gupta and Kumar [GK91] present a detailed scalability analysis of several matrix multiplication algorithms
ref. A. Gupta and V. Kumar. The scalability of matrix multiplication algorithms on parallel computers. Technical Report TR 91-54, Department of Computer Science, University of Minnesota, Minneapolis, MN, 1991. A short version appears in Proceedings of 1993 International Conference on Parallel Processing, pages III-115–III-119, 1993.

Therefore, if we use the DNS algorithm with <eqn>n^3</eqn> processors, which requires <eqn>\Theta(\log n)</eqn> time for matrix multiplication, we can obtain a <eqn>\Theta(\log^2 n)</eqn> algorithm for solving the all-pairs shortest path problem. Since the best-known sequential algorithm has <eqn>\Theta(n^3)</eqn> time complexity, the speedup and efficiency of the parallel algorithm are:

<equation>
S \in \Theta \left ( \frac{n^3}{\log^2 n} \right ) 
</equation>

<equation>
E \in \Theta \left ( \frac{1}{\log^2 n} \right ) 
</equation>

<text>
When we use <eqn>\Theta(n^3 / \log n)</eqn> processors, the efficiency of this algorithm can be improved to <eqn>\Theta(1 / \log n)</eqn>, yet the algorithm is still unscalable. This is the fastest-known parallel algorithm for solving the all-pairs shortest paths problem. However, slower cost-optimal algorithms are employed in practice.
</text>

</document>

<document>
<title>Parallelization based on Dijkstra's algorithm</title>

<text>
Dijkstra's single-source shortest paths algorithms can be used to solve the all-pairs shortest paths problem just by executing it for each vertex. Dijkstra's all-pairs shortest paths algorithm can be parallelized in two different ways:
</text>

<list>

<item>Source-partitioned Dijkstra's all-pairs shortest paths: Vertices are partitioned among different processors and each processor computes the single-source shortest paths for its assigned vertices.</item>

<item>Source-parallel Dijkstra's all-pairs shortest paths: Each vertex is assigned to a set of processors and the parallel version of Dijkstra's single-source shortest paths algorithm is used.</item>

</list>


<text>
The source-partitioned version requires no interprocessor communication, hence the parallel run time is <eqn>t_p \in \Theta(n^2)</eqn> for dense graphs. Since the sequential time for the all-pairs shortest paths problem is <eqn>W=\Theta(n^3)</eqn> for dense graphs, the source-partitioned version achieves a linear speedup <eqn>\Theta(n)</eqn> and optimal efficiency <eqn>\Theta(1)</eqn>. However, the algorithm is poorly scalable, since it can use <eqn>n</eqn> processors at most and its isoefficiency function due to concurrency is <eqn>\Theta(p^3)</eqn>.
</text>

<text>
When the number of available processors p is greater than the number of nodes n, the source-parallel version can improve the performance of the source-partitioned version by partitioning the p processors into n partitions with p/n processors. Parallelizing both the all-pairs algorithm by assigning each vertex to a separate set of processors and the single-source algorithm by using sets of p/n processors, we could use up to n^2 processors in parallel. Assuming an hypercube interconnection network or an equivalent message-passing parallel computer, the source-parallel algorithm can use <eqn>O(n^2 / \log n)</eqn> processors efficiently, requires just <eqn>O(n \log n)</eqn> parallel run time, and its isoefficiency function drops to <eqn>O(p^{3/2} \log^{3/2} p )</eqn>, a substantial improvement over the source-partitioned version.
</text>

</document>

<document>
<title>Parallelization of Floyd's algorithm</title>

<text>
Parallelization strategy for computing the <eqn>D^{(k)}</eqn> matrix using <eqn>p</eqn> processors: Matrix <eqn>D^{(k)}</eqn> is partitioned into <eqn>p</eqn> parts and each part is assigned to a different processor. In order to perform this, each processor need access to segments of rows and columns of the <eqn>D^{(k-1)}</eqn> matrix.
</text>

<text>
Techniques for partitioning the <eqn>D^{(k)}</eqn> matrix:
</text>

<list>

<item>
Block-striped mapping: Each of the <eqn>p</eqn> processors is assigned <eqn>n/p</eqn> consecutive columns of the <eqn>D^{(k)}</eqn> matrix.
</item>

<item>
Block-checkerboard mapping: The <eqn>D^{(k)}</eqn> matrix is divided into <eqn>p</eqn> squares of size <eqn>n/\sqrt{p} \times n/\sqrt{p}</eqn> and each square is assigned to a different processor.
</item>

<item>
Pipelined block-checkerboard mapping: Rather than waiting for the (k-1)-th iteration to complete and ensure that all processors have the appropriate segments of the <eqn>D^{(k-1)}</eqn> matrix, a processor can start its work on the k-th iteration as soon as it has computed its part of the <eqn>D^{(k-1)}</eqn> matrix and has received the relevant parts of the <eqn>D^{(k-1)}</eqn> matrix.
</item>

</list>


<text>
Using the last parallelization strategy, let us consider what happens on a p-processor 2D mesh. In each algorithm step, <eqn>n/\sqrt{p}</eqn> elements from a row are sent from processor <eqn>P_{i,j}</eqn> to processors <eqn>P_{i+1,j}</eqn> and <eqn>P_{i-1,j}</eqn>. Likewise, <eqn>n/\sqrt{p}</eqn> elements from a column are sent from processor <eqn>P_{i,j}</eqn> to processors <eqn>P_{i,j+1}</eqn> and <eqn>P_{i,j-1}</eqn>. Each step requires <eqn>\Theta(n/\sqrt{p})</eqn> time and after <eqn>\Theta(\sqrt{p})</eqn> steps, i.e. in <eqn>\Theta(n)</eqn> time, every processor gets the relevant elements it needs from the first row (and column). Successive rows and columns are received every <eqn>\Theta(n^2/p)</eqn> time in pipeline mode. Hence, every processor can complete its computation in time <eqn>\Theta(n^3/p)</eqn> and send the relevant information to other processors in <eqn>\Theta(n)</eqn> time. The parallel run time required by the algorithm is, therefore,
</text>

<equation>
t_p \in \theta \left( \frac{n^3}{p} + n \right )
</equation>

<text>
Since the sequential run time for dense networks is <eqn>W \in \Theta(n^3)</eqn>, the parallel algorithm speedup and efficiency are
</text>

<equation>
S \in \Theta \left ( \frac{n^2}{n^2/p + 1} \right )
</equation>

<equation>
E = \frac{1}{1 + \Theta(p/n^2)}
</equation>

<text>
This algorithm is cost-optimal for <eqn>p/n^2 \in O(1)</eqn>, hence the pipelined algorithm can use up to <eqn>O(n^2)</eqn> processors efficiently and its isoefficiency function is <eqn>\Theta(p^{3/2})</eqn>.
</text>

tp = n^3/p + n
ts = n^3 = W

to = p*tp - W = n^3 + np - n^3 = np

E = 1 / (1 + to/W) = 1 / (1 + p/n^2)

	p = n^2 === n = p^{1/2}
	W  = n^3
	W  = K to  ===  n^3 = K p^{3/2}

Communication overhead: p^{3/2}
	tc = np
	W = n^3
	W = K to  ===  n^3 = np  ===  n = sqrt(p)  === W = n^3 = p^{3/2}
+
Concurrency: p
	Since n must grow at least as fast as p... W = n^3 = p^3
=
Isoefficiency: \Theta(p^{3/2})


<text>
Table <ref>parallel-all-pairs</ref> compares the performance and scalability properties of different algorithms for solving the all-pairs shortest paths problem for dense networks on architectures with <eqn>O(p)</eqn> bisection bandwidth. The pipelined parallel implementation of Floyd's algorithm is the most scalable: it can use up to <eqn>\Theta(n^2)</eqn> processors to compute the all-pairs shortest paths in linear <eqn>\Theta(n)</eqn> time and it performs equally well on hypercubes with <eqn>O(p)</eqn> bisection bandwidth and meshes with <eqn>O(\sqrt{p})</eqn> bisection bandwidth, both with store-and-forward and cut-through routing <cite>Grama et al. 2003</cite>.
</text>


<dataset>
 <tag>parallel-all-pairs</tag>
 <title>Performance and scalability of parallel all-pairs shortest paths algorithms</title>
 <metadata>
  <field width="25">Algorithm</field>
  <field width="25">Maximum number of processors for <eqn>E \in \Theta(1)</eqn></field>
  <field width="25">Parallel run time</field>
  <field width="25">Isoefficiency function</field>
 </metadata>
 <record>
  <field>Matrix multiplication</field>
  <field><eqn>-</eqn></field>
  <field><eqn>\Theta(\log^2 n)</eqn></field>
  <field><eqn>-</eqn></field>
 </record>
 <record>
  <field>Dijkstra (source-partitioned)</field>
  <field><eqn>\Theta(n)</eqn></field>
  <field><eqn>\Theta(n^2)</eqn></field>
  <field><eqn>\Theta(p^3)</eqn></field>
 </record>
 <record>
  <field>Dijkstra (source-parallel)</field>
  <field><eqn>\Theta(n^2 / \log n)</eqn></field>
  <field><eqn>\Theta(n \log n)</eqn></field>
  <field><eqn>\Theta(p^{3/2} \log^{3/2} p)</eqn></field>
 </record>

 <record>
  <field>Floyd (block-striped)</field>
  <field><eqn>\Theta(n / \log n)</eqn></field>
  <field><eqn>\Theta(n^2 \log n)</eqn></field>
  <field><eqn>\Theta(p^3 \log^3 p)</eqn></field>
 </record>
 <record>
  <field>Floyd (block-checkerboard)</field>
  <field><eqn>\Theta(n^2 / \log^2 n)</eqn></field>
  <field><eqn>\Theta(n \log^2 n)</eqn></field>
  <field><eqn>\Theta(p^{3/2} \log^3 p)</eqn></field>
 </record>
 <record>
  <field>Floyd (pipelined block-checkerboard)</field>
  <field><eqn>\Theta(n^2)</eqn></field>
  <field><eqn>\Theta(n)</eqn></field>
  <field><eqn>\Theta(p^{3/2})</eqn></field>
 </record>

</dataset>


</document>


<!-- Distributed algorithm -->

<text>
DISTRIBUTED ALGORITHM:
Wikipedia: A distributed variant of the Bellman-Ford algorithm is used in distance-vector routing protocols, for example the Routing Information Protocol (RIP). The algorithm is distributed because it involves a number of nodes (routers) within an Autonomous system, a collection of IP networks typically owned by an ISP. It consists of the following steps:
- Each node calculates the distances between itself and all other nodes within the AS and stores this information as a table.
- Each node sends its table to all neighboring nodes.
- When a node receives distance tables from its neighbors, it calculates the shortest routes to all other nodes and updates its own table to reflect any changes.
</text>


<text>
Distributed shortest paths @ MIT 6.852: Lectures 2-3 (synchronous) + Lectures 8-9 (asynchronous)  / Section 4.3 + Section 15.4 <cite>Lynch 1997</cite>
</text>

</document>

</document>