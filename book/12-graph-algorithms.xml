<?xml version="1.0"  encoding="ISO-8859-1" ?> 
<?xml-stylesheet type="text/xsl" href="../book.xsl"?>


<document>
<title>Graph Algorithms</title>


<text>
<q>The key to solving many algorithmic problems is to think of them in terms of graphs... it is amazing how often messy applied problems have a simple description and solution in terms of classical graph properties</q> <cite>Skiena 2008</cite>.
</text>


<text>
0. Propaedeutic
1. Graph representation
2. Graph exploration
3. Optimization problems: MST, shortest paths, maximum flow
4. Hard problems, e.g. graph isomorphisms
5. A catalog of graph algorithms
</text>


<!-- Propaedeutic -->

<document>
<tag>graph-representation</tag>
<title>Propaedeutic</title>

<text>
An introduction to an art and science of algorithm analysis and design... as a prerrequisite for later sections.
</text>

<document>
<title>A primer on algorithm analysis</title>

<text>
Big-Oh notation...
</text>

<text>
Worst-case analysis...
</text>

<text>
Amortized analysis bounds the total amount of time used by any sequence of operations (e.g. coffee @ office or coffee shop). Used when single operations might prove costly but their cost is distributed among many fast operations... O(f(n)) in amortized analysis is worse than O(f(n)) in the worst case, but it might still be useful in practice. Provided that individual response time is not critical, amortized operations might help obtain a good throughput...
</text>

<text>
The dominance pecking order in algorithm analysis <cite>Skiena 2008</cite>:
</text>

<equation>
n! >> c^n >> n^3 >> n^2 >> n^{1+\epsilon} >> n \log n 
</equation>

<equation>
>> n >> sqrt(n) >> \log^2 n >> \log n
</equation>

<equation>
>> \log n / \log \log n >> \log \log n >> \alpha(n) >> 1
</equation>

<list>
<item><eqn>\alpha(n)</eqn>: Inverse Ackerman's function, which grows notoriously slowly (union-find data structure).</item>
<item><eqn>\log n / \log \log n</eqn>: Height of an n-leaf tree of degree <eqn>d = \log n</eqn>.</item>
<item><eqn>n^{1+\epsilon}</eqn>, e.g. <eqn>2^c*n^{1+1/c}</eqn> where <eqn>c</eqn> cannot be arbitrarily large (<eqn>2^c!</eqn>).</item>
</list>

<text>
++ polylogarithmic, polylogarithmic function in n is a polynomial in the logarithm of n,
a_k \log^k (n) + ... + a_1 \log(n) + a_0

</text>

</document>


<document>
<title>Fundamental data structures</title>


<text>
The importance of data structures @ <cite>Skiena 2008</cite>, chapter 3, page 65: <q>Changing the data structure does not change the correctness of the program, since we presumably replace a correct implementation with a different correct implementation. However, the new implementation of the data type realizes different tradeoffs in the time to execute various operations, so the total performance can improve dramatically</q>.
</text>

<text>
3 fundamental data structures, provided as ADTs whose implementation might vary: containers, dictionaries, and priority queues.

DESIGN IDEA: Isolate the interface of the data structure from its particular implementation, so that it can be changed if needed.
</text>


<document>
<title>Collections</title>

<text>
The base container in any collection framework...
</text>

<text>
- Unsorted linked lists or arrays: sequential vs. random access; memory fragmentation and cache performance (spatial locality)
- Sorted linked lists or arrays (-maintenance costs, only appropriate if there are not too many insertions and deletions, +logical deletions, +easy duplicate elimination, +binary search)
- Self-organizing lists: every time a key is accessed, the corresponding object is moved to the head of the list (temporal locality)
- Dynamic arrays (e.g. Java)
</text>

<text>
Standard collection frameworks: C++ Standard Template Library (STL), Java Collections Framework (JCF)...
</text>

</document>


<document>
<title>Dictionaries</title>

<text>
Managing collections of objects than can be identified by keys...
</text>

<text>
KEY OPERATIONS: Efficiently locate, insert, and delete the object associated to a particular key.
</text>

<text>
Multiple data structures have been used for implementing dictionaries, including different incarnations of hash tables and balanced trees:
</text>

<list>

<item>
Hash tables (a function maps keys to integers, which indicate positions within a collection). Well-tuned = O(1). Warning! Check its performance (it can slow down your application if not properly configured).
</item>

<item>
Binary and n-way search trees: O(log n) operations. Unbalanced search trees can degenerate into linked lists, hence rebalancing mechanisms are recommended (AVL-trees, red-black trees, and splay trees; B-trees for secondary storage). Splay trees, for instance, move any accessed key to the top of the tree, in order to exploit locality of reference (as self-organizing lists). B-trees collapse several levels of a BST into a single node in order to benefit from the system-specific page size (or block size in secondary storage) thus reducing cache misses, swapping, or disk accesses.  
</item>

<item>
Skip lists: A modern data structure that is easier to implement than balanced trees. Skip lists consist of a hierarchy of sorted linked lists. Roughly ln n lists, each one half as large as the one above it, support O(ln n) queries using amortized analysis.
</item>

</list>

<text>
Choosing the proper alternative: access patterns (e.g. relative frequencies of insert, delete, and search operations: hash table vs. sorted list). Efficient implementations always try to minimize the number of block data transfers. The so-called cache-oblivious data structures <cite>Frigo et al. 1999</cite> offer performance guarantees without explicit knowledge of the particular block size.
</text>

</document>


<document>
<title>Priority queues</title>

<text>
Managing collections of objects than can be identified by ordered keys...
</text>

<text>
KEY OPERATION: Efficiently access to the object corresponding to the smallest or largest key.
</text>

<text>
Objects are retrieved by their key priority, rather than by their insertion time (as happens with LIFO stacks or FIFO queues) or their key value (as in dictionaries).
</text>

<text>
When no insertions and deletions are performed, a sorted collection suffices. When such operations are mixed with queries, full-fledged priority queues can be implemented using:
</text>

<list>

<item>Binary heaps support insertion and minimum extraction in O(log n) time, but no deletion. Internally, they are implemented using arrays as implicit binary trees where the key of the root of every subtree is always lower than the key of all its descendants. Hence, the minimum is always readily avaiable at the root of the tree.</item>

<item>Fibonacci heaps <cite>Fredman and Tarjan 1987</cite> are unbalanced heaps designed to speed up decrease-key operations (i.e. when the priority of an item in the priority queue is reduced). They support insertion and decrease-key operations in constant amortized time. Applications: fasteer implementations of shortest paths, weighted bipartite matching, and minimum spanning trees.</item>

<item>Pairing heaps <cite>Stasko and Vitter 1987</cite> report the same bounds than Fibonacci heaps with less overhead associated to their maintenance.</item>

<item>Binary search trees can be used as priority queues, since the smallest element is always at the leftmost tree leaf, and they support extraction, insertion, and deletion in logarithmic time when balancing mechanisms are employed.</item>

<item>Bounded-height priority queues: The right priority queue implementation for small ranges of keys. When keys have n different values, say 1..n, we keep an array of n linked lists and a pointer to the smallest non-empty list. Good enough in practice (using amortized analysis), do not guarantee good worst-case behavior. Useful, e.g., for maintaining graph nodes ordered by degree.</item>

<item>Emde Boas priority queues <cite>van Emde Boas et al. 1977</cite> support O(log log n) insertion, deletion, search, max, and min operations when the key is a value from 1 to n.</item>
</list>

<text>
ISSUES:

- Changing priorities and specific operations...

- There is a linear-time merging algorithm for heap construction <cite>Floyd 1964</cite>; 1.625n comparisons are sufficient <cite>Gonnet and Munro 1986</cite> and 1.5n-O(log n) comparisons are necessary <cite>Carlsson and Chen 1992</cite>.
</text>

<text>
Example application: Simulation (a queue of forthcoming events is kept.
</text>


</document>


</document>

</document>

<!-- Graph representation -->

<document>
<tag>graph-representation</tag>
<title>Graph representation</title>

<text>
Graphs <eqn>G=(V,E)</eqn> of order <eqn>n</eqn> (number of vertices or nodes) and size <eqn>m</eqn> (number of edges or links)
</text>


<document>
<tag>graph-representation-matrix</tag>
<title>Adjacency matrix</title>

<text>
The most elementary representation of a graph is the adjacency matrix, also known as the connection matrix.
</text>

<text>
Using a <eqn>n \times n</eqn> square matrix <eqn>A</eqn>, where <eqn>A[i][j]=1</eqn> if there is a link from node i to node j, <eqn>A[i][j]=0</eqn> otherwise.
</text>

<text>
+ Easy to check whether a link exists or not, addition/removal of links: <eqn>O(1)</eqn> operations.
</text>

<text>
- Wasted space, specially for sparse networks: <eqn>O(n^2)</eqn> space vs. <eqn>O(n)</eqn> actual size 
- impractical for large networks unless an alternative representation scheme is employed...

Only for small and or very dense graphs!

- Efficient implementation: bit vectors to represent binary/unweighted graphs.
</text>

<text>
Incidence matrix: An alternative representation scheme, where we use a <eqn>n \times m</eqn> square matrix <eqn>I</eqn>, where <eqn>I[i][j]=1</eqn> if node i is involved in link j, <eqn>I[i][j]=0</eqn> otherwise. Incidence matrices might be useful for representing hypergraphs, i.e. generalized graphs where each edge can link subsets of more then two nodes (on standard graphs, there will be just two non-zero entries in each column).
</text>
</document>


<document>
<tag>graph-representation-lists</tag>
<title>Adjacency lists</title>

<text>
Linked lists are employed to store the neighbors of each node. 
i.e. a directed link (x,y) will appear in x's adjacency list.
i.e. undirected edges (x,y) will appear twice: in both x's and y's adjacency lists.
+ Implemented using pointers/references or dynamic arrays (to reduce memory fragmentation)
</text>

<text>
+ Faster computation of node degrees: <eqn>O(d)</eqn> (vs. <eqn>O(n)</eqn> for adjacency matrix). NOTE: Can be precomputed
</text>

<text>
+ Slower check of edge existence, insertion/deletion of edges: <eqn>O(d)</eqn> (vs. <eqn>O(1)</eqn> for adjacency matrix)

NOTE: Most algorithms can be easily designed so that they do not need to perform such operations, just by scanning all the links in each adjacency list (e.g. see breadth-first and depth-first traversal below).
</text>

<text>
+ Lower memory requirements: <eqn>O(n+m)</eqn> (vs. <eqn>O(n^2)</eqn> for adjacency matrix)
</text>

<text>
+ Faster graph traversal: <eqn>O(n+m)</eqn> (vs. <eqn>O(n^2)</eqn> for adjacency matrix)
</text>

<text>
+ Efficient implementation: dynamic arrays rather than linked lists (removing unnecessary pointers)
</text>

<text>
Preferred choice for most problems: Adjacency lists for sparse graphs, adjacency matrix only for dense graphs (uncommon in data mining problems).
</text>


<text>
Bipartite incidence structures are to adjacency lists as incidence matrices to adjacency matrices and, as incidence matrices, can be used to represent hypergraphs (i.e. generalized graphs where each edge can link subsets of more then two nodes). The incidence structure contains a vertex for each node and link in the hypergraph. An edge (i,j) will appear in the incidence structure if link j is incident on node i in the hypergraph.
</text>



<note>
<text>
Algorithmic libraries often provide general-purpose implementations of graph data structures, hence there is no need to implement them yourself unless your problem imposes very specific requirements, e.g. C++
</text>

<list>

<item>LEDA (Library of Efficient Data types and Algorithms), <url>http://www.algorithmic-solutions.com/</url>, originally from the Max Planck Institute for Informatics in Saarbrücken, Germany; since 2001, maintained by the Algorithmic Solutions Software GmbH.</item>

<item>Boost Graph Library (BGL), <url>http://www.boost.org</url>.</item>

<item>MatlabBGL, <url>http://www.cs.purdue.edu/homes/dgleich/packages/matlab_bgl/</url>, for Matlab: BGL port.</item>

<item>Combinatorica, <url>http://www.combinatorica.com</url>, for Mathematica, by Steven Skiena.</item>

<item>JUNG, <url>http://jung.sourceforge.net/</url>, Java graph library</item>

<item>Stanford Graphbase, implemented in CWEB, a literate version of the C programming language</item>
</list>

<text>
Implementation issues:
- Attribute representation... as fields in link/node records
- Large networks... hierarchical representation (vertices cluster subgraphs), e.g. VLSI circuits using cell libraries
</text>
</note>

</document>


<document>
<tag>graph-representation-compression</tag>
<title>Graph compression</title>

<text>
++ Compression (Liu, "Web Data Mining", 232-242)
++ SNAP
</text>

</document>

</document>



<!-- Graph exploration -->

<document>
<tag>graph-exploration</tag>
<title>Graph exploration</title>

<text>
A fundamental graph problem: Visiting every node (or link) in a network in a systematic way, i.e. graph traversal.
</text>

<text>
++ CS
Graph-searching procedures such as depth-first search (DFS)
<cite>Tarjan 1972</cite> <cite>Hopcroft and Tarjan 1973</cite>
and breadth-first search (BFS) 
<cite>Moore 1959</cite>
form the basic preprocessing steps for most graph algorithms. 
</text>

<text>
For efficiency, we should not visit the same node repeatedly. For correctness, we should guarantee that every node is eventually visited. IDEA: Keep track of which nodes we have already visited...
</text>


<text>
BFS/DFS define a tree on the vertices of the graph.
</text>

<note>
<title>The Visitor design pattern</title>

<text>
++ Visitor design pattern <cite>Gamma et al. 1994</cite>
</text>

</note>


<document>
<tag>graph-exploration-dfs</tag>
<title>Depth-first search</title>

<text>
Implementation: recursion/stack (LIFO: we explore a path until we meet a dead end (i.e. no unvisited neighbors) and then backtrack). Recursion eliminates the need of an explicit stack and provides a neat implementation of the algorithm.
</text>


<text>
EFFICIENCY: <eqn>O(n+m)</eqn> using adjacency lists
</text>



<text>
Time intervals: a clock ticks every time we enter or exit a node and we keep track of the entry and exit times for each node.
</text>

<text>
Properties:

- Time intervals are properly nested: if we enter x before y, we exit y before x; i.e. all nodes reachable from a given node are explored before we finish with it.

- The difference between exit and entry times indicate the number of descendants of any given node. Actually half the time difference is the number of descendants in the search tree.

- For undirected graphs, DFS partitions edges into tree edges and back edges. Tree edges appear in the DFS tree, whereas back edges always point to an ancestor in the DFS tree (never to a sibling nor cousin node, just because all nodes reachable from a given node are explored before we finish with it).
</text>

<!--
Classifying (x,y) edges:

- parent[y] == x  => Tree edge

- discovered[y] and not processed[y]  => Back edge

For directed graphs:

- discovered[y] and (entry_time[y] &gt; entry_time[x]) => Forward edge

- discovered[y] and (entry_time[y] &lt; entry_time[x]) => Cross edge
-->


<text>
APPLICATIONS
</text>

<text>
++ CS        Algorithms based on DFS have been known for a long time for the problem of searching
mazes. However, it was the work of Hopcroft and Tarjan (for which they
received the ACM Turing Award in 1986) that illustrated the full algorithmic
power of DFS <cite>Hopcroft and Tarjan 1973</cite>. They demonstrated efficient algorithms for several problems,
such as finding biconnected components and bridges of a graph and testing
triconnectivity and planarity. DFS on directed graphs can be used to
classify its vertices into strongly connected components, to detect cycles,
and to find a topological order of the vertices of a DAG.
</text>


<text>
Solving mazes (e.g. <cite>Moore 1959</cite>): A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Tremaux as a strategy for solving mazes.
</text>

<text>
ON UNDIRECTED GRAPHS
</text>

<text>
++ Connected components: Identify the different contextec components of a graph G, where x and y are members of different components if no path exists from x to y in G. O(m+n)
</text>


<text>
++ Cycle detection / tree testing: If there are no back edges, all edges belong to the DFS tree, hence the graph is actually a tree. When present, the back edge and the DFS tree define the cycle.
</text>

<text>
++ Graph biconnectedness: Articulation vertices (a.k.a. cut-nodes): Vertices whose deletion disconnects the graph connected component (i.e. single points of failure). Graphs without articulation points are said to be biconnected, since they have no single points of failure. A brute force algorithm would remove each vertex and check whether the graph is still connected (an <eqn>O(n(n+m))</eqn> algorithm), albeit a linear algorithm is possible based on DFS if we track the earliest reachable node for each node. If the DFS tree represented the whole graph, all internal nodes would be articulation points. Three situations have to be considered <cite>Skiena 2008</cite>: 

1) root cut-nodes (if the root of the DFS tree has more than one child, then it is an articulation point), 

2) bridge cut-nodes (if v is an internal node in the DFS tree and its earliest reachable node is v, then v is an articulation point)

3) parent cut-nodes (if v is an internal node in the DFS tree and its earliest reachable node is its parent in the tree, then the parent is an articulation point unless it is the root of the DFS tree, i.e. it cuts v and its descendants from the graph).
</text>

<text>
++ Edge biconnectedness: Bridges (in terms of link failures instead of node failures). A single edge whose deletion disconnects the graph is called a bridge. Any graph without bridges is said to be edge-biconnected. Edge (x,y) is a bridge when it is a DFS tree edge and no back edge connects from y or below to x or above.
</text>


<text>
The classic sequential algorithm for computing biconnected components in a connected undirected graph due to John Hopcroft and Robert Tarjan (1973) runs in linear time, and is based on depth-first search <cite>Hopcroft and Tarjan 1973</cite>

In the online version of the problem, vertices and edges are added (but not removed) dynamically, and a data structure must maintain the biconnected components. <cite>Westbrook and Tarjan 1992</cite> developed an efficient data structure for this problem based on disjoint-set data structures. Specifically, it processes n vertex additions and m edge additions in <eqn>O(m \alpha(m, n))</eqn> total time, where alpha is the inverse Ackermann function. This time bound is proved to be optimal.

<cite>Tarjan and Vishkin 1985</cite> designed a parallel algorithm on CRCW PRAM that runs in O(log n) time with n + m processors. 

<cite>Cong and Bader 2005</cite> developed an algorithm that achieves a speedup of 5 with 12 processors on SMPs.
</text>


<text>
ON DIRECTED GRAPHS
</text>

<text>
++ Cycle detection / DAG testing: If there are no back edges, all edges belong to the DFS tree, hence the graph is actually a DAG. As above, when present, the back edge and the DFS tree define the cycle.
</text>

<text>
++ Topological sorting: A common operation on directed acyclic graphs. Finding a linear ordering of the vertices so that for each arc (i,j), vertex i is to the left of vertex j (such ordering cannot exist if the graph contains a directed cycle, i.e. no back edges). Each DAG has, at least, one topological sort (often, many different ones, up to n! if there are no constraints), which gives an ordering to process each vertex before any of its successors (e.g. precedence constraints). Only DAGs can be topologically sorted. 

1) Considering the nodes in the reverse order they are processed by DFS returns a valid topological sort.

2) Using a DFS to identify all source vertices (of in-degree 0), which can appear at the start of the topological sort without violating any constraints. Deleting outgoing edges of the source vertices creates new source vertices. The process is repeated until all vertices are included in the topological sort. Proper use of data structures is sufficient to obtain a linear time O(n+m) algorithm.

 ref. first described by <cite>Kahn 1962</cite>: http://en.wikipedia.org/wiki/Topological_sorting
 Applications: scheduling tasks under precedence constraints (any topological sort, a.k.a. linear extension, defines an order to perform the tasks that satisfies all precedence constraints).
</text>


<text>
++ Identifying weakly-connected components: Just ignore edge direction.
</text>

<text>
++ Identifying strongly-connected components (first algorithm by <cite>Tarjan 1972</cite>): 1) Traverse the graph atarting from any given node to discover the nodes that are reachable from the node. 2) Build a graph G'=(V,E') with the same vertex set but all its arcs reversed; i.e. <eqn>(y,z) \in E' iff (x,y) \in E</eqn>. 3) Perform a traveral starting from v in G', which will result in discovering the set of nodes that can reach v. 

If the second traversal does not completely traverse G', each DFS performed on G' will correspond to a strongly-connected component.
</text>

<text>
Any graph can be partitioned into a set of strongly-connected components. Using DFS, we can easily identify cycles. If we take into account that all the nodes involved in a cycle must belong to the same strongly-connected component, we can collapse the nodes in the cycle into a single vertex and repeat the process. When no cycles remain, each vertex represents a different strongly-connected component.
</text>

<text>
Linear SCC algorithms:

- Kosaraju's algorithm, 1978: http://en.wikipedia.org/wiki/Kosaraju%27s_algorithm
   ref.  <cite>Aho et al. 1983</cite> credit it to an unpublished paper from 1978 by S. Rao Kosaraju and Micha Sharir. (Al

- Cheriyan-Mehlhorn/Gabow algorithm, 1996/1999: http://en.wikipedia.org/wiki/Cheriyan%E2%80%93Mehlhorn/Gabow_algorithm
   ref. <cite>Cheriyan and Mehlhorn 1996</cite> <cite>Gabow 2003</cite>

- Tarjan's strongly connected components algorithm, 1972: http://en.wikipedia.org/wiki/Tarjan's_strongly_connected_components_algorithm
   ref. <cite>Tarjan 1972</cite>
   


 all efficiently compute the strongly connected components of a directed graph, but Tarjan's and Gabow's are favoured in practice since they require only one depth-first search rather than two. http://en.wikipedia.org/wiki/Strongly_connected_component
</text> 


<text>
Parallelization of DFS 
- for dense graphs @ Section 10.6 <cite>Grama et al. 2003</cite>
- Parallel DFS @ Section 11.4  <cite>Grama et al. 2003</cite>
</text>


</document>



<document>
<tag>graph-exploration-bfs</tag>
<title>Breadth-first search</title>

<text>
Implementation: queue (FIFO: we explore the oldest unexplored vertices first).
</text>

<text>
EFFICIENCY: <eqn>O(n+m)</eqn> using adjacency lists
</text>

<text>
Properties (due to the fact that each path in the tree must be the shortest path in the graph):

- For undirected graphs, edges not appearing in the breadth-first search tree can only point to nodes on the same level or to the level directly below.
</text>



<text>
APPLICATIONS
</text>

<text>
++ Shortest paths can be found by performing a breadth-first search on unweighted/binary graphs <cite>Moore 1959</cite>


(more elaborate algorithms are required for weighted graphs, as we will see later): the tree resulting from BFS defines the shortest paths from the root to the remaining nodes in the graph. IMPLEMENTATION: parent[node] @ Visitor + reversal using recursion/stack
</text>

<text>
++ Connected components: Any node we visit is part of the same connected component. Starting the search from any unvisited node, we can obtain additional connected components.
</text>

<text>
++ Two-coloring graphs = Bipartite testing: A graph is bipartite if it can be colored without conflicts using two colors. Whenever we visit a new node, we color it using the opposite of its parent's color. If edges appear with the same color at both ends, the graph is not bipartite. Otherwise, we have partitioned the graph.
</text>


<text>
++ Distributed algorithm @ MIT Lecture 2 (synchronous) + Lecture 9 (asynchronous) / Section 4.2 + Section 15.4 <cite>Lynch 1997</cite>
@ MIT Distributed algorithms Lecture 21
</text>

<text>
++ Sublinear approximate algorithm @ MIT Sublinear algorithms => Goal: Quickly distinguish inputs that have specific property from those that are far from having the property...
</text>


</document>



<document>
<tag>graph-exploration-best-first</tag>
<title>Best-first search</title>

<text>
Best-first search... heuristics

Judea Pearl described best-first search as estimating the promise of node n by a <q>heuristic evaluation function f(n) which, in general, may depend on the description of n, the description of the goal, the information gathered by the search up to that point, and most important, on any extra knowledge about the problem domain</q> <cite>Pearl 1984</cite>.

</text>

<text>
http://en.wikipedia.org/wiki/A*_search_algorithm
A* search algorithm

HISTORY: In 1964 Nils Nilsson invented a heuristic based approach to increase the speed of Dijkstra's algorithm. This algorithm was called A1. In 1967 Bertram Raphael made dramatic improvements upon this algorithm, but failed to show optimality. He called this algorithm A2. Then in 1968 Peter E. Hart introduced an argument that proved A2 was optimal when using a consistent heuristic with only minor changes. His proof of the algorithm also included a section that showed that the new A2 algorithm was the best algorithm possible given the conditions. He thus named the new algorithm in Kleene star syntax to be the algorithm that starts with A and includes all possible version numbers or A*. <cite>Hart et al. 1968</cite> <cite>Hart et al. 1972</cite>

Often used for path finding...
</text>


<text>
http://en.wikipedia.org/wiki/B*
B* algorithm <cite>Berliner 1979</cite>
</text>


<text>
++ Parallel best-first search @ Section 11.5 <cite>Grama et al. 2003</cite>
</text>

</document>

</document>


<!-- Minimum spanning trees -->

<document>
<tag>graph-spanning-trees</tag>
<title>Minimum spanning trees</title>

<text>
A spanning tree of a graph <eqn>G=(V,E)</eqn> is a subset of <eqn>n-1</eqn> edges from <eqn>E</eqn> that connect all the vertices in <eqn>V</eqn> and, therefore, form a tree. In weighted graphs, where edges have numerical values associated to them, known as weights, a minimum spanning tree is a spanning tree whose sum of edge values is as small as possible. A tree is always the smallest connected graph in terms of its number of edges (and also the most vulnerable to single failures), while a MST is the cheapest connected graph in term of edge costs.
</text>

<text>
The algorithms in this section can also be adapted to find maximum spanning trees (just negate the weights and the MST in the negated graph is the maximum spanning tree in the original graph), minimum product spanning trees (replace edge weights with their logarithms so that products are converted into sums), or minimum bottleneck spanning tree (i.e. a spanning tree that minimizes the maximum edge weight, by just employing Kruskal's algorithm). 
</text>

<text>
However, other related problems cannot be solved using the efficient algorithms described in this section:
- Steiner trees (when we can add intermediate vertices), NP-hard in general.
- Low-degree spanning tree (i.e. the minimum spanning tree whose maximum node degree is smaller; in case it where a simple path, this problem would be equivalent to finding a Hamiltonian path = TSP, NP-complete)
</text>

<text>
APPLICATIONS: 
- Whenever we need to connect a set of nodes with minimum cost (e.g. transportation networks, communication networks, distribution networks...). 
- Bottleneck spanning trees have applications when edge weights represent costs, capacities, or strengths
- Clustering (e.g. hierarchical clustering)
- They can be used to obtain approximate solutions to hard problems (Steiner trees or TSP below)
</text>


<text>
In unweighted graphs (or weighted graphs where all edges have the same cost), any tree is also a minimum spanning tree. Its cost will always be <eqn>n-1</eqn> times the cost of a single edge. In this case, a spanning tree can easily be found by performing any systematic graph traversal (e.g. DFS or BFS).
</text>

<text>
In weighted graphs, efficient greedy algorithms exist than can be employed to identify minimum spanning trees...
</text>


<document>
<tag>graph-spanning-trees-greedy</tag>
<title>Greedy MST algorithms</title>


<text>
@ MIT Graphs - Shortest paths
</text>

<text>
Greedy algorithms choose what to do next by selecting the best local candidate to be added to the solution and, when the optimality of the chosen heuristics can be proved, they provide efficient algorithms for solving problems such as finding the minimum spanning tree.
</text>

<text>
Candidates: 

- Edges (Kruskal's algorithm <cite>Kruskal 1956</cite>): Each vertex starts as a separate tree and separate trees are merged by adding the lowest-cost edge to the MST that does not create a cycle (i.e. connects two separate subtrees)

Kruskal(G):
Sort edges in order of increasing weight
edges_in_tree = 0
while edges_in_tree &lt; n-1
    get next edge (v,w)
    if component(v) != component(w)
       Add edge (v,w) to the MST
       component(v) = component(w)
       edges_in_tree++


- Nodes (Prim's algorithm <cite>Prim 1957</cite>, rediscovered by <cite>Dijkstra 1959</cite>): Start with an arbitrary vertex and iteratively grow the tree by adding the lowest-cost edge that links some new vertex to the tree.

Prim(G):
Select an arbitrary vertex
while...

- The oldest algorithm: Boruvka's algorithm, published in 1926 <cite>Boruvka 1926</cite> and rediscovered by Choquet in 1938 <cite>Choquet 1938</cite>

Since the lowest-weight edge incident on each vertex must be in the minimum spanning tree, the union of these edges will return a forest of at most n/2 trees. For each tree, select the edge (x,y) of lowest weight such that x \in T and y \notin T, which will also have to belong to the MST. Repeat until all vertices are connected. Each iteration at least halves the number of remaining trees, hence, after log n iterations, each taking linear time, we obtain the MST in O(m log n).


An inverse algorithm would also be possible...
  Algoritmo de  Algoritmo de borrado borrado inverso inverso::
  Comenzando con  Comenzando con T=A,  T=A, considerar considerar laslas aristas aristas en  en orden orden decreciente decreciente
  de coste de coste y y eliminar eliminar laslas aristas aristas de T salvo  de T salvo que que esoeso desconectase desconectase T.T.

</text>


<text>
MSTs are unique when all m edge weights are distinct. The way ties are broken leads to different MSTs...
</text>

<document>
<tag>graph-spanning-trees-greedy-prim</tag>
<title>Prim's algorithm</title>

<text>
Starts from any node and adds one edge at a time until all the vertices are connected... heuristics: select the edge with the smallest weight that will enlarge the number of vertices in the tree
</text>

<text>
No cycles are created since only edges between tree and non-tree nodes are added to the solution
</text>

<text>
EFFICIENCY:
<eqn>n</eqn> iterations through <eqn>m</eqn> edges = <eqn>O(mn)</eqn>
</text>

<text>
PROOF? by contradiction
</text>

<text>
Tree reconstruction: parent vector or dynamic structure...
</text>

</document>

<document>
<tag>graph-spanning-trees-greedy-kruskal</tag>
<title>Kruskal's algorithm</title>

<text>
Instead of starting on any particular node and growing a tree, Kruskal's algorithm builds the minimum spanning tree by connecting sets of vertices...
</text>

<text>
Initially: each vertex a separate component + Iteration: consider the least cost edge and test whether it connects two separate components (if its both endpoints are already in the same connected component, the edge is discarded, since it would create a cycle; if not, it is added to the MST and the components it connects are merged).
</text>

<text>
Since each connected component is always a tree, we do not have to check for cycles
</text>

<text>
EFFICIENCY:
Sorting edges <eqn>O(m \log m)</eqn> + <eqn>m</eqn> iterations / testing connectivity by BFS/DFS <eqn>O(n)</eqn> = <eqn>O(mn)</eqn>
</text>

<text>
PROOF? by contradiction
</text>

</document>

</document>


<document>
<tag>graph-spanning-trees-sequential</tag>
<title>Sequential implementation of MST algorithms</title>

<text>
Naive implementation:
<eqn>O(mn)</eqn> for Kruskal's algorithm 
<eqn>O(mn)</eqn> for Prim's algorithm 
(i.e. <eqn>O(n^3)</eqn> for dense graphs, <eqn>O(n^2)</eqn> for sparse graphs)
</text>

<text>
Prim's algorithm:
1) Maintaining a boolean flag to indicate whether a vertex is already in the tree or not, we can test whether the current edge links to a non-tree vertex in constant time, leading to a <eqn>O(n^2)</eqn> algorithm (both for dense and sparse graphs).
2) Optimization: <eqn>O(n \log k)</eqn> on graphs that have only <eqn>k</eqn> different edge costs.
3) The use of priority queues can lead to a more efficient implementation, by making it faster to find the minimum cost edge at each iteration.
- <eqn>O(m \log n)</eqn> using standard binary heaps.
- <eqn>O(m + n \log n)</eqn> using Fibonacci <cite>Fredman and Tarjan 1987</cite> or pairing heaps <cite>Stasko and Vitter 1987</cite>.
</text>

<text>
Kruskal's algorithm:
1) Union-find data structure for implementing the component test in <eqn>O(\log n)</eqn> = <eqn>O(m \log m)</eqn> time (i.e. needed to sort the edges).
+ Typically faster than Prim's algorithm for sparse graphs
</text>


<text>
Union-find data structure for representing set partitions (a partitioning of elements, say nodes, intro a collection of disjoint subsets):
1) Union: Component merging in <eqn>O(\log n)</eqn>
2) Find: Same component check in <eqn>O(\log n)</eqn>

Solution: Represent each component as a backwards tree, with pointers from a node to its parent (choosing how to merge in order to limit the height of the trees and reduce the effect of unbalanced trees: the smaller tree will always be a subtree of the larger tree; why? height of the nodes in the larger tree stay the same, heights in the smaller tree are increased by one)

Result: We must double the number of nodes to increase the height of the resulting tree, hence the <eqn>O(\log n)</eqn> result
</text>

<text>
Set data structures to represent unordered collections of objects. Representing subsets to efficiently test element \in subset, compute union/intersection of subsets, or insert/delete subset members,

1) Bit vectors: Space efficient, insertion/deletion by toggling single bits, union/intersection by or-ing/and-ing bit vectors.

2) Bloom filters <cite>Broder and Mitzenmacher 2005</cite> use hashing.

3) Dictionaries, which can be viewed as sets (no duplicate keys).

4) Standard collections, which can be viewed as multisets (may have multiple occurrences of the same element).

Set partitions: Pairwise-disjoint subsets, when each element is exactly in one subset...

1) Collection of sets (as a generalized bit vector, a collection of collections, or a dictionary with a subset sttribute): Costly union/intersection operations.

2) Union-find data structure *
++ Union-find can be done even faster: 
- Optimization: Path compression. Retraversing the path on each find operation and explicitly pointing all nodes to the root leads to almost constant-height trees.
- Limitation: Does not support breaking up unions
- Upper bound on m union-find operations on an n-element set: <eqn>O(m \alpha(m, n))</eqn>, where alpha is the inverse Ackermann function, which grows notoriously slowly, and leads to an almost-linear performance. <cite>Tarjan 1979</cite>.


NOTE:
Efficient implementation of basic graph algorithms led to the discovery of several data structures, such as the "union-find"
structure <cite>Tarjan 1979</cite> and Fibonacci heaps <cite>Fredman and Tarjan 1987</cite>. These data structures have been used to speed up many other algorithms.

</text>

<text>
A combination of Boruvka's algorhtm <cite>Boruvka 1926</cite> with Prim's algorithm <cite>Prim 1957</cite> yields an O(m log log n) algorithm. Running log log n iterations of Boruvka's algorhtm yields at most n/log n trees. Then create a graph G' with a vertex for each tree and an edge whose weight corresponds to the lightest edge between T_i and T_j. The MST of G' combined with the edges selected by  Boruvka's algorhtm yields the MST of G. Since Prim's algorithm will take O(n+m) time on this n/log n vertex, m edge graph (if implemented using Fibonacci or pairing heaps). <cite>Skiena 2008</cite>. Even tighter bounds, e.g. O(n \alpha(m,n)), can be achieved <cite>Karger et al. 1995</cite> <cite>Chazelle 2000</cite> <cite>Pettie and Ramachandran 2002</cite> <cite>Pettie and Ramachandran 2008</cite>.
</text>

<text>
Dynamic graph algorithms are incremental algorithms that maintain graph invariants (such as MSTs) under edge insertion and deletion operations. <cite>Holm et al. 2001</cite> describes and efficient algorithm to maintain MSTs (and several other invariant) in amortized polylogarithmic time per update.
</text>


</document>

<document>
<tag>graph-spanning-trees-parallel</tag>
<title>Parallel MST algorithms</title>

<text>
Parallel Prim's algorithm
- for dense graphs @ Section 10.2 <cite>Grama et al. 2003</cite>
- for sparse graphs @ Section 10.7.2 <cite>Grama et al. 2003</cite>
+ Bibliographic notes
</text>

</document>

<document>
<tag>graph-spanning-trees-distributed</tag>
<title>Distributed MST algorithms</title>

<text>
Distributed MST algorithms @ MIT 6.852: Distributed Algorithms - Lecture 4 (synchronous) + Lecture 9 (asynchronous) / Section 4.4 + Section 15.5 <cite>Lynch 1997</cite>
</text>

</document>

</document>




<!-- Shortest paths -->

<document>
<tag>graph-shortest-paths</tag>
<title>Shortest paths</title>


<text>
When graphs are weighted, i.e. each edge in the graph is associated to a numerical value, computing their shortest paths is more complicated than performing breadth-first traversals (recall that the BFS tree leads to minimum-number-of-links paths)...
</text>

<text>
Shortest paths in directed acyclic graphs can also be found in linear time: Perform a topological sorting and then process the vertices
from left to right. Observe that d(s, j) = min_(x,i) d(s, i) + w(i, j) since we already know the shortest path d(s, i) for all vertices to the left of j. The same algorithm (replacing min with max) also suffices to find the longest path in a DAG, which is useful in scheduling applications.
</text>


<note>
<title>The Strategy design pattern</title>

<text>
++ The Strategy design pattern <cite>Gamma et al. 1994</cite>
</text>
</note>


<text>
@ MIT Graphs - Shortest paths
</text>


<!-- Shortest paths: Single source -->

<document>
<tag>graph-shortest-paths-dijkstra</tag>
<title>Single source shortest paths</title>

<text>
Finding the shortest path from s to t in G.
Actually, finding the shortest path from a source node <eqn>s</eqn> to every other node in the network requires the same amount of work...
</text>

<text>
APPLICATIONS: Finding directions (note: hierarchical A*), transportation and communications networks, distinguishing
among homophones iin speech recognition systems, graph visualization algorithms...

Approximate shortest paths in road networks, for instance, typically employ a hierarchical variation of A* <cite>Goldberg et al. 2006</cite> <cite>Goldberg et al. 2007</cite>. Such problem differ from the shortest path problems in this Section because
(1) preprocessing costs can be amortized over many point-to-point queries, 
(2) the backbone of high-speed, long-distance highways can reduce the path problem to identifying the best place to get on and off this backbone, and (3) approximate or heuristic solutions suffice in practice
</text>



<text>
IDEA: Shortest path from s to t goes through x, then that path contains the shortest path from s to x
</text>

<text>
Similar to Prim's algorithm... each iteration determines the shortest path from s to a new vertex x by minimizing d(s,v)+w(v,x)

In Prim's algorithm, we just take into account the weighte of the edge we add to the MST; in Dijkstra's algorithm, we consider the cost of the whole path from s to x (i.e. the final edge weight plus the distance from s to the tree vertex that is adjacent to the new edge). Dijkstra's algorithm returns a shortest-path spanning tree from s to all the other nodes in the network.
</text>

<text>
http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm
Dijkstra's algorithm <cite>Dijkstra 1959</cite>: 

computes the shortest path from a given starting vertex x to all n . 1
other vertices. In each iteration, it identifies a new vertex v for which the shortest
path from x to v is known. We maintain a set of vertices S to which we currently
know the shortest path from v, and this set grows by one vertex in each iteration.
In each iteration, we identify the edge (u,v) so that
dist(x, u) + weight(u, v) = min_(w,y) { dist(x, w) + weight(w,y) }

This edge (u, v) gets added to a shortest path tree, whose root is x and describes
all the shortest paths from x.

If we just need to know the shortest
path from x to y, terminate the algorithm as soon as y enters S.
</text>

<text>
EFFICIENCY:
- <eqn>O(n^2)</eqn> using a naive implementation with arrays
- Using binary heaps: <eqn>O(m \log n)</eqn>
- Fastest implementation: <eqn>O(m + n \log n)</eqn> amortized time using Fibonacci heaps <cite>Fredman and Tarjan 1987</cite>
</text>

<text>
Related problem: Single-destination shortest path problem for directed graphs.
</text>

<text>
CORRECTNESS: Only on graphs without negative edges!!!

Dijkstra’s algorithm assumes that all edges have positive cost. 
- Adding a fixed amount of weight to make each edge positive does
not solve the problem. Dijkstra’s algorithm will then favor paths using a fewer
number of edges,

For graphs with edges of negative weight,
you must use the more general, but less efficient, Bellman-Ford algorithm.

NOTE: negative cost cycles =the shortest x to y path in such a graph is not defined because we can detour
from x to the negative cost cycle and repeatedly loop around it, making the total cost arbitrarily small.


</text>

<text>
In a geometric setting, more efficient geometric algorithms compute the shortest path directly from the arrangement of obstacles: motion planning algorithms <cite>Latombe 1991</cite> <cite>LaValle 2006</cite>
</text>


<text>
Parallelization of Dijkstra's algorithm
- for dense graphs @ Section 10.3 <cite>Grama et al. 2003</cite>
+ Bibliographic notes
</text>

</document>



<!-- Shortest paths: All pairs -->

<document>
<tag>graph-shortest-paths-all-pairs</tag>
<title>All-pairs shortest paths</title>


<text>
Some problems requite computing the shortest paths between all pairs of nodes, e.g. the central node (i.e. the node that minimizes the longest or average distance to every other node) or the network diameter (i.e. the longest shortest-path distance between two nodes in the network). 
</text>

<text>
EXAMPLE: 
- bounding the amount of time needed to deliver a packet through a network
</text>

<text>
+ Best on adjacency matrix (anyway we will need to store <eqn>n \times n</eqn> distances), i.e. one of the rare cases where algorithms on adjacency matrices work better
</text>


<text>
A first solution: Repeating Dijkstra's algorithm for each node
</text>

<!-- Floyd-Warshall -->

<text>
http://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm
Floyd-Warshall algorithm: <cite>Floyd 1962</cite>
often referred to as Floyd's algorithm, albeit the same algorithm was published by Bernard Roy in 1959 <cite>Roy 1959</cite> and by Stephen Warshall in 1962 <cite>Warshall 1962</cite>


<eqn>O(n^3)</eqn>

- Define the length of the shortest path from i to j using only the k first vertices as possible intermediate nodes: <eqn>W[i,j]^k</eqn>
- Initial shortest-path matrix = adjacency matrix
- <eqn>W[i,j]^k = min \{ W[i,j]^{k-1}, W[i,k]^{k-1} + W[k,j]^{k-1} \}</eqn>
- Compact implementation: 3 nested loops
- Ancillary matrix to reconstruct the paths
</text>

<code>
D0 = M
for k = 1 to n do
    for i = 1 to n do
        for j = 1 to n do
            Dk(i,j) = min{ Dk-1(ij), Dk-1(ik) + Dk-1(kj) }
return Dn


  for k := 1 to n
        for i := 1 to n
           for j := 1 to n
              if path[i][k] + path[k][j] &lt; path[i][j] then
                 path[i][j] := path[i][k]+path[k][j];
                 next[i][j] := k;

</code>

<text>
Transitive closure of directed graphs (Warshall's algorithm). In Warshall's original formulation of the algorithm, the graph is unweighted and represented by a Boolean adjacency matrix. Then the addition operation is replaced by logical conjunction (AND) and the minimum operation by logical disjunction (OR).
</text>

<!-- Bellman-Ford -->

<text>
http://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm
Bellman-Ford algorithm: <eqn>O(nm)</eqn> time, <eqn>O(n)</eqn> space

Bellman-Ford algorithm <cite>Bellman 1958</cite> <cite>Ford and Fulkerson 1962</cite>

+ Negative weights (arise when we reduce other problems to shortest-paths problems)
</text>

<code>
   // Step 1: initialize graph
   for each vertex v in vertices:
       if v is source then v.distance := 0
       else v.distance := infinity
       v.predecessor := null

   // Step 2: relax edges repeatedly
   for i from 1 to size(vertices)-1:
       for each edge uv in edges: // uv is the edge from u to v
           u := uv.source
           v := uv.destination
           if u.distance + uv.weight &lt; v.distance:
               v.distance := u.distance + uv.weight
               v.predecessor := u

   // Step 3: check for negative-weight cycles
   for each edge uv in edges:
       u := uv.source
       v := uv.destination
       if u.distance + uv.weight &lt; v.distance:
           error "Graph contains a negative-weight cycle"
</code>

<!-- Johnson -->

<text>
http://en.wikipedia.org/wiki/Johnson%27s_algorithm
Johnson's algorithm <cite>Johnson 1977</cite> 

1
First, a new node q is added to the graph, connected by zero-weight edges to each of the other nodes.

2
Second, the Bellman-Ford algorithm is used, starting from the new vertex q, to find for each vertex v the least weight h(v) of a path from q to v. If this step detects a negative cycle, the algorithm is terminated.

3
Next the edges of the original graph are reweighted using the values computed by the Bellman-Ford algorithm: an edge from u to v, having length w(u,v), is given the new length w(u,v) + h(u) - h(v).

4
Finally, q is removed, and Dijkstra's algorithm is used to find the shortest paths from each node s to every other vertex in the reweighted graph.


<eqn>O(n^2 \log n + nm)</eqn> using <eqn>O(nm)</eqn> time for the Bellman-Ford stage of the algorithm, and <eqn>O(n \log n + m)</eqn> for each of <eqn>n</eqn> instantiations of Dijkstra's algorithm. Faster than the Floyd-Warshal algorithm for sparse graphs.


It allows some of the edge weights to be negative
</text>


<!-- Applications -->

<text>
All-pairs shortest paths algorithms can be used to find the shortest cycle in a graph: its girth. Floyd's algorithm can be used to compute d_ii, which is the shortest way to get from vertex i to itself (i.e. the shortest cycle that goes through i). If you want the shortest simple cycle, the easiest
approach is to compute the lengths of the shortest paths from i to all other vertices, and then explicitly check whether there is an acceptable edge from each vertex back to i.
</text>


<text>
- Reachability questions (can I get to x from y?)

RELATED PROBLEMS 
+ Transitive closure, construct a graph G' = (V,E') with edge (i, j) in E' iff there is a directed path from i to j in G. 
+ Transitive reduction (also known as minimum equivalent digraph), the inverse operation of transitive closure: reducing the number of edges while maintaining identical reachability properties, i.e. construct a small graph G'' = (V,E'') with a directed path from i to j in G'' iff there is a directed path from i to j in G

- transitive closure 

1) using graph traversal algorithms O(n(n+m)), 
2) an all-pairs shortest path algorithms returns them all in a whole batch, albeit less efficiently, O(n^3) using Warshall's algorithm.
3) using matrix multiplication: O(log n) multiplications using a divide-and-conquer algorithm

- transitive reduction

Approximate solution: A linear-time, quick-and-dirty transitive reduction algorithm identifies the
strongly connected components of G, replaces each by a simple directed cycle,
and adds these edges to those bridging the different components.

+ Transitive reduction also arises in graph drawing, where it is important to eliminate as many unnecessary edges as possible to reduce the visual clutter
</text>


<text>
- pattern recognition problems, e.g. Viterbi algorithm, a dynamic programming algorithm that basically solves a shortest path problem on a DAG.
</text>


<!-- Parallelization -->

<text>
Parallelization of Floyd's algorithm 
- for dense graphs @ Section 10.4 <cite>Grama et al. 2003</cite>
+ Bibliographic notes
</text>


<text>
Wikipedia: A distributed variant of the Bellman-Ford algorithm is used in distance-vector routing protocols, for example the Routing Information Protocol (RIP). The algorithm is distributed because it involves a number of nodes (routers) within an Autonomous system, a collection of IP networks typically owned by an ISP. It consists of the following steps:
- Each node calculates the distances between itself and all other nodes within the AS and stores this information as a table.
- Each node sends its table to all neighboring nodes.
- When a node receives distance tables from its neighbors, it calculates the shortest routes to all other nodes and updates its own table to reflect any changes.
</text>


<text>
Distributed shortest paths @ MIT 6.852: Lectures 2-3 (synchronous) + Lectures 8-9 (asynchronous)  / Section 4.3 + Section 15.4 <cite>Lynch 1997</cite>
</text>

</document>

</document>




<!-- Network flows -->

<document>
<tag>graph-flow</tag>
<title>Network flows: Maximum flow and minimum cuts</title>

<text>
Interpreting weighted graphs as distribution networks where weights represent capacities
</text>

<text>
two primary classes of network flow problems:
maximum flow and minimum-cost flow:

- "The maximum-flow problem is that of finding a maximum flow of a
single commodity from a source vertex to a sink vertex that satisfies
capacity constraints on the edges and flow conservation constraints at the
vertices. "

- Associating costs with edges yields the minimum-cost flow problem. 
where we also have a targeted amount of flow f we want to send from s to t at minimum total cost.


Polynomial-time algorithms to solve maximum flow and minimum-cost flows are known.
"a surprising variety of linear programming
problems arising in practice can be modeled as network-flow problems,
and network-flow algorithms can solve these problems much faster than
general-purpose linear programming methods"

APPLICATIONS: cost-effective way to ship goods between a set of factories and a set of stores
defines a network-flow problem, as do many resource-allocation problems in communications
networks
</text>

<text>
Network flow problem: The maximum amount of flow that can be sent from s to t in a given graph.
</text>

<text>
The two major techniques for solving flow problems are the augmenting-path method and the preflow-push method.

- Augmenting paths: "These algorithms repeatedly find a path of positive
capacity from source to sink and add it to the flow. It can be shown that
the flow through a network is optimal if and only if it contains no augmenting
path. Since each augmentation adds something to the flow, we eventually
find the maximum. The difference between network-flow algorithms is in how
they select the augmenting path."

- "Preflow-push methods - These algorithms push flows from one vertex to another,
initially ignoring the constraint that in-flow must equal out-flow at each
vertex. Preflow-push methods prove faster than augmenting-path methods,
essentially because multiple paths can be augmented simultaneously. These
algorithms are the method of choice in practice"
</text>

<text>
IDEA: <b>Augmenting paths</b>
i.e. iteratively find a path of positive capacity from s to t and add it to the flow
- As in a traffic jam, the flow is limited by the most congested point.
- A flow through a network is optimal if and only if it contains no augmenting path.
</text>

<text>
MECHANISM: Directed <b>residual flow graph</b> R(G,f), where G is the graph and f the current flow
i.e. For each edge (i,j) in G with capacity c(i,j) and flow f(i,j), R can contain two edges
- an edge (i,j) with weight c(i,j)-f(i,j) if c(i,j)>f(i,j)
- an edge (j,i) with weight f(i,j)-c(i,j) if f(i,j)>0

An edge (i,j) in the residual flow graph indicates that flow can still be pushed from i to j (its weight determines the amount of its unused capacity). 

A path from s to t in the residual graph (and the minimum edge weight on this path) defines an augmenting path (and the amount of flow that can be pushed).
</text>

<text>
- For each edge in the residual graph: keep track of the amount of flow currently going through the edge as well as any unused capacity (the residual capacity)
- Initialization: create directed flow edges (i,j) and (j,i); initial flows set to 0; initial residual flows set to their capacity in the original graph.
</text>

<text>
ITERATIVE ALGORITHM
Ford-Fulkerson algorithm <cite>Ford and Fulkerson 1956</cite>
http://en.wikipedia.org/wiki/Ford%E2%80%93Fulkerson_algorithm

- Use the augmented path to update the residual graph (transfer the maximum possible volume from the residual capacity = the path edge with the smallest amount of residual capacity; NOTE: adding positive flow to (i,j) reduces the residual capacity of (i,j) but also increases the residual capacity of (j,i), i.e. augmenting a path requires updates to both forward and reverse links along the augmenting path ).
- Terminate when no such augmenting path exists.

As described, even though the algorithm converges to the optimal solution, it may take arbitrarily long (each augmenting path might add only a little to the flow).
</text>

<text>
IMPLEMENTATION (Edmonds-Karp algorithm <cite>Edmonds and Karp 1972</cite>):
http://en.wikipedia.org/wiki/Edmonds%E2%80%93Karp_algorithm
The algorithm is identical to the Ford-Fulkerson algorithm, except that the search order when finding the augmenting path is defined. The path found must be a shortest path that has available capacity

- BFS to look for any path from the source to the sink that increases the total flow (only consider edges with residual capacity, i.e. positive residual flow)
</text>

<text>

 <cite>Edmonds and Karp 1972</cite> proved that selecting the shortest unweighted augmenting path (a.k.a. shortest geodesic path) guarantees that <eqn>O(n^3)</eqn> augmentations suffice for optimization.

<eqn>O(nm^2)</eqn> running time: each augmenting path can be found in <eqn>O(m)</eqn> time, every time at least one of the <eqn>m</eqn> edges becomes saturated, the distance from the saturated edge to the source along the augmenting path must be longer than last time it was saturated, and that length is at most <eqn>n</eqn>.

Another property of this algorithm is that the length of the shortest augmenting path increases monotonically.
</text>

<text>
The standard reference for network flow algorithms and their history is <cite>Ahuja et al. 1993</cite>. 
The fastest known general network flow algorithm runs in <eqn>O(nm \log(n^2/m))</eqn> time <cite>Goldberg and Tarjan 1988</cite>.
</text>


<text>
++ @ MIT Graphs - Network flow
</text>


<text>
VARIANTS

- multiple sources and/or sinks? - No problem. We can handle
this by modifying the network to create a vertex to serve as a super-source
that feeds all the sources, and a super-sink that drains all the sinks.

- if all arc capacities are identical, either 0 or 1? - Faster algorithms
exist for 0-1 network flows.

- if all my edge costs are identical? - Use the simpler and faster algorithms
for solving maximum flow as opposed to minimum-cost flow. Max-flow
without edge costs arises in many applications, including edge/vertex connectivity
and bipartite matching.

- What if I have multiple types of material moving through the network? - In
a telecommunications network, every message has a given source and destination.
Each destination needs to receive exactly those calls sent to it, not
an equal amount of communication from arbitrary places. This can be modeled
as a multicommodity flow problem, where each call defines a different
commodity and we seek to satisfy all demands without exceeding the total
capacity of any edge. Linear programming will suffice for multicommodity flow if fractional flows
are permitted. Unfortunately, integral multicommodity flow is NP-complete,
even for only two commodities.
</text>

<document>
<tag>graph-minimum-cuts</tag>
<title>Minimum cuts</title>

<text>
A set of edges whose deletion separates s from t is called an s-t cut. Since no s-t flow can exceed the weight of the minimum s-t cut, finding the maximum flow is equivalent to discovering the minimum cut. In fact, the maximum s-t flow equals the weight of the minimum s-t cut. This maximum-flow, minimum-cut theorem is due to <cite>Ford and Fulkerson 1962</cite>.
</text>

<text>
Therefore, flow algorithms can be used to solve edge and vertex connectivity problems that are employed to answer questions such as
What is the smallest subset of vertices (or edges) whose deletion will disconnect G? Or which will separate s from t? Such problem often appear when stuydying network reliability.
</text>

<text>
"The edge (vertex) connectivity of a graph G is the smallest number of edge
(vertex) deletions sufficient to disconnect G. There is a close relationship between
the two quantities. The vertex connectivity is always less than or equal to the
edge connectivity, since deleting one vertex from each edge in a cut set succeeds
in disconnecting the graph. But smaller vertex subsets may be possible. The minimum
vertex degree is an upper bound for both edge and vertex connectivity, since
deleting all its neighbors (or cutting the edges to all its neighbors) disconnects the
graph into one big and one single-vertex component"
</text>

<text>
- Simple depth-first or breadth-first traversals suffice to identify all connected components in linear time, including strongly-connected and weakly-connected components in directed graphs.

- Weak links: We say that G is biconnected if no single vertex deletion is sufficient to disconnect G. Any vertex that is such a
weak point is called an articulation vertex. A bridge is the analogous concept for edges, meaning a single edge whose deletion disconnects the graph. Linear-time algorithms for identifying articulation vertices and bridges also exist.

- Finding the smallest cut-set that separates a given pair of vertices can be solved using network flow algorithms. 

- Vertex connectivity is characterized by Menger’s theorem, which states that a graph is k-connected if and only if every pair of vertices is joined by at least k vertex-disjoint paths. Network flow can again be used to perform this calculation, since a flow of k between a pair of vertices implies k edge-disjoint paths. we construct a graph G' such that any set of edge-disjoint paths in G' corresponds to vertex-disjoint paths in G. This is done by replacing each vertex vi of G with two vertices vi,1 and vi,2, such that edge (vi,1, vi,2)  G' for all vi G, and by replacing every edge (vi, x) ¸ G by edges (vi,j, xk), j = k {0, 1} in G'. Thus two edge-disjoint paths in G' correspond to each vertex-disjoint path
in G.

A non-flow-based algorithm for edge k-connectivity in O(kn2) is due to Matula <cite>Matula 1987</cite>.
Faster k-connectivity algorithms are known for certain small values of k. All three-connected components of a graph can be generated in linear time <cite>Hopcroft and Tarjan 1973b</cite>, while O(n2) suffices to test 4-connectivity <cite>Kanevsky and Ramachandran 1991</cite>

- Graph partitioning, i.e. splitting the graph into smaller components with specific properties, is, however, an NP-complete problem. Good heuristic techniques exist for this problem <cite>Karypis and Kumar 1999</cite>, which has important applications (e.g. in VLSI circuit layout).
</text>

</document>

<document>
<tag>graph-matching</tag>
<title>Matching</title>

<text>
A matching in a graph <eqn>G=(V,E)</eqn> is a subset of edges <eqn>E' \subset E</eqn> such that no two edges in <eqn>E'</eqn> share a vertex.
</text>

<text>
PROBLEM
Find the largest set of edges E' from E such that each
vertex in V is incident to at most one edge of E'.
</text>

<text>
APPLICATION: Assignment problems, with/without weights (maximum cardinality matching/ maximum weight matching).
- Assignment problems (jobs to be done and people who can do them, male and female customers of a dating web site...)
</text>

<text>
Matching theory and algorithms <cite>Lovasz and Plummer 1986</cite>
</text>

<text>
++ CS 
      A matching in a graph is a subset of edges
that have no common incident vertices. It captures the notion of pairing
off compatible vertices. The maximum matching problem is that of finding
a matching of maximum cardinality. Hall and Tutte gave necessary and
sufficient conditions for a graph to have a perfect matching - a matching
in which all vertices are matched. Edmonds gave a polynomial-time algorithm
for finding a maximum matching. Edges may be assigned weights that denote
the advantage of including them in a matching. Maximum-weight bipartite
matching, also known as the assignment problem, arises in many applications.
Algorithms for maximum cardinality and maximum-weight matchings have been
well studied.  See <cite>Ahuja et al. 1993</cite>,<cite>Bondy and Murty 1976</cite>, 
, and <cite>Papadimitriou and Steiglitz 1982</cite>
for more details.
</text>

<text>
Standard algorithms for bipartite matching based on network flows:
</text>

<text>
Remember that a graph is bipartite (or two-colorable) if its vertices can be divided into two sets, L and R, so that every edge in the graph connects one vertex from the first set to a different vertex on the second set.
</text>
<text>
The largest possible bipartite matching can be found using network flow. Just add a source node s connected to every vertex in L with capacity 1, add a sink node t connected to every node in R, and assign each edge in the bipartite graph a capacity of 1. The maximum flow from s to t determines the largest matching in the bipartite graph.
</text>

<text>
The best algorithm for maximum bipartite matching, based on Hopcroft and Karp's algorithm <cite>Hopcroft and Karp 1973</cite>, repeatedly finds the shortest augmenting paths instead of using network flow, and runs in O(\sqrt{n}m) <cite>Micali and Vazirani 1980</cite>.
</text>

<text>
Classic algorithm: Hungarian algorithm for bipartite matching <cite>Kuhn 1955</cite>. The Hungarian algorithm runs in O(n(m + n log n)) time.
</text>

<text>
Matching based on preferences (e.g. matching medical residents to hospitals or students to university courses): O(n^2) time using Galey and Shapley's algorithm <cite>Gale and Shapley 1962</cite>.
</text>


<text>
@ MIT Graphs - Matching
</text>


</document>

</document>





<!-- Graph isomorphism -->

<document>
<tag>graph-isomorphism</tag>
<title>Graph isomorphism</title>


<text>
@ MIT Interactive proofs and zero knowledge
</text>



<text>
Practical algorithms for graph isomorphism:
- <cite>McKay 1981</cite>
- <cite>Corneil and Gotlieb 1970</cite>
- <cite>Schmidt and Druffel 1976</cite>
- <cite>Ullman 1976</cite>
</text>


<text>
++ http://dabacon.org/pontiff/?p=4148
</text>

<text>
While they seem to perform well on random graphs, a major drawback of these algorithms is their exponential time performance in the worst case.
</text>

<text>
Best known algorithm: Luks (1983) has run time <eqn>2^O(\sqrt{n \log n})</eqn> for graphs with n vertices.
</text>

<text>
In <q>Canonical labeling of graph</q> <cite>Babai and Luks 1983</cite>:  subexponential time algorithm for a graph with n vertices.
</text>

<text>
Isomorphism for many special classes of graphs can be solved in polynomial time, and in practice graph isomorphism can often be solved efficiently:
</text>

<list>

<item>Trees <cite>Kelly 1957</cite> @ <cite>Aho et al. 1974</cite></item>


<item>Planar graphs <cite>Hopcroft and Wong 1974</cite></item>

<item>Maximal outerplanar graphs <cite>Beyer et al. 1979</cite></item>

<item>Interval graphs <cite>Lueker and Booth 1979</cite></item>

<item>Permutation graphs <cite>Colbourn 1981</cite></item>

<item>Trivalent graphs <cite>Galil et al. 1987</cite></item>

<item>Partial k-trees <cite>Bodlaender 1990</cite></item>


<item>Bounded-parameter graphs: 
- Graphs of bounded genus <cite>Miller 1980</cite> <cite>Filotti and Mayer 1980</cite>

- graphs of bounded degree <cite>Luks 1982</cite>

- graphs with bounded eigenvalue multiplicity <cite>Babai et al. 1982</cite>

- k-Contractible graphs (a generalization of bounded degree and bounded genus) <cite>Miller 1983a</cite> <cite>Miller 1983b</cite> 

- Color-preserving isomorphism of colored graphs with bounded color multiplicity (i.e., at most k vertices have the same color for a fixed k)
<cite>Luks 1986</cite>
</item>


</list>

<text>
SOFTWARE: ell now that you’ve gone and starting learning about some computational complexity in relationship to graph isomorphism, it’s probably a good time to stop and look at actual practical algorithms for graph isomorphism.  The king of the hill here, as far as I know, is the program nauty (No AUTomorphism, Yes?) by Brendan D. McKay.  http://cs.anu.edu.au/people/bdm/nauty/ <cite>McKay 1981</cite>
</text>

</document>



<!-- Algorithm catalog -->

<document>
<tag>graph-algorithm-catalog</tag>
<title>A catalog of graph algorithms</title>

<text>
Graphs of order n (nodes) and size m (links)...
</text>

<document>
<tag>graph-algorithm-catalog-linear</tag>
<title>Linear-time algorithms</title>


<text>
The following problems can be solved with algorithms that run in linear time (i.e. <eqn>O(n+m)</eqn>):
</text>

<list>

<item>Graph traversal, either BFS, DFS, or best-first search (as in A*).</item>

<item>Shortest paths in unweighted graphs (based on BFS): Finding the shortest paths and also counting the number of different shortest paths.</item>

<item>Identifying connected and strongly-connected components (based on graph traversal).</item>

<item>Bipartite testing (based on graph traversal).</item>

<item>Cycle detection (based on DFS).</item>

<item>Identification of articulation points, i.e. graph biconnectedness (based on DFS).</item>

<item>Identification of bridges, i.e. edge-biconnectedness (based on DFS).</item>

<item>Topological sorting (based on DFS).</item>

<item>Single-source shortest paths on DAGs, even with negative weights.</item>

<item>Shortest paths, longest paths, and hamiltonian paths on DAGs (topological sorting).</item>

<item>Maximum matching in a tree.</item>

<item>Eulerian cycle: the shortest tour visiting each edge of G 
- Checking is an Eulerian cycle exists: verify that the graph is connected using DFS or BFS, and then count the number of odd-degree vertices.
- Constructing the cycle in linear time: Use DFS to find an arbitrary cycle in the graph. Delete this cycle and
repeat until the entire set of edges has been partitioned into a set of edge-disjoint cycles. These cycles will have common vertices (since the graph is connected), and so can be spliced together as an "eight" at a shared vertex. By so splicing all the extracted cycles together, we construct a single circuit containing all of the edges.
</item>


</list>

</document>

<document>
<tag>graph-algorithm-catalog-almost-linear</tag>
<title>Almost linear-time algorithms</title>


<text>
The following algorithms are almost linear, i.e. <eqn>O(n \log n)</eqn>:
</text>

<list>

<item>Minimum spanning trees (using either Prim's or Kruskal's algorithm).</item>

<item>Single-source shortest paths (using Dijkstra's algorithm), only for positive weights.</item>

</list>


<text>
NOTE: When working with large data sets, only linear (<eqn>O(n)</eqn>) or near linear (e.g. <eqn>O(n \log n)</eqn>) are likely to be suitable.
</text>


</document>

<document>
<tag>graph-algorithm-catalog-polynomial</tag>
<title>Polynomial-time algorithms</title>

<text>
The following algorithms require polynomial time:
</text>

<list>

<item>All-pairs shortest paths (e.g. Floyd's algorithm).</item>

<item>Transitive closure (i.e. all-pairs reachability) and reduction.</item>

<item>Finding a directed cycle of minimum total weight, <eqn>O(n^3)</eqn>.</item>

<item>Maximum network flow.</item>

<item>Minimum cuts.</item>

<item>Maximum bipartite matching.</item>

<item>Minimum vertex cover problem and maximum independent set problems for bipartite graphs (since the maximum bipartite matching is equal in size to the minimum vertex cover in bipartite graphs.</item>

<item>Minimum-size edge cover (i.e. a set of edges, of minimum size, such that each vertex in the graph is incident to at least one edge from the set).</item>

<item>Chinese postman problem (a generalization of the Eulerian circuit) introduced by <cite>Kwan 1962</cite>, hence its name: the shortest tour visiting each edge of G at least once. This minimum
cycle will never visit any edge more than twice, so good tours exist for any road network. The optimal postman tour can be constructed by adding the appropriate edges to the graph G to make it Eulerian. Specifically, we find the shortest path between each pair of odd-degree vertices in G. Adding a path between two odd-degree vertices in G turns both of them to even-degree, moving G closer to becoming an Eulerian graph. Finding the best set of shortest paths to add to G reduces to identifying a minimum-weight perfect matching in a bipartite graph G' <cite>Edmonds and Johnson 1973</cite>. For undirected graphs, the vertices of G' correspond the odd-degree vertices of G, with the weight of edge (i, j) defined to be the length of the shortest path from i to j in G. For directed graphs, the vertices of G' correspond to the degree-imbalanced vertices from G, with the bonus that all edges in G' go from out-degree deficient vertices to in-degree deficient ones. Thus, bipartite matching algorithms suffice when G is directed. Once the graph is Eulerian, the actual cycle can be extracted in linear time.
</item>

</list>

</document>

<document>
<tag>graph-algorithm-catalog-hard</tag>
<title>Hard problems</title>

<text>
++ CS 
        Many optimization problems such as the well known traveling-salesman
problem are NP-hard. Polynomial-time algorithms are not known for these
problems, and many researchers believe that they do not exist. Hence
heuristics that produce suboptimal solutions have been a subject of much
research, especially in the last decade. Other problems that arise in
different contexts are the Steiner tree problem and the graph-coloring
problem. In spite of being NP-hard, large instances of the Steiner-tree
problem and the traveling-salesman problem can be solved in practice,
that is, solutions that are very close to optimal can be obtained. On the
other hand, even small instances of the graph-coloring problem are hard to
solve. Formal evidence for the hardness of the approximability of problems
such as clique, independent set, and coloring was obtained recently. The
complexity of graph isomorphism is a major open problem. Many other problems
on graphs are known to be NP-hard.
</text>

<note>
<text>
NP (non-deterministic polynomial) is the set of all decision problems whose solutions can be verified in polynomial time (or, alternatively, the set of decision problems that can be solved in polynomial time on a nondeterministic Turing machine). That is, even though any given solution to such a problem can be verified quickly (in polynomial time), there is no known efficient way to discover a solution to the problem.
</text>

<text>
In computational complexity theory, NP-hard problems are, at least as hard as the hardest problems in NP (i.e. any NP problem can be converted into L by a transformation of the inputs in polynomial time or, in theoretical parlance, every problem in NP is reducible to L in polynomial time). A problem is NP-complete when it is in NP and also NP-hard. Hence, NP-complete is a subset of NP, which is itself a subset of NP-hard.
</text>

<text>
This the P=NP? question is not resolved, and hence we do not know whether solution verification is really easier than solution discovery for NP problems, we could say that NP problems are <q>not-necessarily polynomial time</q> <cite>Skiena 2008</cite>.
</text>

<text>
More general classes exist, such as PSPACE: the set of all decision problems which can be solved by a Turing machine using a polynomial amount of space. Formally, <eqn>P \subseteq NP \subseteq PSPACE</eqn>. Some graph problems are known to be PSPACE-complete, such as the Canadian Traveler Problem (CTP) <cite>Papadimitriou and Yannakakis 1989</cite> <cite>Papadimitriou and Yannakakis 1991</cite>, a generalization of the shortest path problem to graphs that are partially observable (i.e. the graph is revealed while it is being explored) that has many practical applications.
</text>

</note>


<text>
The following problem occupies a singular position within the complexity hierarchy, since it is not known whether the problem has a fast algorithm that solves it ot is NP-complete:
</text>

<list>

<item>Isomorphism testing, i.e. checking whether the topological structure of two graphs are identical (typically solved using backtracking)</item>

</list>


<text>
The following algorithms are known to be NP-hard (most of them are, in fact, NP-complete): 
- Permutations/ORDERING, e.g. TSP, bandwidth, isomorphism
- Selection: COVERING/PARTITIONING, SUBGRAPHS
- Network design
- Partitioning
</text>

<list>

<item>Hamiltonian circuit on unweighted graphs (both directed and undirected), remains NP-complete even for directed planar graphs (graphs that can be embedded in the plane) and for cubic planar undirected graphs (graphs in which all vertices have degree three).

M. R. Garey, D. S. Johnson, and L. Stockmeyer. Some simplified NP-complete problems. Proceedings of the sixth annual ACM symposium on Theory of computing, p. 47-63. 1974.</item>

<item>The traveling salesman problem (TSP) on weighted graphs, one of the most intensively studied problems in optimization. http://en.wikipedia.org/wiki/Travelling_salesman_problem</item>

<item>Hamiltonian path on unweighted graphs. The Hamiltonian path problem for graph G is equivalent to the Hamiltonian cycle problem in a graph H obtained from G by adding a new vertex and connecting it to all vertices of G.</item>

<item>Eulerian path</item>

<item>Longest simple path</item>

<item>Longest circuit</item>

<item>Path with forbidden pairs or vertices</item>




<item>Minimum-size vertex cover: A vertex cover is a subset of vertices <eqn>V' \in V</eqn> such that every edge in E contains at least one vertex from V'. Obviously, the set of all vertices is a vertex cover. The problem of finding a minimum vertex cover is a classical optimization problem in computer science a, one of Karp's 21 NP-complete problems</item>

<item>Maximum-size independent set: An independent set is a set of vertices U such that no edge in E is incident on two vertices of U. A maximum independent set is a largest independent set and the maximum independent set problem is known to be NP-hard. A set is independent if and only if its complement is a vertex cover. The sum of the size of the maximum independent set and the size of a minimum vertex cover is the number of vertices in the graph.</item>



<item>Maximum cliques, <eqn>O(3^{n/3}) = O(1.4422^n)</eqn>, since any n-vertex graph has at most any n-vertex graph has at most <eqn>3^{n/3}</eqn> maximal cliques;  http://en.wikipedia.org/wiki/Clique_problem</item>

<item>Dense subgraph, i.e. a subgraph with exactly k vertices and at least y edges.</item>

<item>Planar subgraph, i.e. a subgraph with at least k edges that can be embedded on the plane.</item>

<item>Eulerian subgraph, i.e. a subgraph that contains an Eulerian cycle.</item>

<item>Bipartite subgraph, i.e. a subgraph with at least k edges that can is bipartite.</item>

<item>Cubic subgraph, i.e. a cubic subgraph with at least k edges.</item>

<item>Degree-bounded connected subgraph, i.e. a subgraph with at least k edges so that the resulting subgraph is connected and no vertex degree exceeds d.</item>

<item>Transitive subgraph, i.e. (u,w) (w,v) implies (u,v).</item>

<item>Uniconnected subgraph, i.e. subgraph containing at most one directed path between any pair of vertices.</item>

<item>Minimum k-connected subgraph, i.e. k-connected subgraph (cannot be disconnected by removing fewer than k vertices).</item>



<item>Subgraph isomorphism, i.e. given two graphs G and H as input, determine whether G contains a subgraph that is isomorphic to H.</item>

<item>Largest common subgraph, i.e. given two graphs G and H as input, determine their largest isomorphic subgraphs.</item>

<item>Graph contractability, i.e. given two graphs G and H as input, can a graph isomorphic to H be obtained by a sequence of edge contractions? (when two adjacent vertices are replaced by a single one that is adjacent to exactly those vertices that were adjacent to at least one of the original vertices).</item>

<item>Minimum equivalent digraph. i.e. subgraph that contains a directed path from u to v only when the original graph does.</item>


<item>The Bandwidth problem, which seeks the linear ordering of the vertices that minimizes the length of the longest edge. which can be visualized as placing the vertices of a graph at distinct integer points along the x-axis so that the length of the longest edge is minimized.  Such placement is called linear graph arrangement, linear graph layout or linear graph placement.


e.g. the problem of placement of a set of standard cells in a singe row with the goal of minimizing the maximal propagation delay (which is assumed to be proportional to wire length).

e.g. solving linear systems (Gaussian elimination can be sped up when working with banded matrices, where the bandwidth is the distance from the furthest non-zero entry to the matrix diagonal)

Variants
- Linear arrangement: minimize the sum of the edge lengths
- Profile minimization: minimize the sum of one-way distances
- The pathwidth problem

++ applications in VLSI design, graph drawing, and computational linguistics.</item>

<item>Steiner trees, i.e. find the shortest interconnect for a given set of objects (as MST, but including the possibility of adding extra intermediate nodes).</item>

<item>Constrained spanninig trees: degree-constrained spanning tree (a spanning tree with no vertex with degree above k), maximum-leaf spanning trees, (a spanning tree with k or more leaves), shortest-total-path-length spanning tree (i.e. minimizing the sum of the length of the path in the tree between every pair of nodes), bounded-diameter spanning tree (no simple path with more than d edges)...</item>

<item>Maximum cut (the max-cut problem), i.e. finding a cut whose size is at least the size of any other cut. A polynomial-time algorithm to find maximum cuts in planar graphs exists. <cite>Hadlock 1975</cite></item>

<item>Minimum cut into bounded sets, i.e. imposing restrictions on the size of the disjoint sets that partition the network.</item>


<item>Matching on hypergraphs, i.e. when more than two vertices are involved in a single edge. In fact, 3-dimensional matching is known to be NP-hard whereas bipartite matching (a.k.a. 2-dimensional matching) is in P.</item>



<item>Clique cover (a.k.a. partition into cliques), i.e. determining whether the vertices of a graph can be partitioned into k cliques. </item>

<item>Feedback vertex set, i.e. a set of vertices whose removal leaves a DAG (a graph without cycles).</item>

<item>A feedback arc set (FAS) or feedback edge set is a set of edges which, when removed from the graph, leave a DAG.  A minimal feedback arc set has the additional property that, if the edges in it are reversed rather than removed, then the graph remains acyclic. Finding a small edge set with this property is a key step in layered graph drawing</item>

<item>Chromatic number / graph k-colorability / vertex coloring: assignment of labels traditionally called "colors" to elements of a graph subject to certain constraints. In its simplest form, it is a way of coloring the vertices of a graph such that no two adjacent vertices share the same color; this is called a vertex coloring. Using dynamic programming and a bound on the number of maximal independent sets, k-colorability can be decided in time and space O(2.445^n) <cite>Lawler 1976</cite> Using the principle of inclusion-exclusion and Yates’s algorithm for the fast zeta transform, k-colorability can be decided in time O(n2^n) for any k <cite>Bjorklund et al. 2009</cite>. Faster algorithms are known for 3- and 4-colorability, which can be decided in time O(1.3289^n) <cite>Beigel and Eppstein 2005</cite> and O(1.7504^n) <cite>Byskov 2004</cite>, respectively. Note that 2-colorable graphs are bipartite graphs and bipartite testing can be performed in linear time.
</item>

<item>Edge coloring: assigns a color to each edge so that no two adjacent edges share the same color. an edge coloring of a graph is just a vertex coloring of its line graph (a graph that represents the adjacencies between edges of G)</item>

<item>Face coloring of a planar graph assigns a color to each face or region so that no two faces that share a boundary have the same color. a face coloring of a planar graph is just a vertex coloring of its planar dual (a vertex for each plane region of G, and an edge for each edge in G joining two neighboring regions). </item>

<item>Achromatic number: Does a graph G have an achromatic number K or greater? i.e. is there a partition of V into k disjoint sets so that each V_i is an independent set.</item>

<item>Other graph partitioning problems: into triangles, into cliques, into isomorphic subgraphs, into Hamiltonian subgraphs, into forests, into perfect matchings... are also known NP-complete problems.</item>

</list>

<text>
Contrast subtle differences: Eulerian path (easy) vs. Eulerian cycle (hard); bipartite matching (easy) vs. hypergraph matching (hard); shortest paths (easy) vs. longest paths (hard); Hamiltonian path on a graph (hard) vs. on a DAG (easy); the minimum spanning tree (easy), which is the undirected variant of the feedback arc set problem (hard)...
</text>

<text>
Even when a problem is NP-complete, algorithms can be devised that solve them in practice, either by employing a systematic search on the problem space (e.g. employing backtracking or branch-and-bound algorithms with substantial pruning) or heuristic methods that might not find the optimal solution but often return a good-enough one (e.g. simulated annealing). Heuristics are also employed by approximation algorithms that can get reasonable solutions with guaranteed bounds; i.e. the solution they provide is related to a lower bound on the optimal solution and imposes limits on how badly the approximation algorithm might perform.
</text>

<list>

<item>Vertex cover: Greedy algorithm that chooses an edge at random and adds both intervening nodes to the cover. Since one of the incident vertices must always be on the minimum vertex cover, this simple algorithm efficiently returns a cover that is at most twice as large as the optimal cover.
</item>

<item>The Euclidean Traveling Salesman Problem: When the triangle inequality holds, we can approximate the TSP solution by employing minimum spanning trees. Such trees, which connect all the nodes in a graph, can be traversed in depth. That traversal provides a non-Hamiltonian cycle whose length is at most twice the optimal cycle (why? deleting any link from the cycle returns a Hamiltonian path, i.e. a tree, whose weight cannot be greater than that of the MST.
</item>

<item>Maximum DAG: The largest possible subset of edges so that the resulting subgraph is acyclic. Any permutation of nodes can be interpreted as a topological sorting of the graph. Removing either left-to-right or right-to-left edges, we keep at least half of the edges in the original graph, hence the solution contains at least half as many edges as the optimum.
</item>

</list>

<text>
Alternative heuristics might even get better results for particular instances of the above problems, albeit more complex heuristics do not necessarily guarantee better results. In fact, the result if often the opposite. It should also be noted that some clever postprocessing of the results provided by approximation algorithms might also be worthwhile, since it might improve the approximate solution without weakening the approximation bound.
</text>

</document>


<!--
<figure>
<title>Image example (default file type)</title>
<image scale="20" file="image/cover/0321127420"/>
</figure>

<figure>
<title>Another image example (specific file type)</title>
<image scale="20" file="image/cover/0321117425.jpg" type="jpg"/>
</figure>

<text>
Inline image:
</text>

<image scale="20" file="image/cover/0321117425.jpg" type="jpg"/>
-->

</document>


<document>
<tag>graph-algorithms-notes</tag>
<title>Bibliographic notes</title>


<text>
Many textbooks describe at least part of the material mentioned in this chapter.

<cite>Skiena 2008</cite>, <cite>Kleinberg and Tardos 2005</cite>
<cite>Even 2011</cite> discusses many basic graph algorithms known before 1979.
<cite>Tarjan 1983</cite> book provides a survey of both algorithms and data structures.
<cite>Gibbons 1985</cite>
<cite>Sedgewick 1990</cite> in C, <cite>Sedgewick 2002</cite> and <cite>Sedgewick 2003</cite> in Java (2nd volume devoted to graph algorithms)

</text>


<text>
<cite>Grama et al. 2003</cite> includes a chapter on parallel graph algorithms.
</text>



<text>
Network flow; monographs <cite>Ahuja et al. 1993</cite> <cite>Ford and Fulkerson 1962</cite>; surveys <cite>Goldberg et al. 1990</cite> <cite>Schrijver 2002</cite>; original algorithms <cite> Ford and Fulkerson 1956</cite> <cite>Edmonds and Karp 1972</cite>
</text>

<text>
Matching:  The concept of a matching in a graph is very fundamental and forms
the cornerstone of combinatorial optimization, with an entire book devoted
to it <cite>Lovasz and Plummer 1986</cite>
</text>

<text>
NP problems:
- 1972: Landmark paper, "Reducibility Among Combinatorial Problems"  <cite>Karp 1972</cite>, 21 combinatorial and graph theoretical NP-complete problems
- 1979: Catalog containing hundreds of NP problems<cite>Garey and Johnson 1979</cite>, 65 graph theoretical problems and 51 network design problems.
- Wikipedia: More than 3000 known NP-complete problems, http://en.wikipedia.org/wiki/List_of_NP-complete_problems
</text>

<text>

Many graph problems (such as shortest paths, bipartite matching, or network flows) can also be solved as special cases of linear programming.

The books by <cite>Papadimitriou and Steiglitz 1982</cite> and <cite>Ahuja et al. 1993</cite> provide a
comprehensive coverage of combinatorial optimization problems. 

Hochbaum's edited collection <cite>Hochbaum 1996</cite> provides an extensive coverage of work on approximation
algorithms for graph problems. 

Software tools: lp_solve (http://lpsolve.sourceforge.net/), CLP (http://www.coin-or.org), NEOS = Network-Enabled Optimization System @ Argonne National Laboratory, Illinois (submit your own optimization jobs at http://neos.mcs.anl.gov/)

</text>


<text>
Conferences: Many international conferences carry articles on recent advances in graph
algorithms. Some of them are the IEEE Symposium on Foundations of Computer
Science (FOCS), the ACM Symposium on Theory of Computing (STOC), the ACM-SIAM
Symposium on Discrete Algorithms (SODA), and the International Colloquium on
Automata, Languages and Programming (ICALP).
</text>

</document>


<document>
<tag>graph-algorithms-references</tag>
<title>References</title>


<reference id="Aho et al. 1974">
 <author>Alfred V. Aho</author>
 <author>John E. Hopcroft</author>
 <author>Jeffrey D. Ullman</author>
 <title>The Design and Analysis of Computer Algorithms</title>
 <publisher>Addison-Wesley</publisher>
 <year>1974</year>
 <isbn>0201000296</isbn>
</reference>

<reference id="Aho et al. 1983">
 <author>Alfred V. Aho</author>
 <author>John E. Hopcroft</author>
 <author>Jeffrey D. Ullman</author>
 <title>Data Structures and Algorithms</title>
 <publisher>Addison-Wesley</publisher>
 <year>1983</year>
 <isbn>0201000237</isbn>
 <file>[books]/algorithms/ddj/books/book9/toc.htm</file>
</reference>

<reference id="Ahuja et al. 1993">
 <author>Ravindra K. Ahuja</author>
 <author>Thomas L. Magnanti</author>
 <author>James B. Orlin</author>
 <title>Network Flows: Theory, Algorithms, and Applications</title>
 <publisher>Prentice Hall</publisher>
 <year>1993</year>
 <isbn>013617549X</isbn>
 <file>[books]/algorithms/graphs/013617549X [Ahuja] Network Flows.1993.zip</file>
</reference>

<reference id="Babai et al. 1982">
 <author>László Babai</author>
 <author>D. Yu. Grigoryev</author>
 <author>David M. Mount</author>
 <title>Isomorphism of graphs with bounded eigenvalue multiplicity</title>
 <proceedings>14th Annual ACM Symposium on the Theory of Computing</proceedings>
 <conference>STOC</conference>
 <location>San Francisco, California</location>
 <year>1982</year>
 <isbn>0-89791-070-2</isbn>
 <pages>310-324</pages>
 <doi>10.1145/800070.802206</doi>
 <file>Networks/algorithms/graphs/isomorphism/1982 STOC Isomorphism of graphs of bounded eigenvalue multiplicity.pdf</file>
</reference>

<reference id="Babai and Luks 1983">
 <author>László Babai</author>
 <author>Eugene M. Luks</author>
 <title>Canonical labeling of graphs</title>
 <proceedings>15th Annual ACM Symposium on the Theory of Computing</proceedings>
 <conference>STOC</conference>
 <location>Boston, Massachusetts</location>
 <year>1983</year>
 <isbn>0-89791-099-0</isbn>
 <pages>171-183</pages>
 <doi>10.1145/800061.808746</doi>
 <file>Networks/algorithms/graphs/isomorphism/1983 STOC [Babai &amp; Luks] Canonical labeling.pdf</file>
</reference>


<reference id="Beigel and Eppstein 2005">
 <author>Richard Beigel</author>
 <author>David Eppstein</author>
 <title>3-coloring in time O(1.3289^n)</title>
 <journal>Journal of Algorithms</journal>
 <volume>54</volume>
 <number>2</number>
 <pages>168-204</pages>
 <year>2005</year>
 <doi>10.1016/j.jalgor.2004.06.008</doi>
 <file>Networks/algorithms/graphs/np/2005 JA 3-coloring.pdf</file>
</reference>


<reference id="Bellman 1958">
 <author>Richard E. Bellman</author>
 <title>On a routing problem</title>
 <journal>Quarterly of Applied Mathematics</journal>
 <volume>16</volume>
 <pages>87-90</pages>
 <year>1958</year>
 <!--<url>http://monet.skku.ac.kr/course_materials/undergraduate/al/lecture/2006/BellmanFord.pdf</url>-->
 <file>Networks/algorithms/graphs/paths/1958 QAM [Bellman] Shortest paths.pdf</file>
</reference>


<reference id="Berliner 1979">
 <author>Hans Berliner</author>
 <title>The B* Tree Search Algorithm: A Best-First Proof Procedure</title>
 <journal>Artificial Intelligence</journal>
 <volume>12</volume>
 <number>1</number>
 <pages>23-40</pages>
 <year>1979</year>
 <doi>10.1016/0004-3702(79)90003-1</doi>
 <file>Networks/algorithms/graphs/search/1979 AI Bstar.pdf</file>
</reference>


<reference id="Beyer et al. 1979">
 <author>T. Beyer</author>
 <author>W. Jones</author>
 <author>S. Mitchell</author>
 <title>Linear algorithms for isomorphism of maximal outerplanar graphs</title>
 <journal>Journal of the ACM</journal>
 <volume>26</volume>
 <number>4</number>
 <pages>603-610</pages>
 <year>1979</year>
 <doi>10.1145/322154.322155</doi>
 <file>Networks/algorithms/graphs/isomorphism/1979 JACM Outerplanar graph isomorphism.pdf</file>
</reference>


<reference id="Bjorklund et al. 2009">
 <author>Andreas Björklund</author>
 <author>Thore Husfeldt</author>
 <author>Mikko Koivisto</author>
 <title>Set partitioning via inclusion-exclusion</title>
 <journal>SIAM Journal on Computing</journal>
 <volume>39</volume>
 <number>2</number>
 <pages>546-563</pages>
 <year>2009</year>
 <doi>10.1137/070683933</doi>
 <file>Networks/algorithms/graphs/np/2009 SIAM JC Set partitioning via inclusion-exclusion.pdf</file>
</reference>


<reference id="Bodlaender 1990">
 <author>Hans Bodlaender</author>
 <title>Polynomial algorithms for graph isomorphism and chromatic index on partial k-trees</title>
 <journal>Journal of Algorithms</journal>
 <volume>11</volume>
 <number>4</number>
 <pages>631-643</pages>
 <year>1990</year>
 <doi>10.1016/0196-6774(90)90013-5</doi>
 <file>Networks/algorithms/graphs/isomorphism/1990 JA Isomorphism on partial k-trees.pdf</file>
</reference>

<reference id="Bondy and Murty 1976">
 <author>John Adrian Bondy</author>
 <author>U.S.R. Murty</author>
 <title>Graph Theory With Applications</title>
 <publisher>North-Holland</publisher>
 <year>1976</year>
 <isbn>0444194517</isbn>
 <file>[books]\algorithms\graphs\0444194517 [Bondy &amp; Murty] Graph Theory with Applications.pdf</file>
</reference>

<reference id="Boruvka 1926">
 <author>Otakar Boruvka</author>
 <title>O jistém problému minimálním (About a certain minimal problem, in Czech)</title>
 <journal>Práce mor. prírodoved. spol. v Brne III</journal>
 <volume>3</volume>
 <pages>37-58</pages>
 <year>1926</year>
</reference>


<reference id="Broder and Mitzenmacher 2005">
 <author>Andrei Broder</author>
 <author>Michael Mitzenmacher</author>
 <title>Network Applications of Bloom Filters: A Survey</title>
 <journal>Internet Mathematics</journal>
 <volume>1</volume>
 <number>4</number>
 <pages>485-509</pages>
 <year>2005</year>
 <url>http://www.eecs.harvard.edu/~michaelm/postscripts/im2005b.pdf</url>
 <doi>10.1145/355588.365103</doi>
 <file>Computer Science/algorithms/adt/2005 IM Bloom filters.pdf</file>
</reference>


<reference id="Byskov 2004">
 <author>Jesper Makholm Byskov</author>
 <title>Enumerating maximal independent sets with applications to graph colouring</title>
 <journal>Operations Research Letters</journal>
 <volume>32</volume>
 <number>6</number>
 <pages>547-556</pages>
 <year>2004</year>
 <doi>10.1016/j.orl.2004.03.002</doi>
 <file>Networks/algorithms/graphs/np/2004 ORL Graph colouring.pdf</file>
</reference>


<reference id="Carlsson and Chen 1992">
 <author>Svante Carlsson</author>
 <author>Jingsen Chen</author>
 <title>The complexity of heaps</title>
 <proceedings>3rd Annual ACM-SIAM Symposium on Discrete algorithms</proceedings>
 <conference>SODA</conference>
 <location>Orlando, Florida</location>
 <year>1992</year>
 <isbn>0-89791-466-X</isbn>
 <pages>393-402</pages>
 <doi>10.1007/BFb0012776</doi>
 <file>Computer Science/algorithms/adt/1992 SODA Complexity of heaps.pdf</file>
</reference>


<reference id="Chazelle 2000">
 <author>Bernard Chazelle</author>
 <title>A minimum spanning tree algorithm with inverse-Ackermann type complexity</title>
 <journal>Journal of the ACM</journal>
 <volume>47</volume>
 <number>6</number>
 <pages>1028-1047</pages>
 <year>2000</year>
 <doi>10.1145/355541.355562</doi>
 <file>Networks/algorithms/graphs/paths/2000 JACM [Chazelle] MST.pdf</file>
</reference>


<reference id="Cheriyan and Mehlhorn 1996">
 <author>J. Cheriyan</author>
 <author>K. Mehlhorn</author>
 <title>Algorithms for dense graphs and networks on the random access computer</title>
 <journal>Algorithmica</journal>
 <volume>15</volume>
 <number>6</number>
 <pages>521-549</pages>
 <year>1996</year>
 <doi>10.1007/BF01940880</doi>
 <file>Networks/algorithms/graphs/connectivity/1996 Algorithmica [Cheriyan &amp; Mehlhorn] Dense graphs and networks.pdf</file>
</reference>


<reference id="Choquet 1938">
 <author>Gustave Choquet</author>
 <title>Étude de certains réseaux de routes (Study of research on routes, in French)</title>
 <journal>Comptes-rendus de l'Académie des Sciences</journal>
 <volume>206</volume>
 <pages>310-313</pages>
 <year>1938</year>
</reference>


<reference id="Cong and Bader 2005">
 <author>Guojing Cong</author>
 <author>David A. Bader</author>
 <title>An experimental study of parallel biconnected components algorithms on symmetric multiprocessors (SMPs)</title>
 <proceedings>19th IEEE International Conference on Parallel and Distributed Processing Symposium</proceedings>
 <conference>IPDPS</conference>
 <location>Denver, Colorado</location>
 <year>2005</year>
 <isbn>0-7695-2312-9</isbn>
 <pages>45b</pages>
 <doi>10.1109/IPDPS.2005.100</doi>
 <file>Networks/algorithms/graphs/connectivity/2005 IPDPS [Cong &amp; Bader] Parallel biconnected algorithms.pdf</file>
</reference>


<reference id="Corneil and Gotlieb 1970">
 <author>Derek G. Corneil</author>
 <author>Calvin C. Gotlieb</author>
 <title>An efficient algorithm for graph isomorphism</title>
 <journal>Journal of the ACM</journal>
 <volume>17</volume>
 <number>1</number>
 <pages>51-64</pages>
 <year>1970</year>
 <doi>10.1145/321556.321562</doi>
 <file>Networks/algorithms/graphs/isomorphism/1970 JACM Graph isomorphism.pdf</file>
</reference>


<reference id="Colbourn 1981">
 <author>Charles J. Colbourn</author>
 <title>On testing isomorphism of permutation graphs</title>
 <journal>Networks</journal>
 <volume>11</volume>
 <number>1</number>
 <pages>13-21</pages>
 <year>1981</year>
 <doi>10.1002/net.3230110103</doi>
 <!--<file>Networks/algorithms/graphs/isomorphism/1981 Networks...</file>-->
</reference>


<reference id="Dijkstra 1959">
 <author>Edsger W. Dijkstra.</author>
 <title>A note on two problems in connexion with graphs</title>
 <journal>Numerische Mathematik</journal>
 <volume>1</volume>
 <number>1</number>
 <pages>269-271</pages>
 <year>1959</year>
 <doi>10.1007/BF01386390</doi>
 <file>Networks/algorithms/graphs/paths/1959 NM [Dijkstra] MST &amp; Shortest paths.pdf</file>
</reference>


<reference id="Edmonds and Johnson 1973">
 <author>Jack Edmonds</author>
 <author>Ellis L. Johnson</author>
 <title>Matching, Euler tours, and the Chinese postman</title>
 <journal>Mathematical Programming</journal>
 <volume>5</volume>
 <number>1</number>
 <pages>88-124</pages>
 <year>1973</year>
 <doi>10.1007/BF015801133</doi>
 <file>Networks/algorithms/graphs/paths/1973 MP Chinese postman.pdf</file>
</reference>

<reference id="Edmonds and Karp 1972">
 <author>Jack Edmonds</author>
 <author>Richard M. Karp</author>
 <title>Theoretical improvements in algorithmic efficiency for network flow problems</title>
 <journal>Journal of the ACM</journal>
 <volume>19</volume>
 <number>2</number>
 <pages>248-264</pages>
 <year>1972</year>
 <doi>10.1145/321694.321699</doi>
 <file>Networks/algorithms/flow/1972 JACM [Edmonds &amp; Karp] Network flow.pdf</file>
</reference>


<reference id="Even 2011">
 <author>Shimon Even</author>
 <title>Graph Algorithms</title>
 <publisher>Cambridge University Press</publisher>
 <edition>2nd edition</edition>
 <year>2011</year>
 <isbn>9780521517188</isbn>
</reference>


<reference id="Filotti and Mayer 1980">
 <author>I.S. Filotti</author>
 <author>Jack N. Mayer</author>
 <title>A polynomial-time algorithm for determining the isomorphism of graphs of fixed genus</title>
 <proceedings>12th Annual ACM Symposium on the Theory of Computing</proceedings>
 <conference>STOC</conference>
 <location>Los Angeles, California</location>
 <year>1980</year>
 <isbn>0-89791-017-6</isbn>
 <pages>236-243</pages>
 <doi>10.1145/800141.804671</doi>
 <file>Networks/algorithms/graphs/isomorphism/1980 STOC Isomorphism testing for graphs of fixed genus.pdf</file>
</reference>


<reference id="Floyd 1962">
 <author>Robert W. Floyd</author>
 <title>Algorithm 97: Shortest path</title>
 <journal>Communications of the ACM</journal>
 <volume>7</volume>
 <number>6</number>
 <pages>345</pages>
 <year>1962</year>
 <doi>10.1145/367766.368168</doi>
 <file>Networks/algorithms/graphs/paths/1962 CACM [Floyd] Shortest paths.pdf</file>
</reference>


<reference id="Floyd 1964">
 <author>Robert W. Floyd</author>
 <title>Algorithm 245: Treesort 3</title>
 <journal>Communications of the ACM</journal>
 <volume>7</volume>
 <number>12</number>
 <pages>701-702</pages>
 <year>1964</year>
 <doi>10.1145/355588.365103</doi>
 <file>Computer Science/algorithms/sorting/1964 CACM [Floyd] TreeSort.pdf</file>
</reference>

<reference id="Ford and Fulkerson 1956">
 <author>Lester Randolph Ford, Jr.</author>
 <author>Delbert Ray Fulkerson</author>
 <title>Maximal flow through a network</title>
 <journal>Canadian Journal of Mathematics</journal>
 <volume>8</volume>
 <pages>399-404</pages>
 <year>1956</year>
 <file>Networks/algorithms/flow/1956 CJM [Ford &amp; Fulkerson] Maximal flow.pdf</file>
</reference>

<reference id="Ford and Fulkerson 1962">
 <author>Lester Randolph Ford, Jr.</author>
 <author>Delbert Ray Fulkerson</author>
 <title>Flows in Networks</title>
 <publisher>Princeton University Press</publisher>
 <year>1962</year>
 <isbn>0691079625</isbn>
 <file>[books]/algorithms/graphs/0691079625 [Ford &amp; Fulkerson] Flows in Networks.1962.djvu</file>
</reference>


<reference id="Fredman and Tarjan 1987">
 <author>Michael L. Fredman</author>
 <author>Robert E. Tarjan</author>
 <title>Fibonacci heaps and their uses in improved network optimization algorithms</title>
 <journal>Journal of the ACM</journal>
 <volume>34</volume>
 <number>3</number>
 <pages>596-615</pages>
 <year>1987</year>
 <doi>10.1145/28869.28874</doi>
 <file>Computer Science/algorithms/adt/1987 JACM Fibonacci heaps.pdf</file>
</reference>


<reference id="Frigo et al. 1999">
 <author>Matteo Frigo</author>
 <author>Charles E. Leiserson</author>
 <author>Harald Prokop</author>
 <author>Sridhar Ramachandran</author>
 <title>Cache-oblivious algorithms</title>
 <proceedings>40th Annual Symposium on Foundations of Computer Science</proceedings>
 <conference>FOCS</conference>
 <location>New York, NY, USA</location>
 <year>1999</year>
 <pages>285-298</pages>
 <isbn>0-7695-0409-4</isbn>
 <doi>10.1109/SFFCS.1999.814600</doi>
 <file>Computer Science/algorithms/1999 FOCS Cache-oblivious algorithms.pdf</file>
</reference>


<reference id="Gabow 2003">
 <author>Harold N. Gabow</author>
 <title>Searching (10.1)</title>
 <booktitle>Handbook of Graph Theory (Discrete Mathematics and its Applications, 25)</booktitle>
 <publisher>CRC Press</publisher>
 <year>2003</year>
 <pages>953-984</pages>
 <isbn>1584880902</isbn>
 <file>[books]/algorithms/graphs/1584880902 [Gross &amp; Yellen] Handbook of Graph Theory - Discrete Mathematics and Its Applications.pdf</file>
</reference>


<reference id="Gale and Shapley 1962">
 <author>D. Gale</author>
 <author>L.S. Shapley</author>
 <title>College admissions and the stability of marriage</title>
 <journal>The American Mathematical Monthly</journal>
 <volume>69</volume>
 <number>1</number>
 <pages>9-15</pages>
 <year>1962</year>
 <url>http://www.jstor.org/stable/2312726</url>
 <file>Networks/algorithms/flow/matching/1962 AMM Stable marriage.pdf</file>
</reference>


<reference id="Galil et al. 1987">
 <author>Zvi Galil</author>
 <author>Christoph M. Hoffmann</author>
 <author>Eugene M. Luks</author>
 <author>Claus P. Schnorr</author>
 <author>Andreas Weber</author>
 <title>An O(n^3 log n) deterministic and an O(n^3) Las Vegas isomorphism test for trivalent graphs</title>
 <journal>Journal of the ACM</journal>
 <volume>34</volume>
 <number>3</number>
 <pages>513-531</pages>
 <year>1987</year>
 <doi>10.1145/28869.28870</doi>
 <file>Networks/algorithms/graphs/isomorphism/1987 JACM Trivalent graph isomorphism.pdf</file>
</reference>

<reference id="Gamma et al. 1994">
 <author>Erich Gamma</author>
 <author>Richard Helm</author>
 <author>Ralph Johnson</author>
 <author>John Vlissides</author>
 <title>Design Patterns: Elements of reusable object-oriented software</title>
 <publisher>Addison-Wesley</publisher>
 <year>1994</year>
 <isbn>0201633612</isbn>
 <file>[books]/design/patterns/0201633612 [GoF] Design Patterns.chm</file>
</reference>



<reference id="Garey and Johnson 1979">
 <author>Michael R. Garey</author>
 <author>David S. Johnson</author>
 <title>Computers and Intractability: A Guide to the Theory of NP-Completeness</title>
 <publisher>W.H. Freeman</publisher>
 <year>1979</year>
 <isbn>0716710455</isbn>
 <file>[books]/algorithms/0716710455 [Garey &amp; Johnson] Computers and Intractability.pdf</file>
</reference>


<reference id="Gibbons 1985">
 <author>Alan M. Gibbons</author>
 <title>Algorithmic Graph Theory</title>
 <publisher>Cambridge University Press</publisher>
 <year>1985</year>
 <isbn>0521246598</isbn>
</reference>


<reference id="Goldberg and Tarjan 1988">
 <author>Andrew V. Goldberg</author>
 <author>Robert E. Tarjan</author>
 <title>A new approach to the maximum-flow problem</title>
 <journal>Journal of the ACM</journal>
 <volume>35</volume>
 <number>4</number>
 <pages>921-940</pages>
 <year>1988</year>
 <doi>10.1145/48014.61051</doi>
 <file>Networks/algorithms/flow/1988 JACM Maximum-flow problem.pdf</file>
</reference>


<reference id="Goldberg et al. 1990">
 <author>Andrew V. Goldberg</author>
 <author>Eva Tardos</author>
 <author>Robert E. Tarjan</author>
 <title>Network Flow Algorithms</title>
 <booktitle>Algorithms and Combinatorics 9</booktitle>
 <publisher>Springer-Verlag</publisher>
 <year>1990</year>
 <isbn>0387526854</isbn>
 <file>Networks/algorithms/flow/1990 AC Network Flow Algorithms.pdf</file>
</reference>



<reference id="Goldberg et al. 2006">
 <author>Andrew V. Goldberg</author>
 <author>Haim Kaplan</author>
 <author>Renato F. Werneck</author>
 <title>Reach for A*: Efficient point-to-point shortest path algorithms</title>
 <proceedings>8th SIAM Workshop on Algorithms Engineering and Experimentation</proceedings>
 <conference>ALENEX</conference>
 <location>Miami, Florida</location>
 <year>2006</year>
 <pages>129-143</pages>
 <url>http://www.siam.org/proceedings/alenex/2006/alenex06.php</url>
 <!--<url>http://www.siam.org/proceedings/alenex/2006/alx06_013agoldberg.pdf</url>-->
 <file>Networks/algorithms/graphs/paths/2006 ALENEX Astar shortest paths.pdf</file>
 <file>Networks/algorithms/graphs/paths/2006 ALENEX Astar shortest paths.tr.pdf</file>
</reference>

<reference id="Goldberg et al. 2007">
 <author>Andrew V. Goldberg</author>
 <author>Haim Kaplan</author>
 <author>Renato F. Werneck</author>
 <title>Better landmarks within reach</title>
 <proceedings>6th International Workshop on Experimental Algorithms</proceedings>
 <conference>WEA</conference>
 <location>Rome, Italy</location>
 <year>2007</year>
 <pages>38-51</pages>
 <series>Lecture Notes in Computer Science</series>
 <volume>4525</volume>
 <isbn>978-3-540-72844-3</isbn>
 <doi>10.1007/978-3-540-72845-0_4</doi>
 <file>Networks/algorithms/graphs/paths/2007 WEA Astar shortest paths.pdf</file>
</reference>


<reference id="Gonnet and Munro 1982">
 <author>Gaston H. Gonnet</author>
 <author>J. Ian Munro</author>
 <title>Heaps on heaps</title>
 <proceedings>9th International Colloquium on Automata, Languages and Programming</proceedings>
 <conference>ICALP</conference>
 <location>Aarhus, Denmark</location>
 <series>Lecture Notes in Computer Science</series>
 <volume>140</volume>
 <year>1982</year>
 <isbn>3-540-11576-5</isbn>
 <pages>282-291</pages>
 <doi>10.1007/BFb0012776</doi>
</reference>

<reference id="Gonnet and Munro 1986">
 <author>Gaston H. Gonnet</author>
 <author>J. Ian Munro</author>
 <title>Heaps on heaps</title>
 <journal>SIAM Journal on Computing</journal>
 <volume>15</volume>
 <number>4</number>
 <pages>964-971</pages>
 <year>1986</year>
 <doi>10.1137/0215068</doi>
</reference>

<reference id="Grama et al. 2003">
 <author>Ananth Grama</author>
 <author>Anshul Gupta</author>
 <author>George Karypis</author>
 <author>Vipin Kumar</author>
 <title>Introduction to Parallel Computing</title>
 <publisher>Addison Wesley</publisher>
 <edition>2nd edition</edition>
 <year>2003</year>
 <isbn>0201648652</isbn>
 <file>[books]/algorithms/0201648652 [Kumar] Parallel Computing.2nd.chm</file>
</reference>


<reference id="Hadlock 1975">
 <author>Frank Hadlock</author>
 <title>Finding a maximum cut of a planar graph in polynomial time</title>
 <journal>SIAM Journal on Computing</journal>
 <volume>4</volume>
 <number>3</number>
 <pages>221-225</pages>
 <year>1975</year>
 <doi>10.1137/0204019</doi>
</reference>


<reference id="Hart et al. 1968">
 <author>Peter E. Hart</author>
 <author>Nils J. Nilsson</author>
 <author>Bertram Raphael</author>
 <title>A formal basis for the heuristic determination of minimum-cost paths</title>
 <journal>IEEE Transactions on Systems Science and Cybernetics</journal>
 <volume>4</volume>
 <number>2</number>
 <pages>100-107</pages>
 <year>1968</year>
 <doi>10.1109/TSSC.1968.300136</doi>
 <file>Networks/algorithms/graphs/search/1968 TSSC Astar.pdf</file>
</reference>


<reference id="Hart et al. 1972">
 <author>Peter E. Hart</author>
 <author>Nils J. Nilsson</author>
 <author>Bertram Raphael</author>
 <title>Correction to 'A formal basis for the heuristic determination of minimum-cost paths'</title>
 <journal>SIGART Newsletter</journal>
 <volume>37</volume>
 <pages>28-29</pages>
 <year>1972</year>
 <doi>10.1145/1056777.1056779</doi>
 <file>Networks/algorithms/graphs/search/1972 SIGART Correction to Astar.pdf</file>
</reference>


<reference id="Hochbaum 1996">
 <author>Dorit S. Hochbaum</author>
 <title>Approximation Algorithms for NP-Hard Problems</title>
 <publisher>PWS Publishing Company</publisher>
 <year>1996</year>
 <isbn>0-534-94968-1</isbn>
</reference>


<reference id="Holm et al. 2001">
 <author>Jacob Holm</author>
 <author>Kristian de Lichtenberg</author>
 <author>Mikkel Thorup</author>
 <title>Poly-logarithmic deterministic fully-dynamic algorithms for connectivity, minimum spanning tree, 2-edge, and biconnectivity</title>
 <journal>Journal of the ACM</journal>
 <volume>48</volume>
 <number>4</number>
 <pages>723-760</pages>
 <year>2001</year>
 <doi>10.1145/502090.502095</doi>
 <file>Networks/algorithms/graphs/paths/2001 JACM [Holm] MST.pdf</file>
</reference>

<reference id="Hopcroft and Karp 1973">
 <author>John E. Hopcroft</author>
 <author>Richard M. Karp</author>
 <title>An $n^{5/3}$ algorithm for maximum matchings in bipartite graphs</title>
 <journal>SIAM Journal on Computing</journal>
 <volume>2</volume>
 <number>4</number>
 <pages>225-231</pages>
 <year>1973</year>
 <doi>10.1137/0202019</doi>
</reference>


<reference id="Hopcroft and Tarjan 1973">
 <author>John Hopcroft</author>
 <author>Robert E. Tarjan</author>
 <title>Efficient algorithms for graph manipulations</title>
 <journal>Communications of the ACM</journal>
 <volume>16</volume>
 <number>6</number>
 <pages>372-378</pages>
 <year>1973</year>
 <doi>10.1145/362248.362272</doi>
 <file>Networks/algorithms/graphs/connectivity/1973 CACM [Hopcroft &amp; Tarjan] Graph manipulation.pdf</file>
</reference>


<reference id="Hopcroft and Tarjan 1973b">
 <author>John E. Hopcroft</author>
 <author>Robert E. Tarjan</author>
 <title>Dividing a graph into triconnected components</title>
 <journal>SIAM Journal on Computing</journal>
 <volume>2</volume>
 <number>3</number>
 <pages>135-158</pages>
 <year>1973</year>
 <doi>10.1137/0202012</doi>
</reference>


<reference id="Hopcroft and Wong 1974">
 <author>John E. Hopcroft</author>
 <author>Eugene M. Luks</author>
 <title>Linear time algorithm for isomorphism of planar graphs</title>
 <proceedings>6th Annual ACM Symposium on the Theory of Computing</proceedings>
 <conference>STOC</conference>
 <location>Seattle, Washington</location>
 <year>1974</year>
 <pages>172-184</pages>
 <doi>10.1145/800119.803896</doi>
 <file>Networks/algorithms/graphs/isomorphism/1974 STOC Linear algorithm for planar graph isomorphism.pdf</file>
</reference>


<reference id="Johnson 1977">
 <author>Donald B. Johnson</author>
 <title>Efficient algorithms for shortest paths in sparse networks</title>
 <journal>Journal of the ACM</journal>
 <volume>24</volume>
 <number>1</number>
 <pages>1-13</pages>
 <year>1977</year>
 <doi>10.1145/321992.321993</doi>
 <file>Networks/algorithms/graphs/paths/1977 JACM [Johnson] Shortest paths.pdf</file>
</reference>

<reference id="Kahn 1962">
 <author>A.B. Kahn</author>
 <title>Topological sorting of large networks</title>
 <journal>Communications of the ACM</journal>
 <volume>5</volume>
 <number>11</number>
 <pages>558-562</pages>
 <year>1962</year>
 <doi>10.1145/368996.369025</doi>
 <file>Networks/algorithms/graphs/1962 CACM [Kahn] Topological sorting.pdf</file>
</reference>

  
<reference id="Kanevsky and Ramachandran 1991">
 <author>Arkady Kanevsky</author>
 <author>Vijaya Ramachandran</author>
 <title>Improved algorithms for graph four-connectivity</title>
 <journal>Journal of Computer and System Sciences</journal>
 <volume>42</volume>
 <number>3</number>
 <pages>288-306</pages>
 <year>1991</year>
 <doi>10.1016/0022-0000(91)90004-O</doi>
 <file>Networks/algorithms/graphs/connectivity/1991 JCSS 4-connectivity.pdf</file>
</reference>


<reference id="Karger et al. 1995">
 <author>David R. Karger</author>
 <author>Philip N. Klein</author>
 <author>Robert E. Tarjan</author>
 <title>A randomized linear-time algorithm to find minimum spanning trees</title>
 <journal>Journal of the ACM</journal>
 <volume>42</volume>
 <number>2</number>
 <pages>321-328</pages>
 <year>1995</year>
 <doi>10.1145/201019.201022</doi>
 <file>Networks/algorithms/graphs/paths/1995 JACM [Karger] MST.pdf</file>
</reference>


<reference id="Karp 1972">
 <author>Richard M. Karp</author>
 <title>Reducibility among combinatorial problems</title>
 <booktitle>Complexity of Computer Computations</booktitle>
 <publisher>IBM Research Symposia Series, Plenum Press</publisher>
 <year>1972</year>
 <isbn>0306307073</isbn>
 <file>Networks/algorithms/1972 [Karp] Reducibility.pdf</file>
</reference>


<reference id="Karypis and Kumar 1999">
 <author>George Karypis</author>
 <author>Vipin Kumar</author>
 <title>A fast and high quality multilevel scheme for partitioning irregular graphs</title>
 <journal>SIAM Journal on Scientific Computing</journal>
 <volume>20</volume>
 <number>1</number>
 <pages>359-392</pages>
 <year>1999</year>
 <doi>10.1137/S1064827595287997</doi>
 <file>Networks/algorithms/graphs/connectivity/1999 SIAMSC [Karypis &amp; Kumar] METIS.pdf</file>
</reference>

<reference id="Kelly 1957">
 <author>Paul J. Kelly</author>
 <title>A congruence theorem for trees</title>
 <journal>Pacific Journal of Mathematics</journal>
 <volume>7</volume>
 <number>1</number>
 <pages>961-968</pages>
 <year>1957</year>
 <url>http://projecteuclid.org/euclid.pjm/1103043674</url>
 <file>Networks/algorithms/graphs/isomorphism/1957 PJM [Kelly] Congruence theorem for trees.pdf</file>
</reference>

<reference id="Kleinberg and Tardos 2005">
 <author>Jon Kleinberg</author>
 <author>Éva Tardos</author>
 <title>Algorithm Design</title>
 <publisher>Addison-Wesley</publisher>
 <year>2005</year>
 <isbn>0321295358</isbn>
 <file>[books]/algorithms/0321295358 [Kleinberg] Algorithm Design.pdf</file>
</reference>

<reference id="Kruskal 1956">
 <author>Joseph B. Kruskal, Jr.</author>
 <title>On the shortest spanning subtree of a graph and the traveling salesman problem</title>
 <journal>Proceedings of the American Mathematical Society</journal>
 <volume>7</volume>
 <number>1</number>
 <pages>48-50</pages>
 <year>1956</year>
 <url>http://www.jstor.org/stable/2033241</url>
 <file>Networks/algorithms/graphs/paths/1956 AMS [Kruskal] MST.pdf</file>
</reference>

<reference id="Kuhn 1955">
 <author>Harold W. Kuhn</author>
 <title>The Hungarian Method for the assignment problem</title>
 <journal>Naval Research Logistics Quarterly</journal>
 <volume>2</volume>
 <number>1-2</number>
 <pages>83–97</pages>
 <year>1955</year>
 <doi>10.1002/nav.3800020109</doi>
 <!--<file>Networks/algorithms/graphs/matching...</file>-->
</reference>

<reference id="Kwan 1962">
 <author>Mei-Ko Kwan</author>
 <title>Graphic programming using odd and even points</title>
 <journal>Chinese Mathematics</journal>
 <volume>1</volume>
 <pages>273-277</pages>
 <year>1962</year>
 <!--<doi>10.1002/nav.3800020109</doi>-->
 <!--<file>Networks/algorithms/graphs/...</file>-->
</reference>



<reference id="Latombe 1991">
 <author>Jean Claude Latombe</author>
 <title>Robot Motion Planning</title>
 <publisher>Kluwer Academic Publishers</publisher>
 <year>1991</year>
 <isbn>079239206X</isbn>
</reference>

<reference id="LaValle 2006">
 <author>Steven M. LaValle</author>
 <title>Planning Algorithms</title>
 <publisher>Cambridge University Press</publisher>
 <year>2006</year>
 <isbn>0-521-86205-1</isbn>
 <url>http://planning.cs.uiuc.edu/</url>
</reference>

<reference id="Lawler 1976">
 <author>E.L. Lawler</author>
 <title>A note on the complexity of the chromatic number problem</title>
 <journal>Information Processing Letters</journal>
 <volume>5</volume>
 <number>3</number>
 <pages>66-67</pages>
 <year>1976</year>
 <doi>10.1016/0020-0190(76)90065-X</doi>
 <file>Networks/algorithms/graphs/np/1976 IPL [Lawler] Chromatic number problem.pdf</file>
</reference>


<reference id="Lovasz and Plummer 1986">
 <author>László Lovász</author>
 <author>M.D. Plummer</author>
 <title>Matching Theory</title>
 <publisher>Elsevier B.V.</publisher>
 <year>1986</year>
 <isbn>0444879161</isbn>
</reference>

<reference id="Lueker and Booth 1979">
 <author>George S. Lueker</author>
 <author>Kellogg S.Booth</author>
 <title>A linear time algorithm for deciding interval graph isomorphism</title>
 <journal>Journal of the ACM</journal>
 <volume>26</volume>
 <number>2</number>
 <pages>183-195</pages>
 <year>1979</year>
 <doi>10.1145/322123.322125</doi>
 <file>Networks/algorithms/graphs/isomorphism/1979 JACM Interval graph isomorphism.pdf</file>
</reference>


<reference id="Luks 1982">
 <author>Eugene M. Luks</author>
 <title>Isomorphism of graphs of bounded valence can be tested in polynomial time</title>
 <journal>Journal of Computer and System Sciences</journal>
 <volume>25</volume>
 <number>1</number>
 <pages>42-65</pages>
 <year>1982</year>
 <doi>10.1016/0022-0000(82)90009-5</doi>
 <file>Networks/algorithms/graphs/isomorphism/1982 JCSS [Luks] Isomorphism of graphs of bounded valence.pdf</file>
</reference>


<reference id="Luks 1986">
 <author>Eugene M. Luks</author>
 <title>Parallel algorithms for permutation groups and graph isomorphism</title>
 <proceedings>27th Annual IEEE Symposium on Foundations of Computer Science</proceedings>
 <conference>SFCS</conference>
 <location>Toronto, Ontario, Canada</location>
 <year>1986</year>
 <isbn>0-8186-0740-8</isbn>
 <pages>292-302</pages>
 <doi>10.1109/SFCS.1986.39</doi>
 <file>Networks/algorithms/graphs/isomorphism/1986 SFCS Isomorphism of colored graphs with bounded color multiplicity.pdf</file>
</reference>


<reference id="Lynch 1997">
 <author>Nancy A. Lynch</author>
 <title>Distributed Algorithms</title>
 <publisher>Morgan Kaufmann</publisher>
 <year>1997</year>
 <isbn>1558603484</isbn>
 <file>[books]/algorithms/1558603484 [Lynch] Distributed Algorithms.pdf</file>
</reference>


<reference id="Matula 1987">
 <author>David W. Matula</author>
 <title>Determining edge connectivity in O(nm)</title>
 <proceedings>28th Annual Symposium on Foundations of Computer Science</proceedings>
 <conference>FOCS</conference>
 <location>Los Angeles, California</location>
 <year>1987</year>
 <pages>249-251</pages>
 <isbn>0-8186-0807-2</isbn>
 <doi>10.1109/SFCS.1987.19</doi>
 <file>Networks/algorithms/graphs/connectivity/1987 FOCS [Matula] Edge connectivity.pdf</file>
</reference>

<reference id="McKay 1981">
 <author>Brendan D. McKay</author>
 <title>Practical graph isomorphism</title>
 <proceedings>10th Manitoba Conference on Numerical Mathematics and Computing</proceedings>
 <booktitle>Congressus Numerantium 30</booktitle>
 <location>Winnipeg, Canada</location>
 <year>1980</year>
 <pages>45-87</pages>
 <year>1981</year>
 <file>Networks/algorithms/graphs/isomorphism/1981 CN [McKay] Practical Graph Isomorphism.pdf</file>
 <url>http://cs.anu.edu.au/~bdm/papers/pgi.pdf</url>
</reference>


<reference id="Micali and Vazirani 1980">
 <author>Silvio Micali</author>
 <author>Vijay V. Vazirani</author>
 <title>An $O(\sqrt{|V|}|e|)$ algorithm for finding maximum matchings in general graphs</title>
 <proceedings>21th Annual Symposium on Foundations of Computer Science</proceedings>
 <conference>FOCS</conference>
 <location>Syracuse, New York</location>
 <year>1980</year>
 <pages>17-27</pages>
 <doi>10.1109/SFCS.1980.12</doi>
 <file>Networks/algorithms/flow/matching/1980 FOCS Maximum matchings.pdf</file>
</reference>



<reference id="Miller 1980">
 <author>Gary L. Miller</author>
 <title>Isomorphism testing for graphs of bounded genus</title>
 <proceedings>12th Annual ACM Symposium on the Theory of Computing</proceedings>
 <conference>STOC</conference>
 <location>Los Angeles, California</location>
 <year>1980</year>
 <isbn>0-89791-017-6</isbn>
 <pages>225-235</pages>
 <doi>10.1145/800141.804670</doi>
 <file>Networks/algorithms/graphs/isomorphism/1980 STOC Isomorphism testing for graphs of bounded genus.pdf</file>
</reference>

<reference id="Miller 1983a">
 <author>Gary L. Miller</author>
 <title>Isomorphism testing and canonical forms for k-contractable graphs (A generalization of bounded valence and bounded genus)</title>
 <proceedings>International Conference on Foundations of Computer Theory</proceedings>
 <conference>FCT</conference>
 <location>Borgholm, Sweden</location>
 <series>Lecture Notes in Computer Science</series>
 <volume>158</volume>
 <year>1983</year>
 <isbn>3-540-12689-9</isbn>
 <pages>310-327</pages>
 <doi>10.1145/800070.802206</doi>
 <file>Networks/algorithms/graphs/isomorphism/1983 FCT Isomorphism of k-contractable graphs.pdf</file>
</reference>

<reference id="Miller 1983b">
 <author>Gary L. Miller</author>
 <title>Isomorphism of k-Contractible Graphs. A Generalization of Bounded Valence and Bounded Genus</title>
 <journal>Information and Control</journal>
 <volume>56</volume>
 <number>1/2</number>
 <pages>1-20</pages>
 <year>1983</year>
 <doi>10.1016/S0019-9958(83)80047-3</doi>
 <file>Networks/algorithms/graphs/isomorphism/1983 IC Isomorphism of k-contractible graphs.pdf</file>
</reference>


<reference id="Moore 1959">
 <author>Edward F. Moore</author>
 <title>The shortest path through a maze</title>
 <proceedings>International Symposium on the Theory of Switching</proceedings>
 <conference>ISST</conference>
 <location>Cambridge, Massachusetts</location>
 <year>1959</year>
 <pages>285-292</pages>
</reference>


<reference id="Papadimitriou and Steiglitz 1982">
 <author>Christos H. Papadimitriou</author>
 <author>Kenneth Steiglitz</author>
 <title>Combinatorial Optimization: Algorithms and Complexity</title>
 <publisher>Prentice Hall</publisher>
 <year>1982</year>
 <isbn>0131524623</isbn>
</reference>

<reference id="Papadimitriou and Yannakakis 1989">
 <author>Christos H. Papadimitriou</author>
 <author>Mihalis Yannakakis</author>
 <title>Shortest paths without a map</title>
 <proceedings>16th International Colloquium on Automata, Languages and Programming</proceedings>
 <conference>ICALP</conference>
 <location>Stresa, Italy</location>
 <series>Lecture Notes in Computer Science</series>
 <volume>372</volume>
 <year>1989</year>
 <isbn>3-540-51371-X</isbn>
 <pages>610-620</pages>
 <doi>10.1007/BFb0035787</doi>
</reference>

<reference id="Papadimitriou and Yannakakis 1991">
 <author>Christos H. Papadimitriou</author>
 <author>Mihalis Yannakakis</author>
 <title>Shortest paths without a map</title>
 <journal>Theoretical Computer Science</journal>
 <volume>84</volume>
 <number>1</number>
 <pages>127-150</pages>
 <year>1991</year>
 <doi>10.1016/0304-3975(91)90263-2</doi>
 <file>Networks/algorithms/graphs/paths/1991 TCS Shortest paths without a map.pdf</file>
</reference>

<reference id="Pearl 1984">
 <author>Judea Pearl</author>
 <title>Heuristics: Intelligent Search Strategies for Computer Problem Solving</title>
  <publisher>Addison-Wesley</publisher>
  <year>1984</year>
  <isbn>0201055945</isbn>
</reference>


<reference id="Pettie and Ramachandran 2002">
 <author>Seth Pettie</author>
 <author>Vijaya Ramachandran</author>
 <title>An optimal minimum spanning tree algorithm</title>
 <journal>Journal of the ACM</journal>
 <volume>49</volume>
 <number>1</number>
 <pages>16-34</pages>
 <year>2002</year>
 <doi>10.1145/505241.505243</doi>
 <file>Networks/algorithms/graphs/paths/2002 JACM [Pettie] MST.pdf</file>
</reference>

<reference id="Pettie and Ramachandran 2008">
 <author>Seth Pettie</author>
 <author>Vijaya Ramachandran</author>
 <title>Randomized minimum spanning tree algorithms using exponentially fewer random bits</title>
 <journal>ACM Transactions on Algorithms</journal>
 <volume>4</volume>
 <number>1</number>
 <pages>a5</pages>
 <year>2008</year>
 <doi>10.1145/1328911.1328916</doi>
 <file>Networks/algorithms/graphs/paths/2008 TALG [Pettie] MST.pdf</file>
</reference>


<reference id="Prim 1957">
 <author>Robert C. Prim:</author>
 <title>Shortest connection networks and some generalizations</title>
 <journal>Bell System Technical Journal</journal>
 <volume>36</volume>
 <pages>1389-1401</pages>
 <year>1957</year>
 <url>http://www.alcatel-lucent.com/bstj/vol36-1957/articles/bstj36-6-1389.pdf</url>
 <file>Networks/algorithms/graphs/paths/1957 BSTJ [Prim] MST.pdf</file>
</reference>


<reference id="Roy 1959">
 <author>Bernard Roy</author>
 <title>Transitivité et connexité (Transitivity and connectivity, in French)</title>
 <journal>C.R. Acad. Sci. Paris</journal>
 <volume>249</volume>
 <pages>216-218</pages>
 <year>1959</year>
 <!--<doi>10.1002/nav.3800020109</doi>-->
 <!--<file>Networks/algorithms/graphs/...</file>-->
</reference>


<reference id="Schmidt and Druffel 1976">
 <author>Douglas C. Schmidt</author>
 <author>Larry E. Druffel</author>
 <title>A fast backtracking algorithm to test directed graphs for isomorphism using distance matrices</title>
 <journal>Journal of the ACM</journal>
 <volume>23</volume>
 <number>3</number>
 <pages>433-445</pages>
 <year>1976</year>
 <doi>10.1145/321958.321963</doi>
 <file>Networks/algorithms/graphs/isomorphism/1976 JACM [Schmidt &amp; Druffel] Graph isomorphism.pdf</file>
</reference>


<reference id="Schrijver 2002">
 <author>Alexander Schrijver</author>
 <title>On the history of the transportation and maximum flow problems</title>
 <journal>Mathematical Programming</journal>
 <volume>91</volume>
 <number>3</number>
 <pages>437-445</pages>
 <year>2002</year>
 <doi>10.1007/s101070100259</doi>
 <file>Networks/algorithms/flow/2002 MP Transportation and network flow.pdf</file>
</reference>

<reference id="Sedgewick 1990">
 <author>Robert Sedgewick</author>
 <title>Algorithms in C</title>
 <publisher>Addison-Wesley</publisher>
 <year>1990</year>
 <isbn>0201514257</isbn>
 <file>[books]/algorithms/0201514257 [Sedgewick] Algorithms in C.pdf</file>
</reference>

<reference id="Sedgewick 2002">
 <author>Robert Sedgewick</author>
 <title>Algorithms in Java. Parts 1-4</title>
 <publisher>Addison-Wesley</publisher>
 <edition>3rd edition</edition>
 <year>2002</year>
 <isbn>0201361205</isbn>
 <file>[books]/algorithms/0201361205 [Sedgewick] Algorithms in Java.chm</file>
</reference>

<reference id="Sedgewick 2003">
 <author>Robert Sedgewick</author>
 <title>Algorithms in Java. Part 5. Graph Algorithms</title>
 <publisher>Addison-Wesley</publisher>
 <edition>3rd edition</edition>
 <year>2003</year>
 <isbn>0201361213</isbn>
 <file>[books]/algorithms/0201361213 [Sedgewick] Algorithms in Java - Graph Algorithms.chm</file>
</reference>

<reference id="Skiena 2008">
 <author>Steven S. Skiena</author>
 <title>The Algorithm Design Manual</title>
 <publisher>Springer</publisher>
 <edition>2nd edition</edition>
 <year>2008</year>
 <isbn>1848000693</isbn>
 <file>[books]/algorithms/1848000693 [Skiena] The Algorithm Design Manual.2nd.pdf</file>
</reference>

<reference id="Stasko and Vitter 1987">
 <author>John T. Stasko</author>
 <author>Jeffrey Scott Vitter</author>
 <title>Pairing heaps: Experiments and analysis</title>
 <journal>Communications of the ACM</journal>
 <volume>30</volume>
 <number>3</number>
 <pages>234-249</pages>
 <year>1987</year>
 <doi>10.1145/214748.214759</doi>
 <file>Computer Science/algorithms/adt/1987 CACM Pairing heaps.pdf</file>
</reference>

<reference id="Tarjan 1972">
 <author>Robert E. Tarjan</author>
 <title>Depth-first search and linear graph algorithms</title>
 <journal>SIAM Journal on Computing</journal>
 <volume>1</volume>
 <number>2</number>
 <pages>146-160</pages>
 <year>1972</year>
 <doi>10.1137/0201010</doi>
 <file>Networks/algorithms/graphs/connectivity/1972 SIAM JC [Tarjan] DFS and linear graph algorithms.pdf</file>
 <file>Networks/algorithms/graphs/connectivity/1972 SIAM JC [Tarjan] DFS and linear graph algorithms.classic.pdf</file>
</reference>

<reference id="Tarjan 1979">
 <author>Robert E. Tarjan</author>
 <title>A class of algorithms which require nonlinear time to maintain disjoint sets</title>
 <journal>Journal of Computer and System Sciences</journal>
 <volume>18</volume>
 <number>2</number>
 <pages>110-127</pages>
 <year>1979</year>
 <doi>10.1016/0022-0000(79)90042-4</doi>
 <file>Computer Science/algorithms/adt/1979 JCSS [Tarjan] Union-find.pdf</file>
</reference>


<reference id="Tarjan 1983">
 <author>Robert E. Tarjan</author>
 <title>Data Structures and Network Algorithms.</title>
 <publisher>Society for Industrial and Applied Mathematics</publisher>
 <year>1983</year>
 <isbn>0898711878</isbn>
</reference>


<reference id="Tarjan and Vishkin 1985">
 <author>Robert E. Tarjan</author>
 <author>Uzi Vishkin</author>
 <title>An efficient parallel biconnectivity algorithm</title>
 <journal>SIAM Journal on Computing</journal>
 <volume>14</volume>
 <number>4</number>
 <pages>862-874</pages>
 <year>1985</year>
 <doi>10.1137/0214061</doi>
 <file>Networks/algorithms/graphs/connectivity/1985 SIAM JC [Tarjan &amp; Vishkin] Efficient parallel biconnectivity algorithm.tr.pdf</file>
</reference>

<reference id="Ullman 1976">
 <author>Julian R. Ullman</author>
 <title>An algorithm for subgraph isomorphism</title>
 <journal>Journal of the ACM</journal>
 <volume>23</volume>
 <number>1</number>
 <pages>31-42</pages>
 <year>1976</year>
 <doi>10.1145/321921.321925</doi>
 <file>Networks/algorithms/graphs/isomorphism/1976 JACM [Ullman] Subgraph isomorphism.pdf</file>
</reference>

<reference id="van Emde Boas et al. 1977">
 <author>Peter van Emde Boas</author>
 <author>R. Kaas</author>
 <author>E. Zijlstra</author>
 <title>Design and implementation of an efficient priority queue</title>
 <journal>Mathematical Systems Theory</journal>
 <volume>10</volume>
 <number>1</number>
 <pages>99-127</pages>
 <year>1977</year>
 <doi>10.1007/BF01683268</doi>
 <file>Computer Science/algorithms/adt/1977 MST [Emde Boas] Priority queue.pdf</file>
</reference>


<reference id="Warshall 1962">
 <author>Stephen Warshall</author>
 <title>A theorem on Boolean matrices</title>
 <journal>Journal of the ACM</journal>
 <volume>9</volume>
 <number>1</number>
 <pages>11-12</pages>
 <year>1962</year>
 <doi>10.1145/321105.321107</doi>
 <file>Networks/algorithms/graphs/paths/1962 JACM [Warshall] Boolean matrices.pdf</file>
</reference>

<reference id="Westbrook and Tarjan 1992">
 <author>Jeffery Westbrook</author>
 <author>Robert E. Tarjan</author>
 <title>Maintaining bridge-connected and biconnected components on-line</title>
 <journal>Algorithmica</journal>
 <volume>7</volume>
 <number>1-6</number>
 <pages>433-464</pages>
 <year>1992</year>
 <doi>10.1007/BF01758773</doi>
 <file>Networks/algorithms/graphs/connectivity/1992 Algorithmica [Westbrook &amp; Tarjan] Bridge-connected and biconnected components.pdf</file>
</reference>

<!-- NEW -->



</document>

</document>
